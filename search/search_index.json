{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"EHR Sequencing","text":"<p>Research Framework for Longitudinal EHR Sequence Modeling</p> <p>A comprehensive toolkit for exploring temporal representations, learning objectives, and model architectures for disease progression, survival analysis, and temporal phenotyping under censoring and irregular follow-up.</p>"},{"location":"#overview","title":"Overview","text":"<p>EHR Sequencing applies sequence modeling techniques from genomics and NLP to Electronic Health Records, treating medical codes as \"words\" and patient histories as \"documents\" to enable:</p> <ul> <li>Disease Progression Modeling - Predict future diagnoses and outcomes</li> <li>Survival Analysis - Time-to-event modeling with proper censoring handling</li> <li>Temporal Phenotyping - Discover disease subtypes from patient trajectories  </li> <li>Patient Segmentation - Cluster patients by clinical similarity</li> <li>Clinical Trajectory Analysis - Understand disease evolution patterns</li> </ul>"},{"location":"#the-analogy","title":"The Analogy","text":"<pre><code>DNA Sequences (ATCG...)  \u2192  Genomic Language Models\n    \u2193                              \u2193\nMedical Code Sequences   \u2192  EHR Sequencing Models\n(LOINC, SNOMED, ICD...)      (This Project)\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#comprehensive-data-pipeline","title":"\ud83c\udfe5 Comprehensive Data Pipeline","text":"<ul> <li>Multi-source adapters: Synthea, MIMIC-III support</li> <li>Visit grouping: Semantic code ordering (diagnoses \u2192 procedures \u2192 medications)</li> <li>Flexible tokenization: Visit-based, flat, or hierarchical sequences</li> <li>PyTorch integration: Ready-to-use datasets and dataloaders</li> </ul>"},{"location":"#survival-analysis","title":"\ud83e\uddec Survival Analysis","text":"<ul> <li>Discrete-time survival models: LSTM-based hazard prediction</li> <li>Synthetic outcome generation: Validated correlation (r = -0.5)</li> <li>Proper censoring handling: Negative log-likelihood loss</li> <li>C-index evaluation: Fixed-horizon risk scores to avoid length bias</li> </ul>"},{"location":"#model-architectures","title":"\ud83e\udd16 Model Architectures","text":"<ul> <li>LSTM baseline: Visit-level sequence encoding</li> <li>Discrete-time survival LSTM: Hazard prediction at each visit</li> <li>Extensible framework: Easy to add Transformers, BEHRT, etc.</li> </ul>"},{"location":"#evaluation-validation","title":"\ud83d\udcca Evaluation &amp; Validation","text":"<ul> <li>Concordance index (C-index): Survival model ranking quality</li> <li>Synthetic data validation: Fast iteration with pre-validated outcomes</li> <li>Memory estimation: Plan GPU requirements before training</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># 1. Clone repository\ngit clone https://github.com/pleiadian53/ehr-sequencing.git\ncd ehr-sequencing\n\n# 2. Create conda environment (choose your platform)\n# macOS (M1/M2/M3):\nmamba env create -f environment-macos.yml\n# Linux/Windows with NVIDIA GPU:\nmamba env create -f environment-cuda.yml\n# CPU-only:\nmamba env create -f environment-cpu.yml\n\n# 3. Activate environment\nmamba activate ehrsequencing\n\n# 4. Install package with poetry\npoetry install\n\n# 5. Verify installation\npython -c \"import ehrsequencing; print(f'\u2705 EHR Sequencing v{ehrsequencing.__version__} ready!')\"\n</code></pre> <p>See Installation Guide for detailed instructions.</p>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from ehrsequencing.data import SyntheaAdapter, VisitGrouper, PatientSequenceBuilder\n\n# 1. Load EHR data\nadapter = SyntheaAdapter('data/synthea/')\npatients = adapter.load_patients(limit=100)\nevents = adapter.load_events(patient_ids=[p.patient_id for p in patients])\n\n# 2. Group events into visits (with semantic code ordering)\ngrouper = VisitGrouper(strategy='hybrid', preserve_code_types=True)\npatient_visits = grouper.group_by_patient(events)\n\n# 3. Build patient sequences\nbuilder = PatientSequenceBuilder(max_visits=50, max_codes_per_visit=100)\nvocab = builder.build_vocabulary(patient_visits, min_frequency=5)\nsequences = builder.build_sequences(patient_visits, min_visits=2)\n\n# 4. Create PyTorch dataset\ndataset = builder.create_dataset(sequences)\nprint(f\"Created dataset with {len(dataset)} patients\")\nprint(f\"Vocabulary size: {len(vocab)}\")\n</code></pre>"},{"location":"#survival-analysis-example","title":"Survival Analysis Example","text":"<pre><code>from ehrsequencing.models import DiscreteTimeSurvivalLSTM\nfrom ehrsequencing.synthetic.survival import DiscreteTimeSurvivalGenerator\n\n# Generate synthetic outcomes\ngenerator = DiscreteTimeSurvivalGenerator(censoring_rate=0.3)\noutcome = generator.generate(sequences)\n\n# Train survival model\nmodel = DiscreteTimeSurvivalLSTM(\n    vocab_size=len(vocab),\n    embedding_dim=128,\n    hidden_dim=256\n)\n\n# Evaluate with C-index\n# See notebooks/02_survival_analysis/ for complete workflow\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#tutorials","title":"\ud83d\udcda Tutorials","text":"<ul> <li>Survival Analysis: Prediction Problem</li> <li>Synthetic Data Design &amp; Labeling</li> <li>Loss Function Formulation</li> </ul>"},{"location":"#notebooks","title":"\ud83d\udcd3 Notebooks","text":"<ul> <li>Discrete-Time Survival LSTM</li> </ul>"},{"location":"#methods","title":"\ud83d\udcd6 Methods","text":"<ul> <li>Causal Survival Analysis</li> <li>Modern Code Embeddings</li> </ul>"},{"location":"#guides","title":"\ud83d\udee0\ufe0f Guides","text":"<ul> <li>Data Generation with Synthea</li> <li>Pretrained Embeddings</li> <li>RunPods Training</li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<p>Phase: 1.5 - Survival Analysis (80% Complete) Version: 0.1.0 (Alpha) Status: Active Development</p>"},{"location":"#recent-updates-january-2026","title":"Recent Updates (January 2026)","text":"<ul> <li>\u2705 Implemented discrete-time survival LSTM model</li> <li>\u2705 Created synthetic outcome generator with validated correlation</li> <li>\u2705 Resolved C-index calculation issues (achieved 0.65-0.70)</li> <li>\u2705 Comprehensive survival analysis tutorials</li> <li>\ud83d\udd04 Next: Code embeddings (Med2Vec, BEHRT)</li> </ul> <p>See the project repository for detailed development plan.</p>"},{"location":"#research-focus","title":"Research Focus","text":"<p>This project explores multiple dimensions of EHR sequence modeling:</p>"},{"location":"#temporal-representations","title":"Temporal Representations","text":"<ul> <li>Visit-based sequences</li> <li>Flat event streams</li> <li>Hierarchical code structures</li> <li>Time-aware embeddings</li> </ul>"},{"location":"#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Supervised prediction (disease onset, mortality)</li> <li>Self-supervised pre-training (masked language modeling)</li> <li>Survival analysis (time-to-event with censoring)</li> <li>Representation learning (patient embeddings)</li> </ul>"},{"location":"#model-architectures_1","title":"Model Architectures","text":"<ul> <li>LSTMs (baseline and survival variants)</li> <li>Transformers (BEHRT-style)</li> <li>Graph neural networks (code relationships)</li> <li>Hybrid architectures</li> </ul>"},{"location":"#real-world-challenges","title":"Real-World Challenges","text":"<ul> <li>Censoring (patients lost to follow-up)</li> <li>Irregular sampling (variable visit frequencies)</li> <li>Missing data (incomplete records)</li> <li>Length bias (variable sequence lengths)</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use this framework in your research, please cite:</p> <pre><code>@software{ehr_sequencing_2026,\n  title = {EHR Sequencing: Research Framework for Longitudinal EHR Modeling},\n  author = {EHR Sequencing Research Team},\n  year = {2026},\n  url = {https://github.com/pleiadian53/ehr-sequencing}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<ul> <li>Synthea: Synthetic patient data generation</li> <li>PyHealth: Reference implementations for EHR modeling</li> <li>Material for MkDocs: Documentation framework with LaTeX support</li> </ul>"},{"location":"#contact","title":"Contact","text":"<ul> <li>GitHub: pleiadian53/ehr-sequencing</li> <li>Issues: Report bugs or request features</li> </ul> <p>Built with \u2764\ufe0f for advancing healthcare AI research</p>"},{"location":"INSTALL/","title":"Installation Guide","text":""},{"location":"INSTALL/#prerequisites","title":"Prerequisites","text":"<ul> <li>Conda/Mamba: For environment management</li> <li>Python: 3.10 or higher (&lt; 3.13)</li> <li>Git: For version control</li> </ul>"},{"location":"INSTALL/#quick-start","title":"Quick Start","text":""},{"location":"INSTALL/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone &lt;repository-url&gt;\ncd ehr-sequencing\n</code></pre>"},{"location":"INSTALL/#2-create-environment","title":"2. Create Environment","text":"<p>Choose the appropriate environment file for your system:</p> <p>macOS (M1/M2/M3 Mac) - Recommended:</p> <pre><code># Uses MPS (Metal Performance Shaders) for GPU acceleration\nmamba env create -f environment-macos.yml\nmamba activate ehrsequencing\n</code></pre> <p>Linux/Windows with NVIDIA GPU:</p> <pre><code># Uses CUDA 12.1 for GPU acceleration\nmamba env create -f environment-cuda.yml\nmamba activate ehrsequencing\n</code></pre> <p>CPU-only (any platform):</p> <pre><code># No GPU acceleration\nmamba env create -f environment-cpu.yml\nmamba activate ehrsequencing\n</code></pre> <p>Default (macOS-compatible):</p> <pre><code># Same as environment-macos.yml\nmamba env create -f environment.yml\nmamba activate ehrsequencing\n</code></pre> <p>Note: Replace <code>mamba</code> with <code>conda</code> if you prefer conda over mamba.</p>"},{"location":"INSTALL/#3-install-package-with-poetry","title":"3. Install Package with Poetry","text":"<pre><code># Install poetry if not already installed\npip install poetry\n\n# Install package and dependencies\npoetry install\n</code></pre> <p>Alternative: Install with pip (editable mode)</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"INSTALL/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code># Check Python version\npython --version  # Should be 3.10+\n\n# Test import\npython -c \"import ehrsequencing; print(ehrsequencing.__version__)\"\n\n# Run tests\npytest tests/\n</code></pre>"},{"location":"INSTALL/#environment-management","title":"Environment Management","text":""},{"location":"INSTALL/#activating-the-environment","title":"Activating the Environment","text":"<pre><code>mamba activate ehrsequencing\n# or\nconda activate ehrsequencing\n</code></pre>"},{"location":"INSTALL/#updating-dependencies","title":"Updating Dependencies","text":"<pre><code># Update from environment file (use your platform-specific file)\nmamba env update -f environment-macos.yml  # macOS\n# or\nmamba env update -f environment-cuda.yml   # Linux/Windows GPU\n# or\nmamba env update -f environment-cpu.yml    # CPU-only\n\n# Update with poetry\npoetry update\n</code></pre>"},{"location":"INSTALL/#deactivating","title":"Deactivating","text":"<pre><code>conda deactivate\n</code></pre>"},{"location":"INSTALL/#development-setup","title":"Development Setup","text":""},{"location":"INSTALL/#additional-development-tools","title":"Additional Development Tools","text":"<pre><code># Install pre-commit hooks (optional)\npip install pre-commit\npre-commit install\n\n# Install Jupyter extensions\njupyter contrib nbextension install --user\n</code></pre>"},{"location":"INSTALL/#ide-setup","title":"IDE Setup","text":"<p>VS Code: 1. Install Python extension 2. Select interpreter: <code>ehrsequencing</code> environment 3. Enable linting (Ruff) and formatting (Black)</p> <p>PyCharm: 1. Set project interpreter to <code>ehrsequencing</code> environment 2. Enable Black formatter 3. Configure Ruff for linting</p>"},{"location":"INSTALL/#data-setup","title":"Data Setup","text":""},{"location":"INSTALL/#synthea-synthetic-data","title":"Synthea (Synthetic Data)","text":"<pre><code># Download and install Synthea\n# See: https://github.com/synthetichealth/synthea\n\n# Generate synthetic data\n./run_synthea -p 10000\n\n# Move to project data directory\nmkdir -p data/synthea\ncp output/csv/*.csv data/synthea/\n</code></pre>"},{"location":"INSTALL/#mimic-iiiiv-real-data","title":"MIMIC-III/IV (Real Data)","text":"<ol> <li>Apply for access: https://physionet.org/</li> <li>Complete CITI training</li> <li>Sign data use agreement</li> <li>Download data (after approval)</li> <li>Set up PostgreSQL database (optional)</li> </ol> <pre><code># Create data directories\nmkdir -p data/mimic\n</code></pre>"},{"location":"INSTALL/#pre-trained-models","title":"Pre-trained Models","text":"<pre><code># Download CEHR-BERT pre-trained embeddings\n# See the pretrained embeddings guide for details\n\nmkdir -p checkpoints/cehrbert\n# Download from Hugging Face or model repository\n</code></pre>"},{"location":"INSTALL/#hardware-specific-setup","title":"Hardware-Specific Setup","text":""},{"location":"INSTALL/#m1-macbook-local-development","title":"M1 MacBook (Local Development)","text":"<pre><code># Verify MPS (Metal Performance Shaders) support\npython -c \"import torch; print(f'MPS available: {torch.backends.mps.is_available()}')\"\n\n# Use small model configs for development\n# See: docs/implementation/resource-aware-models.md\n</code></pre>"},{"location":"INSTALL/#runpod-cloud-gpu","title":"RunPod / Cloud GPU","text":"<p>For detailed setup on RunPod or cloud GPU instances: - Includes A40, A100, RTX 4090 configurations - SSH setup, data transfer, and training workflows - See the RunPods Training Guide for details</p>"},{"location":"INSTALL/#troubleshooting","title":"Troubleshooting","text":""},{"location":"INSTALL/#conda-environment-issues","title":"Conda Environment Issues","text":"<pre><code># Remove and recreate environment\nmamba env remove -n ehrsequencing\nmamba env create -f environment.yml\n</code></pre>"},{"location":"INSTALL/#poetry-installation-issues","title":"Poetry Installation Issues","text":"<pre><code># Clear poetry cache\npoetry cache clear pypi --all\n\n# Reinstall\npoetry install --no-cache\n</code></pre>"},{"location":"INSTALL/#import-errors","title":"Import Errors","text":"<pre><code># Ensure package is installed in editable mode\npip install -e .\n\n# Check PYTHONPATH\necho $PYTHONPATH\n</code></pre>"},{"location":"INSTALL/#pytorch-mps-issues-m1-mac","title":"PyTorch MPS Issues (M1 Mac)","text":"<pre><code># If MPS is not available, PyTorch will fall back to CPU\n# Ensure you have the latest PyTorch version\nmamba install pytorch::pytorch -c pytorch\n</code></pre>"},{"location":"INSTALL/#database-connection-mimic","title":"Database Connection (MIMIC)","text":"<pre><code># Test PostgreSQL connection\npsql -h localhost -U your_username -d mimic3\n\n# Set environment variables\nexport MIMIC_USER=your_username\nexport MIMIC_PASSWORD=your_password\n</code></pre>"},{"location":"INSTALL/#next-steps","title":"Next Steps","text":"<ol> <li>Read documentation: <code>docs/README.md</code></li> <li>Explore notebooks: <code>notebooks/README.md</code></li> <li>Run examples: <code>examples/README.md</code></li> <li>Check implementation plan: <code>docs/implementation/visit-grouped-sequences.md</code></li> <li>Review model configs: <code>docs/implementation/resource-aware-models.md</code></li> </ol>"},{"location":"INSTALL/#detailed-installation-guides","title":"Detailed Installation Guides","text":"<p>For more detailed guides, see:</p> <ul> <li>Pretrained Embeddings Guide - CEHR-BERT, Med-BERT</li> <li>RunPods Training Guide - Cloud GPU training</li> <li>Data Generation Guide - Synthea setup</li> </ul>"},{"location":"INSTALL/#getting-help","title":"Getting Help","text":"<ul> <li>Check the documentation for detailed guides</li> <li>Open an issue on GitHub</li> </ul>"},{"location":"INSTALL/#related-projects","title":"Related Projects","text":"<ul> <li>loinc-predictor - LOINC code prediction</li> <li>genai-lab - Generative AI for biomedical data</li> </ul>"},{"location":"pretrained_embeddings_guide/","title":"Pretrained Embeddings for Medical Codes","text":""},{"location":"pretrained_embeddings_guide/#overview","title":"Overview","text":"<p>Pretrained embeddings capture semantic relationships between medical codes learned from large corpora. Using them instead of random initialization can significantly improve model performance, especially with limited training data.</p> <p>Expected Impact: +0.05-0.10 C-index improvement</p>"},{"location":"pretrained_embeddings_guide/#available-pretrained-embeddings","title":"Available Pretrained Embeddings","text":""},{"location":"pretrained_embeddings_guide/#1-med2vec-recommended","title":"1. Med2Vec (Recommended)","text":"<p>What it is: Skip-gram embeddings trained on visit co-occurrence patterns</p> <p>Advantages: - Learns from EHR visit structure (codes that co-occur have similar embeddings) - Captures clinical relationships (e.g., diabetes codes cluster together) - Relatively easy to train on your own data</p> <p>How to get: - Train on your own EHR data using Med2Vec implementation - Use publicly available Med2Vec embeddings (if available)</p> <p>Training Med2Vec: <pre><code>from med2vec import Med2Vec\n\n# Prepare visit data: List[List[str]] (list of visits, each visit is list of codes)\nvisits = [\n    ['250.00', '401.9', '272.4'],  # Visit 1\n    ['250.00', '585.9'],            # Visit 2\n    ...\n]\n\n# Train Med2Vec\nmodel = Med2Vec(\n    num_codes=len(vocab),\n    embedding_dim=128,\n    num_visits=len(visits)\n)\n\nmodel.train(visits, epochs=100)\nmodel.save('med2vec_embeddings.pkl')\n</code></pre></p>"},{"location":"pretrained_embeddings_guide/#2-cui2vec","title":"2. CUI2Vec","text":"<p>What it is: UMLS Concept embeddings trained on clinical notes</p> <p>Advantages: - Trained on large clinical text corpora - Captures semantic relationships from natural language - Publicly available</p> <p>Requirements: - Map ICD/CPT codes to UMLS CUIs - Download CUI2Vec embeddings</p> <p>Download: <pre><code># CUI2Vec embeddings\nwget https://figshare.com/ndownloader/files/10959626 -O cui2vec_pretrained.txt\n</code></pre></p>"},{"location":"pretrained_embeddings_guide/#3-clinical-bert-embeddings","title":"3. Clinical BERT Embeddings","text":"<p>What it is: Contextualized embeddings from BERT models trained on clinical text</p> <p>Advantages: - State-of-the-art performance - Captures context-dependent meanings - Multiple variants (BioBERT, ClinicalBERT, PubMedBERT)</p> <p>Disadvantages: - More complex to use (requires tokenization) - Larger model size - May be overkill for simple code embeddings</p>"},{"location":"pretrained_embeddings_guide/#using-pretrained-embeddings","title":"Using Pretrained Embeddings","text":""},{"location":"pretrained_embeddings_guide/#option-1-load-from-file","title":"Option 1: Load from File","text":"<pre><code>from ehrsequencing.embeddings import PretrainedEmbedding\n\n# Load embeddings\nembedding = PretrainedEmbedding.from_file(\n    embedding_path='med2vec_embeddings.pkl',\n    vocab=builder.vocab,\n    embedding_dim=128,\n    freeze=True  # Don't update during training\n)\n\n# Use in model\nmodel = DiscreteTimeSurvivalLSTM(\n    vocab_size=builder.vocabulary_size,\n    embedding_dim=128,\n    hidden_dim=256,\n    num_layers=2,\n    dropout=0.3,\n    pretrained_embedding=embedding  # Pass pretrained embeddings\n)\n</code></pre>"},{"location":"pretrained_embeddings_guide/#option-2-med2vec-specific","title":"Option 2: Med2Vec Specific","text":"<pre><code>from ehrsequencing.embeddings import Med2VecEmbedding\n\nembedding = Med2VecEmbedding.from_med2vec_checkpoint(\n    checkpoint_path='med2vec_model.pkl',\n    vocab=builder.vocab,\n    freeze=True\n)\n</code></pre>"},{"location":"pretrained_embeddings_guide/#option-3-train-your-own-med2vec","title":"Option 3: Train Your Own Med2Vec","text":"<pre><code># 1. Extract visits from your data\nvisits_by_patient = defaultdict(list)\nfor visit in visits:\n    visits_by_patient[visit.patient_id].append(visit)\n\n# 2. Convert to code lists\nall_visits = []\nfor patient_visits in visits_by_patient.values():\n    for visit in patient_visits:\n        codes = visit.get_all_codes()\n        all_visits.append(codes)\n\n# 3. Train Med2Vec\nfrom med2vec import Med2Vec\n\nmodel = Med2Vec(\n    num_codes=len(builder.vocab),\n    embedding_dim=128,\n    num_visits=len(all_visits)\n)\n\nmodel.train(all_visits, epochs=100)\n\n# 4. Extract embeddings\ncode_embeddings = model.get_code_embeddings()\n\n# 5. Create embedding layer\nembedding = PretrainedEmbedding(\n    vocab=builder.vocab,\n    embedding_dim=128,\n    pretrained_weights=code_embeddings,\n    freeze=False  # Allow fine-tuning\n)\n</code></pre>"},{"location":"pretrained_embeddings_guide/#freezing-vs-fine-tuning","title":"Freezing vs. Fine-tuning","text":""},{"location":"pretrained_embeddings_guide/#frozen-embeddings-recommended-for-small-data","title":"Frozen Embeddings (Recommended for Small Data)","text":"<pre><code>embedding = PretrainedEmbedding.from_file(\n    embedding_path='embeddings.pkl',\n    vocab=vocab,\n    embedding_dim=128,\n    freeze=True  # Don't update during training\n)\n</code></pre> <p>Advantages: - Prevents overfitting with small data - Faster training - Preserves pretrained knowledge</p> <p>Disadvantages: - Can't adapt to task-specific patterns</p>"},{"location":"pretrained_embeddings_guide/#fine-tuned-embeddings-recommended-for-large-data","title":"Fine-tuned Embeddings (Recommended for Large Data)","text":"<pre><code>embedding = PretrainedEmbedding.from_file(\n    embedding_path='embeddings.pkl',\n    vocab=vocab,\n    embedding_dim=128,\n    freeze=False  # Update during training\n)\n\n# Or unfreeze after initial training\nembedding.unfreeze()\n</code></pre> <p>Advantages: - Adapts to task-specific patterns - Better performance with sufficient data</p> <p>Disadvantages: - Risk of overfitting with small data - Slower training</p>"},{"location":"pretrained_embeddings_guide/#hybrid-approach-best-of-both","title":"Hybrid Approach (Best of Both)","text":"<pre><code># Phase 1: Train with frozen embeddings\nembedding.freeze = True\ntrain_model(model, epochs=10)\n\n# Phase 2: Fine-tune embeddings\nembedding.unfreeze()\ntrain_model(model, epochs=5, lr=0.0001)  # Lower learning rate\n</code></pre>"},{"location":"pretrained_embeddings_guide/#handling-unknown-codes","title":"Handling Unknown Codes","text":"<p>Pretrained embeddings may not cover all codes in your vocabulary:</p> <pre><code># Check coverage\nembedding = PretrainedEmbedding.from_file(...)\n# Logs: \"Pretrained embedding coverage: 75.3% (583/776)\"\n</code></pre> <p>Strategies:</p> <ol> <li>Random initialization for unknown codes (default)</li> <li>Unknown codes get random embeddings</li> <li> <p>Can be learned during training</p> </li> <li> <p>Average of known embeddings <pre><code># Replace random init with average\nunknown_mask = (embedding.weight == 0).all(dim=1)\nknown_embeddings = embedding.weight[~unknown_mask]\navg_embedding = known_embeddings.mean(dim=0)\nembedding.weight[unknown_mask] = avg_embedding\n</code></pre></p> </li> <li> <p>Hierarchical fallback</p> </li> <li>Map ICD-10 codes to ICD-9 if ICD-10 not found</li> <li>Use code prefixes (e.g., '250' for all diabetes codes)</li> </ol>"},{"location":"pretrained_embeddings_guide/#expected-performance","title":"Expected Performance","text":"Setup C-index Notes Random embeddings, 106 patients 0.45-0.52 Baseline (current) Random embeddings, 1000 patients 0.60-0.70 More data helps Pretrained embeddings, 106 patients 0.50-0.58 Small improvement Pretrained embeddings, 1000 patients 0.65-0.75 Best combination Pretrained + fine-tuned, 1000 patients 0.70-0.80 Optimal"},{"location":"pretrained_embeddings_guide/#implementation-checklist","title":"Implementation Checklist","text":"<ul> <li> Generate/download more training data (1000+ patients)</li> <li> Train Med2Vec on your data OR download pretrained embeddings</li> <li> Load pretrained embeddings using <code>PretrainedEmbedding.from_file()</code></li> <li> Update survival LSTM to accept pretrained embeddings</li> <li> Train with frozen embeddings first</li> <li> Optionally fine-tune embeddings with lower learning rate</li> <li> Evaluate C-index improvement</li> <li> Document embedding source and parameters</li> </ul>"},{"location":"pretrained_embeddings_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"pretrained_embeddings_guide/#low-coverage-50","title":"Low Coverage (&lt;50%)","text":"<p>Problem: Most codes not in pretrained embeddings</p> <p>Solutions: - Train Med2Vec on your own data - Use code mapping (ICD-10 \u2192 ICD-9, detailed \u2192 general) - Use hierarchical embeddings (code prefixes)</p>"},{"location":"pretrained_embeddings_guide/#no-performance-improvement","title":"No Performance Improvement","text":"<p>Problem: Pretrained embeddings don't help</p> <p>Possible causes: - Embeddings trained on different code system - Task-specific patterns not captured by embeddings - Data too small to benefit from pretraining</p> <p>Solutions: - Try fine-tuning instead of freezing - Train task-specific embeddings - Get more training data</p>"},{"location":"pretrained_embeddings_guide/#overfitting-with-fine-tuning","title":"Overfitting with Fine-tuning","text":"<p>Problem: Training loss decreases but validation C-index doesn't improve</p> <p>Solutions: - Use frozen embeddings - Add regularization (weight decay, dropout) - Reduce learning rate for embedding layer - Use hybrid approach (freeze \u2192 fine-tune)</p>"},{"location":"pretrained_embeddings_guide/#references","title":"References","text":"<ul> <li>Med2Vec: Choi et al. (2016). \"Multi-layer Representation Learning for Medical Concepts\"</li> <li>CUI2Vec: Beam et al. (2018). \"Clinical Concept Embeddings Learned from Massive Sources\"</li> <li>Clinical BERT: Alsentzer et al. (2019). \"Publicly Available Clinical BERT Embeddings\"</li> </ul>"},{"location":"pretrained_embeddings_guide/#next-steps","title":"Next Steps","text":"<ol> <li>Get more data (see <code>data_generation_guide.md</code>)</li> <li>Train or download pretrained embeddings</li> <li>Update model to use pretrained embeddings</li> <li>Retrain and evaluate</li> <li>Compare with random embedding baseline</li> </ol>"},{"location":"runpods_training_guide/","title":"RunPods Training Guide for EHR Survival Models","text":"<p>This guide explains how to train large-scale survival models on cloud GPUs when local resources are insufficient.</p>"},{"location":"runpods_training_guide/#when-to-use-cloud-training","title":"When to Use Cloud Training","text":""},{"location":"runpods_training_guide/#local-system-limitations","title":"Local System Limitations","text":"<p>Symptoms: - <code>RuntimeError: MPS backend out of memory</code> - Training takes &gt;30 minutes per epoch - System becomes unresponsive during training - GPU memory allocation failures</p> <p>Typical Limits: - MacBook M1/M2/M3: 8-20 GB unified memory - Consumer GPUs: 8-12 GB (RTX 3060/3070) - Workstation GPUs: 16-24 GB (RTX 3080/3090)</p>"},{"location":"runpods_training_guide/#cloud-training-benefits","title":"Cloud Training Benefits","text":"<ul> <li>Larger datasets: Train on 1,000+ patients instead of 100-200</li> <li>Faster iteration: 10x faster training on dedicated GPUs</li> <li>Better performance: More data \u2192 better C-index (0.65-0.75 vs 0.50-0.60)</li> <li>Cost-effective: Pay only for compute time (~$0.30-0.50/hour)</li> </ul>"},{"location":"runpods_training_guide/#memory-requirements-by-dataset-size","title":"Memory Requirements by Dataset Size","text":"Patients Avg Visits Vocab Size Memory Needed Recommended GPU 100 30 500 2-4 GB Local MPS/CPU 200 40 800 4-8 GB RTX 3060 (12GB) 500 50 1,500 8-12 GB RTX 3080 (10GB) 1,000 60 3,000 16-20 GB RTX 3090 (24GB) 2,000+ 70 5,000 24-32 GB RTX 4090 (24GB) or A100 (40GB)"},{"location":"runpods_training_guide/#runpods-setup-step-by-step","title":"RunPods Setup (Step-by-Step)","text":""},{"location":"runpods_training_guide/#1-create-runpods-account","title":"1. Create RunPods Account","text":"<ol> <li>Visit https://www.runpod.io/</li> <li>Sign up with email or GitHub</li> <li>Add payment method (credit card)</li> <li>Add initial credits ($10-20 recommended)</li> </ol>"},{"location":"runpods_training_guide/#2-select-gpu-pod","title":"2. Select GPU Pod","text":"<p>Recommended GPUs (as of 2026):</p> GPU VRAM Price/hr Best For RTX 3090 24 GB $0.30 Most cost-effective for our use case RTX 4090 24 GB $0.40 Faster training, newer architecture A100 (40GB) 40 GB $1.00 Overkill for &lt;2,000 patients A100 (80GB) 80 GB $1.50 Only for massive datasets (5,000+ patients) <p>Selection Process: 1. Click \"Deploy\" \u2192 \"GPU Pods\" 2. Filter by GPU type (e.g., \"RTX 3090\") 3. Sort by price (lowest first) 4. Check \"Secure Cloud\" for reliability 5. Select pod with good uptime (&gt;95%)</p>"},{"location":"runpods_training_guide/#3-configure-pod-template","title":"3. Configure Pod Template","text":"<p>Option A: Use PyTorch Template (Recommended) <pre><code>Template: RunPod PyTorch 2.1\nCUDA: 11.8 or 12.1\nPython: 3.10+\nDisk: 50 GB (sufficient for our data)\n</code></pre></p> <p>Option B: Custom Docker Image <pre><code>FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime\n\nRUN pip install pandas numpy matplotlib seaborn scikit-learn tqdm jupyter\n</code></pre></p>"},{"location":"runpods_training_guide/#4-upload-code-and-data","title":"4. Upload Code and Data","text":"<p>Method 1: Git Clone (Recommended) <pre><code># SSH into pod\nssh root@&lt;pod-ip&gt; -p &lt;port&gt;\n\n# Clone repository\ngit clone https://github.com/yourusername/ehr-sequencing.git\ncd ehr-sequencing\n\n# Install dependencies\npip install -e .\n</code></pre></p> <p>Method 2: Jupyter Upload 1. Open Jupyter interface (port 8888) 2. Upload notebook: <code>notebooks/02_survival_analysis/01_discrete_time_survival_lstm.ipynb</code> 3. Upload data directory: <code>data/synthea/large_cohort_1000/</code></p> <p>Method 3: Cloud Storage <pre><code># Download from S3/GCS\naws s3 sync s3://your-bucket/synthea-data ./data/synthea/large_cohort_1000/\n\n# Or use wget for public URLs\nwget https://your-storage.com/synthea-data.tar.gz\ntar -xzf synthea-data.tar.gz\n</code></pre></p>"},{"location":"runpods_training_guide/#5-install-dependencies","title":"5. Install Dependencies","text":"<pre><code># If using git clone\ncd ehr-sequencing\npip install -e .\n\n# Or install manually\npip install pandas numpy matplotlib seaborn scikit-learn tqdm torch\n</code></pre>"},{"location":"runpods_training_guide/#6-configure-notebook-for-full-training","title":"6. Configure Notebook for Full Training","text":"<p>Open the notebook and modify cell 11:</p> <pre><code># Change from:\nMAX_PATIENTS = 200  # Local testing\n\n# To:\nMAX_PATIENTS = None  # Full training on cloud GPU\n</code></pre>"},{"location":"runpods_training_guide/#7-run-training","title":"7. Run Training","text":"<p>Option A: Jupyter Notebook 1. Open notebook in Jupyter 2. Run all cells (Cell \u2192 Run All) 3. Monitor progress in output</p> <p>Option B: Python Script <pre><code># Convert notebook to script\njupyter nbconvert --to script 01_discrete_time_survival_lstm.ipynb\n\n# Run as script\npython 01_discrete_time_survival_lstm.py\n</code></pre></p> <p>Option C: Screen Session (for long training) <pre><code># Start screen session\nscreen -S training\n\n# Run training\njupyter nbconvert --execute 01_discrete_time_survival_lstm.ipynb\n\n# Detach: Ctrl+A, then D\n# Reattach: screen -r training\n</code></pre></p>"},{"location":"runpods_training_guide/#8-monitor-training","title":"8. Monitor Training","text":"<p>GPU Utilization: <pre><code># Check GPU usage\nnvidia-smi\n\n# Watch in real-time\nwatch -n 1 nvidia-smi\n</code></pre></p> <p>Expected Output: <pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  NVIDIA RTX 3090    Off  | 00000000:01:00.0 Off |                  N/A |\n| 30%   65C    P2   280W / 350W |  18432MiB / 24576MiB |     95%      Default |\n+-------------------------------+----------------------+----------------------+\n</code></pre></p> <p>Training Logs: <pre><code>Epoch 1/10: Train Loss=4.2089, Val Loss=3.5575, Val C-index=0.5234\nEpoch 2/10: Train Loss=3.8123, Val Loss=3.2341, Val C-index=0.5891\nEpoch 3/10: Train Loss=3.5234, Val Loss=3.0123, Val C-index=0.6234\n...\n</code></pre></p>"},{"location":"runpods_training_guide/#9-save-results","title":"9. Save Results","text":"<p>Save Model Weights: <pre><code># In notebook, add cell:\ntorch.save(model.state_dict(), 'survival_lstm_1000patients.pth')\n</code></pre></p> <p>Save Training History: <pre><code>import pickle\nwith open('training_history.pkl', 'wb') as f:\n    pickle.dump(history, f)\n</code></pre></p> <p>Download Results: <pre><code># From local machine\nscp -P &lt;port&gt; root@&lt;pod-ip&gt;:/workspace/ehr-sequencing/survival_lstm_1000patients.pth ./\nscp -P &lt;port&gt; root@&lt;pod-ip&gt;:/workspace/ehr-sequencing/training_history.pkl ./\n</code></pre></p>"},{"location":"runpods_training_guide/#10-stop-pod","title":"10. Stop Pod","text":"<p>Important: Stop pod when done to avoid charges!</p> <ol> <li>Go to RunPods dashboard</li> <li>Click \"Stop\" on your pod</li> <li>Verify pod is stopped (status: \"Stopped\")</li> <li>Download any remaining files before terminating</li> </ol>"},{"location":"runpods_training_guide/#cost-estimation","title":"Cost Estimation","text":""},{"location":"runpods_training_guide/#training-time-estimates","title":"Training Time Estimates","text":"Dataset Epochs Time per Epoch Total Time Cost (RTX 3090 @ $0.30/hr) 200 patients 10 2 min 20 min $0.10 500 patients 10 5 min 50 min $0.25 1,000 patients 10 10 min 100 min $0.50 2,000 patients 20 15 min 300 min $1.50"},{"location":"runpods_training_guide/#budget-planning","title":"Budget Planning","text":"<p>Development Phase (testing, debugging): - Budget: $5-10 - Duration: 2-3 days - Usage: Multiple short runs (10-30 min each)</p> <p>Training Phase (final models): - Budget: $10-20 - Duration: 1-2 days - Usage: Few long runs (1-2 hours each)</p> <p>Production (ongoing): - Budget: $50-100/month - Usage: Weekly retraining on new data</p>"},{"location":"runpods_training_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"runpods_training_guide/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Error: <code>RuntimeError: CUDA out of memory</code></p> <p>Solutions: 1. Reduce batch size: <code>batch_size = 16</code> or <code>8</code> 2. Reduce model size: <code>embedding_dim=64, hidden_dim=128</code> 3. Enable gradient checkpointing 4. Use mixed precision training (FP16)</p> <pre><code># Mixed precision training\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\n# In training loop:\nwith autocast():\n    hazards = model(visit_codes, visit_mask, sequence_mask)\n    loss = criterion(hazards, event_times, event_indicators, sequence_mask)\n\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n</code></pre>"},{"location":"runpods_training_guide/#slow-training","title":"Slow Training","text":"<p>Symptom: &lt;1 it/s, hours per epoch</p> <p>Solutions: 1. Check GPU utilization: <code>nvidia-smi</code> (should be &gt;80%) 2. Increase batch size if memory allows 3. Use DataLoader with <code>num_workers=4</code> 4. Pin memory: <code>DataLoader(..., pin_memory=True)</code></p>"},{"location":"runpods_training_guide/#connection-lost","title":"Connection Lost","text":"<p>Symptom: SSH/Jupyter disconnects during training</p> <p>Solutions: 1. Use <code>screen</code> or <code>tmux</code> for persistent sessions 2. Save checkpoints every epoch 3. Enable auto-resume from last checkpoint</p> <pre><code># Save checkpoint every epoch\nif (epoch + 1) % 1 == 0:\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'history': history,\n    }, f'checkpoint_epoch_{epoch+1}.pth')\n</code></pre>"},{"location":"runpods_training_guide/#data-transfer-issues","title":"Data Transfer Issues","text":"<p>Symptom: Slow upload/download speeds</p> <p>Solutions: 1. Compress data: <code>tar -czf data.tar.gz data/</code> 2. Use cloud storage (S3, GCS) as intermediate 3. Use <code>rsync</code> instead of <code>scp</code> for resumable transfers</p> <pre><code># Resumable transfer\nrsync -avz --progress -e \"ssh -p &lt;port&gt;\" \\\n  ./data/ root@&lt;pod-ip&gt;:/workspace/data/\n</code></pre>"},{"location":"runpods_training_guide/#best-practices","title":"Best Practices","text":""},{"location":"runpods_training_guide/#1-start-small-scale-up","title":"1. Start Small, Scale Up","text":"<pre><code># First run: Test with subset\nMAX_PATIENTS = 100  # Quick validation\n\n# Second run: Medium dataset\nMAX_PATIENTS = 500  # Verify scaling\n\n# Final run: Full dataset\nMAX_PATIENTS = None  # Production training\n</code></pre>"},{"location":"runpods_training_guide/#2-use-version-control","title":"2. Use Version Control","text":"<pre><code># Track experiments\ngit checkout -b experiment/1000-patients-lstm\n# ... make changes ...\ngit commit -m \"Train on 1000 patients, C-index=0.68\"\ngit tag v1.0-1000patients\n</code></pre>"},{"location":"runpods_training_guide/#3-log-everything","title":"3. Log Everything","text":"<pre><code>import logging\nlogging.basicConfig(\n    filename='training.log',\n    level=logging.INFO,\n    format='%(asctime)s - %(message)s'\n)\n\nlogging.info(f\"Starting training with {len(sequences)} patients\")\nlogging.info(f\"Vocab size: {builder.vocabulary_size}\")\n# ... log metrics each epoch ...\n</code></pre>"},{"location":"runpods_training_guide/#4-save-intermediate-results","title":"4. Save Intermediate Results","text":"<pre><code># Save every 5 epochs\nif (epoch + 1) % 5 == 0:\n    torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n\n# Save best model\nif val_c_index &gt; best_c_index:\n    best_c_index = val_c_index\n    torch.save(model.state_dict(), 'best_model.pth')\n</code></pre>"},{"location":"runpods_training_guide/#5-monitor-costs","title":"5. Monitor Costs","text":"<ul> <li>Set spending alerts in RunPods dashboard</li> <li>Stop pods immediately after training</li> <li>Use spot instances for non-urgent training (50% cheaper)</li> </ul>"},{"location":"runpods_training_guide/#alternative-cloud-providers","title":"Alternative Cloud Providers","text":""},{"location":"runpods_training_guide/#vastai","title":"Vast.ai","text":"<ul> <li>Pros: Often cheaper than RunPods</li> <li>Cons: Less reliable, more setup required</li> <li>Price: RTX 3090 @ $0.20-0.30/hr</li> </ul>"},{"location":"runpods_training_guide/#google-colab-pro","title":"Google Colab Pro","text":"<ul> <li>Pros: Familiar interface, easy setup</li> <li>Cons: Session limits (12-24 hours), shared resources</li> <li>Price: $10/month for Pro</li> </ul>"},{"location":"runpods_training_guide/#lambda-labs","title":"Lambda Labs","text":"<ul> <li>Pros: Dedicated GPUs, good for long training</li> <li>Cons: Higher minimum commitment</li> <li>Price: RTX 3090 @ $0.50/hr</li> </ul>"},{"location":"runpods_training_guide/#awsgcpazure","title":"AWS/GCP/Azure","text":"<ul> <li>Pros: Enterprise-grade, scalable</li> <li>Cons: Complex setup, expensive</li> <li>Price: V100 @ $2-3/hr, A100 @ $4-6/hr</li> </ul>"},{"location":"runpods_training_guide/#summary","title":"Summary","text":"<p>For our survival LSTM training: - Local: 100-200 patients, testing/debugging - RunPods RTX 3090: 500-1,000 patients, optimal cost/performance - RunPods RTX 4090: 1,000-2,000 patients, faster training - RunPods A100: 2,000+ patients or multi-GPU training</p> <p>Estimated total cost for complete project: $10-30 - Development/testing: $5-10 - Final training runs: $5-10 - Hyperparameter tuning: $5-10</p> <p>Time to results: 2-4 hours of actual training time spread over 1-2 days of development.</p>"},{"location":"data_generation/","title":"Data Generation Documentation","text":"<p>This directory contains comprehensive guides for generating synthetic patient data using Synthea for EHR sequence modeling and survival analysis.</p>"},{"location":"data_generation/#documents","title":"Documents","text":""},{"location":"data_generation/#1-data-generation-guide","title":"1. Data Generation Guide","text":"<p>Purpose: Main guide for generating synthetic patient data with Synthea</p> <p>Topics covered: - Why synthetic data is needed for EHR deep learning - Dataset size recommendations for different use cases - Installing and setting up Synthea - Generating different patient cohorts - Configuring Synthea for specific conditions - Integrating generated data into training pipelines - Expected model performance by dataset size</p> <p>When to use: Start here for overview and general guidance on data generation.</p>"},{"location":"data_generation/#2-synthea-csv-export-troubleshooting","title":"2. Synthea CSV Export Troubleshooting","text":"<p>Purpose: Detailed troubleshooting guide for CSV export issues</p> <p>Topics covered: - Why CSV files may not be generated despite configuration - Root cause analysis of configuration precedence - Reliable solutions using command-line flags - Verification steps and best practices - Common pitfalls and how to avoid them - Lessons learned from trial-and-error debugging</p> <p>When to use: Reference this when CSV files are not being generated, or to understand the correct way to ensure CSV export.</p>"},{"location":"data_generation/#quick-start","title":"Quick Start","text":""},{"location":"data_generation/#generate-1000-patients-csv-format","title":"Generate 1000 Patients (CSV Format)","text":"<pre><code>cd ~/work/synthea\n\n# Clean output directory\nrm -rf output/csv/*\n\n# Generate with explicit CSV export\njava -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  -p 1000\n\n# Verify success\nwc -l output/csv/patients.csv  # Should show 1001 (1000 + header)\ndu -sh output/csv/              # Should be ~30-50 MB\n\n# Copy to project\ncp -r output/csv/* ~/work/loinc-predictor/data/synthea/large_cohort_1000/\n</code></pre>"},{"location":"data_generation/#key-lessons","title":"Key Lessons","text":""},{"location":"data_generation/#configuration-precedence","title":"Configuration Precedence","text":"<p>Command-line arguments &gt; Local properties file &gt; Embedded JAR defaults</p> <p>Always use command-line flags to ensure settings are applied.</p>"},{"location":"data_generation/#verification-is-critical","title":"Verification is Critical","text":"<p>Never assume data was generated in the expected format. Always verify: - CSV directory exists - Files have reasonable sizes - Patient count matches expectation</p>"},{"location":"data_generation/#document-your-process","title":"Document Your Process","text":"<p>Save the exact commands used for reproducibility and debugging.</p>"},{"location":"data_generation/#common-use-cases","title":"Common Use Cases","text":""},{"location":"data_generation/#small-dataset-for-testing-100-patients","title":"Small Dataset for Testing (100 patients)","text":"<pre><code>java -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  -p 100\n</code></pre>"},{"location":"data_generation/#medium-dataset-for-development-1000-patients","title":"Medium Dataset for Development (1000 patients)","text":"<pre><code>java -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  -p 1000\n</code></pre>"},{"location":"data_generation/#large-dataset-for-training-10000-patients","title":"Large Dataset for Training (10000 patients)","text":"<pre><code>java -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  -p 10000\n</code></pre>"},{"location":"data_generation/#disease-specific-cohort-ckd-patients","title":"Disease-Specific Cohort (CKD patients)","text":"<pre><code>java -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  -p 1000 \\\n  -m chronic_kidney_disease\n</code></pre>"},{"location":"data_generation/#related-documentation","title":"Related Documentation","text":"<ul> <li>Pretrained Embeddings Guide - Using pretrained medical code embeddings</li> <li>Survival Analysis Methods - Causal survival analysis theory</li> <li>Synthea Official Wiki - Comprehensive Synthea documentation</li> </ul>"},{"location":"data_generation/#troubleshooting-decision-tree","title":"Troubleshooting Decision Tree","text":"<pre><code>CSV files not generated?\n\u2502\n\u251c\u2500 Are FHIR files being created instead?\n\u2502  \u2514\u2500 YES \u2192 See \"Synthea CSV Export Troubleshooting\" guide\n\u2502\n\u251c\u2500 Is Synthea running without errors?\n\u2502  \u251c\u2500 NO \u2192 Check Java installation and JAR file\n\u2502  \u2514\u2500 YES \u2192 Use command-line flags instead of properties file\n\u2502\n\u2514\u2500 Are you in the correct directory?\n   \u2514\u2500 Run: cd ~/work/synthea\n</code></pre>"},{"location":"data_generation/#best-practices-summary","title":"Best Practices Summary","text":"<ol> <li>Use explicit command-line flags for export format control</li> <li>Clean output directory before each generation</li> <li>Verify output immediately after generation</li> <li>Document commands in scripts or logs</li> <li>Test with small datasets before scaling up</li> <li>Check file sizes to ensure data was actually generated</li> </ol>"},{"location":"data_generation/#support","title":"Support","text":"<p>If you encounter issues not covered in these guides:</p> <ol> <li>Check the Synthea GitHub Issues</li> <li>Review the Synthea Wiki</li> <li>Verify your Java version: <code>java -version</code> (requires Java 11+)</li> <li>Check Synthea version and update if needed</li> </ol>"},{"location":"data_generation/#contributing","title":"Contributing","text":"<p>When adding new data generation documentation:</p> <ul> <li>Place files in this directory</li> <li>Update this README with links and descriptions</li> <li>Include practical examples and code snippets</li> <li>Document lessons learned from troubleshooting</li> <li>Add to the \"Common Use Cases\" section if applicable</li> </ul>"},{"location":"data_generation/data_generation_guide/","title":"Generating Synthea Data for EHR Sequence Modeling","text":""},{"location":"data_generation/data_generation_guide/#overview","title":"Overview","text":"<p>Deep learning models for EHR sequences require substantial training data to learn meaningful patterns. While small datasets (100-200 patients) are useful for rapid prototyping and development, production models typically need:</p> <ul> <li>Minimum: 1,000+ patients for basic performance</li> <li>Recommended: 5,000-10,000 patients for robust models</li> <li>Optimal: 50,000+ patients for state-of-the-art results</li> </ul> <p>This guide shows how to generate synthetic EHR data at scale using Synthea, a realistic patient generator that creates complete medical histories with encounters, conditions, procedures, medications, and observations.</p>"},{"location":"data_generation/data_generation_guide/#option-1-generate-new-synthea-data-recommended","title":"Option 1: Generate New Synthea Data (Recommended)","text":""},{"location":"data_generation/data_generation_guide/#install-synthea","title":"Install Synthea","text":"<pre><code># Download Synthea\ncd ~/work/data\nwget https://github.com/synthetichealth/synthea/releases/download/master-branch-latest/synthea-with-dependencies.jar\n\n# Or clone and build from source\ngit clone https://github.com/synthetichealth/synthea.git\ncd synthea\n./gradlew build check test\n</code></pre>"},{"location":"data_generation/data_generation_guide/#generate-1000-patients","title":"Generate 1000+ Patients","text":"<pre><code># Generate 1000 patients\njava -jar synthea-with-dependencies.jar -p 1000\n\n# Output will be in ./output/csv/\n# Move to your data directory\nmv output/csv ~/work/loinc-predictor/data/synthea/large_cohort/\n</code></pre>"},{"location":"data_generation/data_generation_guide/#generate-with-specific-conditions","title":"Generate with Specific Conditions","text":"<p>For disease progression modeling, generate patients with specific conditions:</p> <pre><code># CKD patients\njava -jar synthea-with-dependencies.jar \\\n  -p 500 \\\n  -m chronic_kidney_disease\n\n# Diabetes patients\njava -jar synthea-with-dependencies.jar \\\n  -p 500 \\\n  -m diabetes\n\n# Combine multiple cohorts\nmkdir ~/work/loinc-predictor/data/synthea/combined_1000/\ncat large_cohort/patients.csv &gt; combined_1000/patients.csv\ncat large_cohort/encounters.csv &gt; combined_1000/encounters.csv\n# ... repeat for other files\n</code></pre>"},{"location":"data_generation/data_generation_guide/#configuration-options","title":"Configuration Options","text":"<p>Edit <code>src/main/resources/synthea.properties</code>:</p> <pre><code># Generate more realistic data\nexporter.years_of_history = 10\n\n# Include more conditions\ngenerate.only_alive_patients = false\ngenerate.append_numbers_to_person_names = true\n\n# Increase prevalence of chronic conditions\ngenerate.chronic_kidney_disease.prevalence = 0.15\ngenerate.diabetes.prevalence = 0.20\n</code></pre>"},{"location":"data_generation/data_generation_guide/#option-2-use-public-synthea-datasets","title":"Option 2: Use Public Synthea Datasets","text":""},{"location":"data_generation/data_generation_guide/#syntheticmass-dataset","title":"SyntheticMass Dataset","text":"<p>Large pre-generated Synthea dataset:</p> <pre><code># Download SyntheticMass (1M+ patients)\nwget https://synthea.mitre.org/downloads/synthea_sample_data_csv_apr2020.zip\n\n# Extract specific subset\nunzip synthea_sample_data_csv_apr2020.zip\nhead -n 1001 csv/patients.csv &gt; subset_1000/patients.csv\n# Filter other files by patient IDs\n</code></pre>"},{"location":"data_generation/data_generation_guide/#mitre-synthea-downloads","title":"MITRE Synthea Downloads","text":"<ul> <li>https://synthea.mitre.org/downloads</li> <li>Pre-generated datasets available</li> <li>Various sizes and configurations</li> </ul>"},{"location":"data_generation/data_generation_guide/#option-3-use-real-de-identified-ehr-data","title":"Option 3: Use Real De-identified EHR Data","text":"<p>If available, use real de-identified data:</p> <ul> <li>MIMIC-III/IV (ICU data)</li> <li>eICU Collaborative Research Database</li> <li>UK Biobank</li> <li>All of Us Research Program</li> </ul> <p>Advantages: - Real clinical patterns - Better generalization - Meaningful outcomes</p> <p>Requirements: - IRB approval - Data use agreements - Privacy compliance</p>"},{"location":"data_generation/data_generation_guide/#updating-the-training-pipeline","title":"Updating the Training Pipeline","text":"<p>Once you have more data, update the data path:</p> <pre><code># In notebook or training script\ndata_dir = Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'large_cohort'\n\n# Or combine multiple cohorts\ndata_dirs = [\n    Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'cohort1',\n    Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'cohort2',\n    Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'cohort3',\n]\n\nall_events = []\nfor data_dir in data_dirs:\n    adapter = SyntheaAdapter(data_dir)\n    events = adapter.load_events()\n    all_events.extend(events)\n</code></pre>"},{"location":"data_generation/data_generation_guide/#expected-performance-by-dataset-size","title":"Expected Performance by Dataset Size","text":"<p>The relationship between dataset size and model performance for survival analysis:</p> Dataset Size Expected C-index Training Time Use Case 100-200 patients 0.45-0.55 5-10 min Development/debugging 500 patients 0.55-0.65 20-30 min Initial experiments 1,000 patients 0.60-0.70 45-60 min Baseline models 5,000 patients 0.65-0.75 3-4 hours Production models 10,000+ patients 0.70-0.80 8-10 hours State-of-the-art <p>Notes: - Performance estimates assume well-defined outcomes and sufficient event rates - With pretrained embeddings, expect +0.05-0.10 improvement in C-index - Training time varies by hardware (estimates for single GPU)</p>"},{"location":"data_generation/data_generation_guide/#next-steps","title":"Next Steps","text":"<ol> <li>Generate/download more data (this guide)</li> <li>Implement pretrained embeddings (Phase 2)</li> <li>Retrain with larger dataset</li> <li>Evaluate performance improvement</li> </ol>"},{"location":"data_generation/data_generation_guide/#resources","title":"Resources","text":"<ul> <li>Synthea GitHub: https://github.com/synthetichealth/synthea</li> <li>Synthea Wiki: https://github.com/synthetichealth/synthea/wiki</li> <li>SyntheticMass: https://synthea.mitre.org/downloads</li> <li>Synthea Module Builder: https://synthetichealth.github.io/module-builder/</li> </ul>"},{"location":"data_generation/synthea_csv_export_troubleshooting/","title":"Synthea CSV Export Troubleshooting Guide","text":""},{"location":"data_generation/synthea_csv_export_troubleshooting/#problem-statement","title":"Problem Statement","text":"<p>When generating synthetic patient data with Synthea, CSV files may not be exported even when <code>exporter.csv.export = true</code> is set in the <code>synthea.properties</code> configuration file. This document explains why this happens and provides reliable solutions.</p>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#symptoms","title":"Symptoms","text":"<ul> <li>Synthea runs successfully and reports generating patients (e.g., \"You've just generated 1151 patients!\")</li> <li>FHIR JSON files are created in <code>output/fhir/</code> directory</li> <li>CSV directory (<code>output/csv/</code>) is either empty or not created</li> <li>No error messages indicate CSV export failure</li> </ul>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#root-cause-analysis","title":"Root Cause Analysis","text":""},{"location":"data_generation/synthea_csv_export_troubleshooting/#why-configuration-files-may-be-ignored","title":"Why Configuration Files May Be Ignored","text":"<p>Synthea's configuration system has a specific precedence order:</p> <ol> <li>Command-line arguments (highest priority)</li> <li>Local <code>synthea.properties</code> file (in working directory)</li> <li>Embedded default properties (in JAR file)</li> </ol> <p>Key Issue: When running Synthea with <code>java -jar synthea-with-dependencies.jar</code>, the JAR may contain embedded default properties that override your local <code>synthea.properties</code> file, especially if:</p> <ul> <li>The JAR was built with specific export settings</li> <li>The properties file path is not correctly resolved</li> <li>The working directory doesn't match expectations</li> </ul>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#what-we-learned","title":"What We Learned","text":"<p>Through multiple trials, we discovered:</p> <ol> <li> <p>Properties files are not always read: The <code>synthea.properties</code> file in the working directory may be ignored if the JAR has embedded defaults or if there's a path resolution issue.</p> </li> <li> <p>FHIR is often the default: Many Synthea distributions default to FHIR export only, as FHIR is the modern healthcare data standard.</p> </li> <li> <p>Silent failures: Synthea doesn't warn when it ignores export format settings - it simply exports in the default format.</p> </li> <li> <p>Command-line flags are reliable: Using <code>--config*=value</code> flags guarantees the setting is applied, bypassing any configuration file issues.</p> </li> </ol>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#solution-command-line-configuration-override","title":"Solution: Command-Line Configuration Override","text":""},{"location":"data_generation/synthea_csv_export_troubleshooting/#recommended-approach","title":"Recommended Approach","text":"<p>Always use command-line flags to explicitly control export formats:</p> <pre><code>cd ~/work/synthea\n\n# Generate 1000 patients with CSV export only\njava -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  -p 1000\n</code></pre>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#why-this-works","title":"Why This Works","text":"<ul> <li>Explicit control: Command-line arguments have the highest precedence</li> <li>No ambiguity: Settings are visible in the command itself</li> <li>Reproducible: Anyone can see exactly what settings were used</li> <li>Portable: Works regardless of local configuration files</li> </ul>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#additional-export-control-options","title":"Additional Export Control Options","text":"<pre><code># CSV only (recommended for EHR sequence modeling)\njava -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  --exporter.ccda.export=false \\\n  --exporter.text.export=false \\\n  -p 1000\n\n# Both CSV and FHIR (for interoperability testing)\njava -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=true \\\n  -p 1000\n\n# CSV with specific output directory\njava -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  --exporter.baseDirectory=./custom_output/ \\\n  -p 1000\n</code></pre>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#verification-steps","title":"Verification Steps","text":"<p>After running Synthea, verify CSV export was successful:</p> <pre><code># Check if CSV directory exists and contains files\nls -lh ~/work/synthea/output/csv/\n\n# Count patients in CSV file (should be N+1 for N patients due to header)\nwc -l ~/work/synthea/output/csv/patients.csv\n\n# Check file sizes (CSV files should be substantial, not empty)\ndu -sh ~/work/synthea/output/csv/\n</code></pre> <p>Expected output for 1,000 patients: - <code>patients.csv</code>: ~300-400 KB - <code>encounters.csv</code>: ~2-5 MB - <code>observations.csv</code>: ~10-20 MB - Total directory size: ~30-50 MB</p>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#troubleshooting-workflow","title":"Troubleshooting Workflow","text":""},{"location":"data_generation/synthea_csv_export_troubleshooting/#step-1-verify-synthea-installation","title":"Step 1: Verify Synthea Installation","text":"<pre><code>cd ~/work/synthea\njava -jar synthea-with-dependencies.jar --help\n</code></pre> <p>Should display help text without errors.</p>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#step-2-test-with-minimal-command","title":"Step 2: Test with Minimal Command","text":"<pre><code># Generate 10 patients with explicit CSV export\njava -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  -p 10\n</code></pre>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#step-3-check-output","title":"Step 3: Check Output","text":"<pre><code>ls -la output/csv/\n</code></pre> <p>If CSV files appear, the issue was configuration precedence.</p>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#step-4-scale-up","title":"Step 4: Scale Up","text":"<p>Once verified, generate full dataset:</p> <pre><code># Clean previous output\nrm -rf output/*\n\n# Generate desired number of patients\njava -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  -p 1000\n</code></pre>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#common-pitfalls-and-solutions","title":"Common Pitfalls and Solutions","text":""},{"location":"data_generation/synthea_csv_export_troubleshooting/#pitfall-1-assuming-properties-file-works","title":"Pitfall 1: Assuming Properties File Works","text":"<p>Problem: Editing <code>synthea.properties</code> but seeing no effect.</p> <p>Solution: Use command-line flags instead of relying on properties file.</p> <p>Why: JAR may have embedded defaults or path resolution issues.</p>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#pitfall-2-not-verifying-output-format","title":"Pitfall 2: Not Verifying Output Format","text":"<p>Problem: Assuming CSV was generated without checking.</p> <p>Solution: Always verify with <code>ls</code> and <code>wc -l</code> commands.</p> <p>Why: Synthea doesn't fail or warn when exporting different format than expected.</p>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#pitfall-3-reusing-old-output-directory","title":"Pitfall 3: Reusing Old Output Directory","text":"<p>Problem: Mixing old and new data in same output directory.</p> <p>Solution: Clear output directory before each generation:</p> <pre><code>rm -rf ~/work/synthea/output/*\n</code></pre> <p>Why: Synthea may append or skip existing files depending on configuration.</p>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#pitfall-4-wrong-working-directory","title":"Pitfall 4: Wrong Working Directory","text":"<p>Problem: Running Synthea from different directory than where JAR is located.</p> <p>Solution: Always <code>cd</code> to Synthea directory first:</p> <pre><code>cd ~/work/synthea\njava -jar synthea-with-dependencies.jar [options]\n</code></pre> <p>Why: Relative paths in configuration may break if working directory is wrong.</p>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#best-practices","title":"Best Practices","text":""},{"location":"data_generation/synthea_csv_export_troubleshooting/#1-use-explicit-command-line-flags","title":"1. Use Explicit Command-Line Flags","text":"<pre><code># Good: Explicit and clear\njava -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  -p 1000\n\n# Avoid: Relying on properties file\n# (Just running with -p 1000 and hoping properties file works)\n</code></pre>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#2-document-your-generation-command","title":"2. Document Your Generation Command","text":"<p>Save the exact command used in a script or README:</p> <pre><code># generate_data.sh\n#!/bin/bash\ncd ~/work/synthea\nrm -rf output/csv/*\njava -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  -p 1000 \\\n  2&gt;&amp;1 | tee generation.log\n</code></pre>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#3-verify-before-moving-data","title":"3. Verify Before Moving Data","text":"<pre><code># Generate\njava -jar synthea-with-dependencies.jar [options]\n\n# Verify\necho \"Checking CSV output...\"\nls -lh output/csv/patients.csv\nwc -l output/csv/patients.csv\n\n# Only move if verification passes\nif [ -f output/csv/patients.csv ]; then\n    cp -r output/csv/* ~/work/loinc-predictor/data/synthea/large_cohort_1000/\n    echo \"Data copied successfully\"\nelse\n    echo \"ERROR: CSV files not generated!\"\n    exit 1\nfi\n</code></pre>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#4-keep-generation-logs","title":"4. Keep Generation Logs","text":"<pre><code>java -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  -p 1000 \\\n  2&gt;&amp;1 | tee synthea_generation_$(date +%Y%m%d_%H%M%S).log\n</code></pre> <p>This helps debug issues and provides a record of what was generated.</p>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#understanding-syntheas-export-system","title":"Understanding Synthea's Export System","text":""},{"location":"data_generation/synthea_csv_export_troubleshooting/#export-formats-available","title":"Export Formats Available","text":"<ul> <li>CSV: Tabular format, best for data analysis and ML pipelines</li> <li>FHIR: JSON format, healthcare interoperability standard</li> <li>CCDA: XML format, clinical document architecture</li> <li>Text: Human-readable clinical notes</li> <li>CPCDS: Claims data format</li> </ul>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#why-csv-for-ehr-sequence-modeling","title":"Why CSV for EHR Sequence Modeling?","text":"<ol> <li>Easy to parse: Standard pandas/CSV libraries work out of the box</li> <li>Efficient: Smaller file sizes than JSON</li> <li>Relational: Natural fit for patient-visit-event hierarchy</li> <li>Familiar: Most data scientists are comfortable with CSV</li> <li>Fast loading: Faster than parsing nested JSON structures</li> </ol>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#when-to-use-other-formats","title":"When to Use Other Formats","text":"<ul> <li>FHIR: When testing FHIR-based pipelines or interoperability</li> <li>Both CSV + FHIR: When you need both analysis and standards compliance</li> <li>Text: When working with NLP models on clinical notes</li> </ul>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#summary-of-lessons-learned","title":"Summary of Lessons Learned","text":"<ol> <li>Configuration precedence matters: Command-line &gt; local file &gt; embedded defaults</li> <li>Explicit is better than implicit: Always specify export format explicitly</li> <li>Verify, don't assume: Check output before proceeding</li> <li>Document your process: Save commands and logs for reproducibility</li> <li>Clean slate approach: Clear output directory to avoid confusion</li> <li>Test small first: Generate 10 patients before generating 1000</li> </ol>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#quick-reference","title":"Quick Reference","text":""},{"location":"data_generation/synthea_csv_export_troubleshooting/#generate-csv-only-most-common","title":"Generate CSV Only (Most Common)","text":"<pre><code>cd ~/work/synthea\nrm -rf output/csv/*\njava -jar synthea-with-dependencies.jar \\\n  --exporter.csv.export=true \\\n  --exporter.fhir.export=false \\\n  -p 1000\n</code></pre>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#verify-success","title":"Verify Success","text":"<pre><code>wc -l output/csv/patients.csv  # Should be 1001 (1000 + header)\ndu -sh output/csv/              # Should be ~30-50 MB\n</code></pre>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#copy-to-project","title":"Copy to Project","text":"<pre><code>cp -r output/csv/* ~/work/loinc-predictor/data/synthea/large_cohort_1000/\n</code></pre>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#related-documentation","title":"Related Documentation","text":"<ul> <li>Data Generation Guide - Overview of generating synthetic data</li> <li>Synthea Documentation - Official Synthea wiki</li> <li>Synthea Configuration - Configuration system details</li> </ul>"},{"location":"data_generation/synthea_csv_export_troubleshooting/#troubleshooting-checklist","title":"Troubleshooting Checklist","text":"<ul> <li> Synthea JAR file exists and is executable</li> <li> Running from correct directory (<code>cd ~/work/synthea</code>)</li> <li> Using command-line flags for export format</li> <li> Output directory is clean or cleared</li> <li> Verified CSV files are created after generation</li> <li> Checked file sizes are reasonable (not empty)</li> <li> Patient count matches expected (N+1 lines in patients.csv)</li> <li> Documented exact command used for reproducibility</li> </ul>"},{"location":"datasets/","title":"Dataset Documentation","text":"<p>This directory contains guides for obtaining and using datasets with the EHR sequencing project.</p>"},{"location":"datasets/#available-guides","title":"Available Guides","text":""},{"location":"datasets/#synthea-synthetic-data","title":"Synthea Synthetic Data","text":"<ul> <li>SYNTHEA_SETUP.md - Complete guide for generating synthetic patient data</li> <li>Installation instructions</li> <li>Data generation commands</li> <li>Configuration options</li> <li>Troubleshooting</li> </ul>"},{"location":"datasets/#quick-start","title":"Quick Start","text":""},{"location":"datasets/#using-shared-data-recommended","title":"Using Shared Data (Recommended)","text":"<p>If you have the <code>loinc-predictor</code> project with Synthea data already generated:</p> <pre><code>from pathlib import Path\nfrom ehrsequencing.data.adapters import SyntheaAdapter\n\n# Use shared data directory\ndata_path = Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'all_cohorts'\nadapter = SyntheaAdapter(data_path=str(data_path))\n</code></pre>"},{"location":"datasets/#generating-new-data","title":"Generating New Data","text":"<pre><code># See SYNTHEA_SETUP.md for detailed instructions\ncd ~/synthea\njava -jar synthea-with-dependencies.jar -p 1000 -c synthea.properties\ncp -r output/csv/* ~/work/ehr-sequencing/data/synthea/\n</code></pre>"},{"location":"datasets/#data-requirements","title":"Data Requirements","text":"<p>The <code>SyntheaAdapter</code> requires the following CSV files:</p> <ul> <li><code>patients.csv</code> - Patient demographics</li> <li><code>encounters.csv</code> - Healthcare encounters</li> <li><code>conditions.csv</code> - Diagnoses (SNOMED codes)</li> <li><code>observations.csv</code> - Lab results and vitals (LOINC codes)</li> <li><code>medications.csv</code> - Prescriptions (RxNorm codes)</li> <li><code>procedures.csv</code> - Medical procedures (SNOMED codes)</li> </ul>"},{"location":"datasets/#dataset-sizes","title":"Dataset Sizes","text":"<p>Recommended dataset sizes for different purposes:</p> Purpose Patients Generation Time Disk Space Testing 100 5 min ~50 MB Development 5,000 30 min ~2 GB Training 25,000 2 hours ~10 GB Large-scale 100,000 Overnight ~40 GB"},{"location":"datasets/#data-sharing-between-projects","title":"Data Sharing Between Projects","text":"<p>Both <code>ehr-sequencing</code> and <code>loinc-predictor</code> use the same <code>SyntheaAdapter</code> and can share data:</p>"},{"location":"datasets/#option-1-symlink","title":"Option 1: Symlink","text":"<pre><code>cd ~/work/ehr-sequencing\nmkdir -p data\nln -s ~/work/loinc-predictor/data/synthea data/synthea\n</code></pre>"},{"location":"datasets/#option-2-absolute-path","title":"Option 2: Absolute Path","text":"<p>Use absolute paths in your code to reference the shared location.</p>"},{"location":"datasets/#option-3-copy","title":"Option 3: Copy","text":"<p>Copy data to each project's <code>data/synthea/</code> directory (uses more disk space).</p>"},{"location":"datasets/#related-documentation","title":"Related Documentation","text":"<ul> <li>Data Pipeline: <code>../implementation/visit-grouped-sequences.md</code></li> <li>Data Exploration: <code>../../notebooks/data-exploration/README.md</code></li> <li>Variable-Length Sequences: <code>../methods/variable-length-sequences.md</code></li> </ul>"},{"location":"datasets/#external-resources","title":"External Resources","text":"<ul> <li>Synthea Project: https://github.com/synthetichealth/synthea</li> <li>Synthea Wiki: https://github.com/synthetichealth/synthea/wiki</li> <li>Module Gallery: https://synthetichealth.github.io/module-builder/</li> <li>LOINC Codes: https://loinc.org/</li> <li>SNOMED CT: https://browser.ihtsdotools.org/</li> </ul>"},{"location":"datasets/SYNTHEA_SETUP/","title":"Synthea Setup and Data Generation Guide","text":"<p>Purpose: Generate synthetic patient data for EHR sequence modeling</p>"},{"location":"datasets/SYNTHEA_SETUP/#quick-start","title":"Quick Start","text":"<p>\u26a0\ufe0f IMPORTANT: Synthea generates FHIR format by default, NOT CSV! You must configure CSV export first.</p>"},{"location":"datasets/SYNTHEA_SETUP/#1-download-synthea","title":"1. Download Synthea","text":"<pre><code>cd ~/Downloads\ncurl -LO https://github.com/synthetichealth/synthea/releases/download/master-branch-latest/synthea-with-dependencies.jar\n</code></pre> <p>Or download manually from: https://github.com/synthetichealth/synthea/releases</p>"},{"location":"datasets/SYNTHEA_SETUP/#2-enable-csv-export-critical-step","title":"2. Enable CSV Export (CRITICAL STEP)","text":"<pre><code># Move JAR to ~/synthea directory\nmkdir -p ~/synthea\nmv synthea-with-dependencies.jar ~/synthea/\ncd ~/synthea\n\n# Create configuration file to enable CSV export\ncat &gt; synthea.properties &lt;&lt; 'EOF'\nexporter.csv.export = true\nexporter.fhir.export = false\nEOF\n</code></pre> <p>Why this is needed: By default, Synthea only generates FHIR JSON files. The <code>SyntheaAdapter</code> requires CSV files, so you must explicitly enable CSV export via the configuration file.</p>"},{"location":"datasets/SYNTHEA_SETUP/#3-generate-data","title":"3. Generate Data","text":"<pre><code># ALWAYS use -c flag to enable CSV export!\n# Generate 1000 patients (quick test)\njava -jar synthea-with-dependencies.jar -p 1000 -c synthea.properties\n\n# Generate with specific disease module\njava -jar synthea-with-dependencies.jar -p 1000 -m diabetes -c synthea.properties\n\n# Generate multiple modules\njava -jar synthea-with-dependencies.jar -p 1000 -m \"diabetes*,cardiovascular*\" -c synthea.properties\n</code></pre> <p>\u26a0\ufe0f Common Mistake: Forgetting the <code>-c synthea.properties</code> flag will generate FHIR files instead of CSV!</p>"},{"location":"datasets/SYNTHEA_SETUP/#4-verify-csv-files-created","title":"4. Verify CSV Files Created","text":"<pre><code># Check that CSV directory exists\nls -lh output/csv/\n\n# Should show:\n# observations.csv\n# patients.csv\n# encounters.csv\n# conditions.csv\n# medications.csv\n# procedures.csv\n# etc.\n</code></pre> <p>If you only see <code>output/fhir/</code> directory, you forgot the <code>-c synthea.properties</code> flag!</p>"},{"location":"datasets/SYNTHEA_SETUP/#5-copy-to-project","title":"5. Copy to Project","text":"<pre><code># From ~/synthea directory\n# Copy to ehr-sequencing project\ncp -r output/csv/* ~/work/ehr-sequencing/data/synthea/\n\n# Or use shared data directory (recommended if you have loinc-predictor)\ncp -r output/csv/* ~/work/loinc-predictor/data/synthea/all_cohorts/\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#shared-data-directory-recommended","title":"Shared Data Directory (Recommended)","text":"<p>If you have multiple projects using Synthea data (e.g., <code>loinc-predictor</code> and <code>ehr-sequencing</code>), you can share the same data directory:</p>"},{"location":"datasets/SYNTHEA_SETUP/#option-1-symlink-to-shared-location","title":"Option 1: Symlink to Shared Location","text":"<pre><code># Create shared data directory in loinc-predictor\nmkdir -p ~/work/loinc-predictor/data/synthea/all_cohorts\n\n# Create symlink in ehr-sequencing\ncd ~/work/ehr-sequencing\nmkdir -p data\nln -s ~/work/loinc-predictor/data/synthea data/synthea\n\n# Verify\nls -la data/synthea\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#option-2-use-absolute-path-in-code","title":"Option 2: Use Absolute Path in Code","text":"<pre><code># In notebooks or scripts\ndata_path = Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'all_cohorts'\nadapter = SyntheaAdapter(data_path=str(data_path))\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#detailed-instructions","title":"Detailed Instructions","text":""},{"location":"datasets/SYNTHEA_SETUP/#installation-options","title":"Installation Options","text":""},{"location":"datasets/SYNTHEA_SETUP/#option-a-download-jar-recommended","title":"Option A: Download JAR (Recommended)","text":"<pre><code># Create synthea directory\nmkdir -p ~/synthea\ncd ~/synthea\n\n# Download latest release\ncurl -LO https://github.com/synthetichealth/synthea/releases/download/master-branch-latest/synthea-with-dependencies.jar\n\n# Test installation\njava -jar synthea-with-dependencies.jar --help\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#option-b-build-from-source","title":"Option B: Build from Source","text":"<pre><code># Clone repository\ngit clone https://github.com/synthetichealth/synthea.git\ncd synthea\n\n# Build with Gradle\n./gradlew build check test\n\n# Run\n./run_synthea --help\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#option-c-docker","title":"Option C: Docker","text":"<pre><code># Pull image\ndocker pull synthetichealth/synthea\n\n# Run\ndocker run --rm -v $PWD/output:/output synthetichealth/synthea -p 1000\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#data-generation","title":"Data Generation","text":""},{"location":"datasets/SYNTHEA_SETUP/#basic-generation","title":"Basic Generation","text":"<pre><code># Generate 1000 patients in Massachusetts\njava -jar synthea-with-dependencies.jar -p 1000 -c synthea.properties\n\n# Specify state\njava -jar synthea-with-dependencies.jar -p 1000 -s California -c synthea.properties\n\n# Specify city\njava -jar synthea-with-dependencies.jar -p 1000 -c synthea.properties \"San Francisco\"\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#disease-specific-cohorts","title":"Disease-Specific Cohorts","text":"<pre><code># Diabetes\njava -jar synthea-with-dependencies.jar -p 5000 -m diabetes -c synthea.properties\n\n# Cardiovascular disease\njava -jar synthea-with-dependencies.jar -p 5000 -m cardiovascular_disease -c synthea.properties\n\n# Multiple diseases\njava -jar synthea-with-dependencies.jar -p 10000 -m \"diabetes*,cardiovascular*,kidney*\" -c synthea.properties\n\n# All modules\njava -jar synthea-with-dependencies.jar -p 1000 -m \"*\" -c synthea.properties\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#available-disease-modules","title":"Available Disease Modules","text":"<p>Common modules: - <code>diabetes</code> - Type 2 Diabetes - <code>cardiovascular_disease</code> - Heart disease - <code>kidney_disease</code> - Chronic kidney disease - <code>metabolic_syndrome</code> - Metabolic syndrome - <code>hepatitis_c</code> - Hepatitis C - <code>hypertension</code> - High blood pressure - <code>copd</code> - Chronic obstructive pulmonary disease - <code>asthma</code> - Asthma - <code>lung_cancer</code> - Lung cancer - <code>colorectal_cancer</code> - Colorectal cancer</p> <p>See all modules: https://github.com/synthetichealth/synthea/tree/master/src/main/resources/modules</p>"},{"location":"datasets/SYNTHEA_SETUP/#advanced-options","title":"Advanced Options","text":"<pre><code># Seed for reproducibility\njava -jar synthea-with-dependencies.jar -p 1000 -s 12345 -c synthea.properties\n\n# Keep only patients with specific conditions\njava -jar synthea-with-dependencies.jar -p 1000 -k \"Diabetes\" -c synthea.properties\n\n# Age range\njava -jar synthea-with-dependencies.jar -p 1000 -a 40-65 -c synthea.properties\n\n# Gender\njava -jar synthea-with-dependencies.jar -p 1000 -g M -c synthea.properties  # Male only\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#configuration","title":"Configuration","text":""},{"location":"datasets/SYNTHEA_SETUP/#enable-csv-export","title":"Enable CSV Export","text":"<p>Edit <code>synthea.properties</code>:</p> <pre><code># Enable CSV export\nexporter.csv.export = true\n\n# Disable other formats (optional)\nexporter.fhir.export = false\nexporter.ccda.export = false\n\n# CSV output directory\nexporter.baseDirectory = ./output/\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#output-files","title":"Output Files","text":"<p>Synthea generates CSV files in <code>output/csv/</code>:</p>"},{"location":"datasets/SYNTHEA_SETUP/#key-files-for-ehr-sequencing","title":"Key Files for EHR Sequencing","text":"<ol> <li>patients.csv - Patient demographics</li> <li> <p>Columns: Id, BIRTHDATE, DEATHDATE, SSN, DRIVERS, PASSPORT, PREFIX, FIRST, LAST, GENDER, RACE, etc.</p> </li> <li> <p>encounters.csv - Healthcare encounters</p> </li> <li> <p>Columns: Id, START, STOP, PATIENT, ENCOUNTERCLASS, CODE, DESCRIPTION, REASONCODE, etc.</p> </li> <li> <p>conditions.csv - Patient diagnoses</p> </li> <li> <p>Columns: START, STOP, PATIENT, ENCOUNTER, CODE (SNOMED), DESCRIPTION</p> </li> <li> <p>observations.csv - Laboratory test results and vital signs</p> </li> <li> <p>Columns: DATE, PATIENT, ENCOUNTER, CODE (LOINC), DESCRIPTION, VALUE, UNITS, TYPE</p> </li> <li> <p>medications.csv - Prescribed medications</p> </li> <li> <p>Columns: START, STOP, PATIENT, ENCOUNTER, CODE (RXNORM), DESCRIPTION, REASONCODE</p> </li> <li> <p>procedures.csv - Medical procedures</p> </li> <li> <p>Columns: DATE, PATIENT, ENCOUNTER, CODE (SNOMED), DESCRIPTION, REASONCODE</p> </li> <li> <p>immunizations.csv - Vaccinations</p> </li> </ol> <p>All files are used by the <code>SyntheaAdapter</code> to construct complete patient histories.</p>"},{"location":"datasets/SYNTHEA_SETUP/#recommended-generation-strategy","title":"Recommended Generation Strategy","text":""},{"location":"datasets/SYNTHEA_SETUP/#for-ehr-sequence-modeling","title":"For EHR Sequence Modeling","text":""},{"location":"datasets/SYNTHEA_SETUP/#phase-1-quick-test-5-minutes","title":"Phase 1: Quick Test (5 minutes)","text":"<pre><code># Small dataset for testing pipeline\njava -jar synthea-with-dependencies.jar -p 100 -c synthea.properties\n</code></pre> <p>Use for: Testing adapters, visit grouping, sequence building</p>"},{"location":"datasets/SYNTHEA_SETUP/#phase-2-development-dataset-30-minutes","title":"Phase 2: Development Dataset (30 minutes)","text":"<pre><code># Medium dataset for model development\njava -jar synthea-with-dependencies.jar -p 5000 -c synthea.properties\n</code></pre> <p>Use for: LSTM baseline training, hyperparameter tuning</p>"},{"location":"datasets/SYNTHEA_SETUP/#phase-3-multi-cohort-2-hours","title":"Phase 3: Multi-Cohort (2 hours)","text":"<pre><code># Multiple disease cohorts for diverse patterns\njava -jar synthea-with-dependencies.jar -p 25000 -m \"diabetes*,cardiovascular*,kidney*,metabolic*\" -c synthea.properties\n</code></pre> <p>Use for: Med2Vec embedding training, disease trajectory analysis</p>"},{"location":"datasets/SYNTHEA_SETUP/#phase-4-large-scale-overnight","title":"Phase 4: Large-Scale (overnight)","text":"<pre><code># Comprehensive dataset for final training\njava -jar synthea-with-dependencies.jar -p 100000 -m \"*\" -c synthea.properties\n</code></pre> <p>Use for: Final model training, benchmarking, publication</p>"},{"location":"datasets/SYNTHEA_SETUP/#project-integration","title":"Project Integration","text":""},{"location":"datasets/SYNTHEA_SETUP/#directory-structure","title":"Directory Structure","text":"<pre><code>ehr-sequencing/\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 synthea/           # Symlink to shared data or local copy\n        \u251c\u2500\u2500 test/          # Small test dataset (100 patients)\n        \u251c\u2500\u2500 dev/           # Development dataset (5K patients)\n        \u2514\u2500\u2500 full/          # Full dataset (100K patients)\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#copy-script","title":"Copy Script","text":"<p>Create <code>scripts/copy_synthea_data.sh</code>:</p> <pre><code>#!/bin/bash\n# Copy Synthea output to project\n\nSYNTHEA_DIR=~/synthea/output/csv\nPROJECT_DIR=~/work/ehr-sequencing/data/synthea\n\n# Create directories\nmkdir -p $PROJECT_DIR/test\nmkdir -p $PROJECT_DIR/dev\nmkdir -p $PROJECT_DIR/full\n\n# Copy test data\necho \"Copying test data...\"\ncp $SYNTHEA_DIR/*.csv $PROJECT_DIR/test/\n\necho \"\u2705 Synthea data copied to $PROJECT_DIR\"\nls -lh $PROJECT_DIR/test/\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#validation","title":"Validation","text":""},{"location":"datasets/SYNTHEA_SETUP/#check-generated-data","title":"Check Generated Data","text":"<pre><code># Count records\nwc -l output/csv/patients.csv\nwc -l output/csv/encounters.csv\nwc -l output/csv/conditions.csv\n\n# Check unique patients\ncut -d',' -f2 output/csv/encounters.csv | sort -u | wc -l\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#python-validation","title":"Python Validation","text":"<pre><code>import pandas as pd\nfrom pathlib import Path\n\ndata_path = Path(\"output/csv\")\n\n# Load key files\npatients = pd.read_csv(data_path / \"patients.csv\")\nencounters = pd.read_csv(data_path / \"encounters.csv\")\nconditions = pd.read_csv(data_path / \"conditions.csv\")\nobservations = pd.read_csv(data_path / \"observations.csv\")\n\nprint(f\"Patients: {len(patients)}\")\nprint(f\"Encounters: {len(encounters)}\")\nprint(f\"Conditions: {len(conditions)}\")\nprint(f\"Observations: {len(observations)}\")\n\nprint(f\"\\nEncounters per patient: {len(encounters) / len(patients):.1f}\")\nprint(f\"Conditions per patient: {len(conditions) / len(patients):.1f}\")\n\n# Check date ranges\nencounters['START'] = pd.to_datetime(encounters['START'])\nprint(f\"\\nDate range: {encounters['START'].min()} to {encounters['START'].max()}\")\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#test-with-syntheaadapter","title":"Test with SyntheaAdapter","text":"<pre><code>from ehrsequencing.data.adapters import SyntheaAdapter\n\n# Load data\nadapter = SyntheaAdapter(data_path=\"output/csv\")\n\n# Load patients\npatients = adapter.load_patients()\nprint(f\"Loaded {len(patients)} patients\")\n\n# Load encounters\nencounters = adapter.load_encounters()\nprint(f\"Loaded {len(encounters)} encounters\")\n\n# Test visit grouping\nfrom ehrsequencing.data.visit_grouper import VisitGrouper\n\ngrouper = VisitGrouper(adapter=adapter)\npatient_id = patients['Id'].iloc[0]\nvisits = grouper.group_visits(patient_id)\nprint(f\"\\nPatient {patient_id[:8]}... has {len(visits)} visits\")\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"datasets/SYNTHEA_SETUP/#java-memory-issues","title":"Java Memory Issues","text":"<pre><code># Increase heap size\njava -Xmx4g -jar synthea-with-dependencies.jar -p 10000 -c synthea.properties\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#slow-generation","title":"Slow Generation","text":"<pre><code># Reduce history\njava -jar synthea-with-dependencies.jar -p 10000 --exporter.years_of_history=5 -c synthea.properties\n</code></pre>"},{"location":"datasets/SYNTHEA_SETUP/#problem-only-fhir-files-generated-no-csv-directory","title":"Problem: Only FHIR files generated, no CSV directory","text":"<p>Symptom: <pre><code>ls -la output/\n# Shows only: fhir/ and metadata/\n# Missing: csv/\n</code></pre></p> <p>Solution: You forgot to use the <code>-c synthea.properties</code> flag! Synthea defaults to FHIR export.</p> <ol> <li>Create <code>synthea.properties</code> file (see step 2 above)</li> <li>Re-run with <code>-c</code> flag:    <pre><code>java -jar synthea-with-dependencies.jar -p 100 -c synthea.properties\n</code></pre></li> </ol>"},{"location":"datasets/SYNTHEA_SETUP/#problem-java-not-found-or-version-error","title":"Problem: Java not found or version error","text":"<p>Solution: <pre><code># macOS\nbrew install openjdk@21\necho 'export PATH=\"/opt/homebrew/opt/openjdk@21/bin:$PATH\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n\n# Verify\njava -version\n</code></pre></p>"},{"location":"datasets/SYNTHEA_SETUP/#next-steps","title":"Next Steps","text":"<p>After generating data:</p> <ol> <li> <p>Test the data pipeline: <pre><code>cd ~/work/ehr-sequencing\nmamba activate ehrsequencing\njupyter notebook notebooks/data-exploration/01_synthea_data_exploration.ipynb\n</code></pre></p> </li> <li> <p>Train LSTM baseline: <pre><code>python examples/train_lstm_baseline.py \\\n    --data-path data/synthea/dev \\\n    --output-dir results/lstm_baseline\n</code></pre></p> </li> <li> <p>Implement Med2Vec embeddings (Phase 2)</p> </li> </ol>"},{"location":"datasets/SYNTHEA_SETUP/#resources","title":"Resources","text":"<ul> <li>Synthea Wiki: https://github.com/synthetichealth/synthea/wiki</li> <li>Module Gallery: https://synthetichealth.github.io/module-builder/</li> <li>Disease Modules: https://github.com/synthetichealth/synthea/tree/master/src/main/resources/modules</li> <li>LOINC Browser: https://loinc.org/</li> <li>SNOMED CT Browser: https://browser.ihtsdotools.org/</li> </ul>"},{"location":"datasets/SYNTHEA_SETUP/#quick-reference","title":"Quick Reference","text":"<pre><code># Download\ncurl -LO https://github.com/synthetichealth/synthea/releases/download/master-branch-latest/synthea-with-dependencies.jar\n\n# Setup\nmkdir -p ~/synthea &amp;&amp; mv synthea-with-dependencies.jar ~/synthea/ &amp;&amp; cd ~/synthea\ncat &gt; synthea.properties &lt;&lt; 'EOF'\nexporter.csv.export = true\nexporter.fhir.export = false\nEOF\n\n# Generate test data\njava -jar synthea-with-dependencies.jar -p 100 -c synthea.properties\n\n# Copy to project\ncp -r output/csv/* ~/work/ehr-sequencing/data/synthea/test/\n\n# Test\ncd ~/work/ehr-sequencing\nmamba activate ehrsequencing\njupyter notebook notebooks/data-exploration/01_synthea_data_exploration.ipynb\n</code></pre> <p>Happy data generation! \ud83c\udf89</p>"},{"location":"implementation/resource-aware-models/","title":"Resource-Aware Model Configurations","text":"<p>Date: January 20, 2026 Focus: Small/Medium/Large model presets for M1 MacBook (16GB) and RunPod deployment</p>"},{"location":"implementation/resource-aware-models/#overview","title":"Overview","text":"<p>This document defines resource-aware model configurations for the EHR Sequencing project, optimized for different hardware constraints:</p> <ul> <li>SMALL: M1 MacBook Pro 16GB (local development, fast iteration)</li> <li>MEDIUM: RunPod with 24GB GPU (A10, RTX 4090)</li> <li>LARGE: Cloud instances with 40GB+ GPU (A40, A100)</li> </ul> <p>Key Principle: All models are logically equivalent but scaled for available resources. This enables: - Fast iteration on M1 MacBook - Realistic training on RunPod - Production-scale deployment on cloud</p>"},{"location":"implementation/resource-aware-models/#part-1-lstm-baseline-for-visit-grouped-sequences","title":"Part 1: LSTM Baseline for Visit-Grouped Sequences","text":""},{"location":"implementation/resource-aware-models/#should-we-use-lstm-as-a-baseline","title":"Should We Use LSTM as a Baseline?","text":"<p>Answer: YES - LSTM is an excellent baseline for visit-grouped sequences.</p>"},{"location":"implementation/resource-aware-models/#rationale","title":"Rationale","text":"<p>1. Natural Fit for Visit Sequences <pre><code># Visit sequence is naturally sequential\npatient_trajectory = [visit1, visit2, visit3, ..., visitN]\n\n# LSTM processes sequences step-by-step\nhidden_state = lstm(visit_embeddings)\n</code></pre></p> <p>2. Computational Efficiency - Much faster than Transformers for long sequences - Lower memory footprint - Suitable for M1 MacBook development</p> <p>3. Strong Baseline Performance - LSTMs have proven effective for EHR sequences - Captures temporal dependencies - Easier to interpret than Transformers</p> <p>4. Comparison Point - Establishes baseline performance - Compare against Transformer-based models - Validate that complexity is justified</p>"},{"location":"implementation/resource-aware-models/#lstm-vs-transformer-for-visit-sequences","title":"LSTM vs Transformer for Visit Sequences","text":"Aspect LSTM Transformer Sequence Length Efficient for long sequences (50+ visits) Quadratic complexity O(n\u00b2) Memory O(n) - Linear O(n\u00b2) - Quadratic Training Speed Fast Slower Long-range Deps Limited (vanishing gradients) Excellent (attention) Interpretability Moderate (hidden states) High (attention weights) M1 MacBook \u2705 Runs well \u26a0\ufe0f Slower, more memory Best For Baseline, fast iteration Production, best performance <p>Recommendation: Implement both LSTM and Transformer, use LSTM as baseline.</p>"},{"location":"implementation/resource-aware-models/#part-2-lstm-baseline-architecture","title":"Part 2: LSTM Baseline Architecture","text":""},{"location":"implementation/resource-aware-models/#two-level-lstm-for-visit-grouped-sequences","title":"Two-Level LSTM for Visit-Grouped Sequences","text":"<pre><code>import torch\nimport torch.nn as nn\n\nclass LSTMVisitEncoder(nn.Module):\n    \"\"\"\n    Two-level LSTM for visit-grouped sequences.\n\n    Level 1: Encode codes within each visit\n    Level 2: Model sequence of visits\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        code_embed_dim: int = 128,\n        visit_embed_dim: int = 256,\n        hidden_dim: int = 512,\n        num_layers: int = 2,\n        dropout: float = 0.1,\n        bidirectional: bool = False\n    ):\n        super().__init__()\n\n        # Code embeddings (can use pre-trained)\n        self.code_embeddings = nn.Embedding(vocab_size, code_embed_dim, padding_idx=0)\n\n        # Level 1: Within-visit LSTM\n        self.visit_lstm = nn.LSTM(\n            input_size=code_embed_dim,\n            hidden_size=code_embed_dim,\n            num_layers=1,\n            batch_first=True,\n            dropout=0,\n            bidirectional=False\n        )\n\n        # Project to visit embedding\n        self.visit_projection = nn.Linear(code_embed_dim, visit_embed_dim)\n\n        # Level 2: Visit sequence LSTM\n        self.sequence_lstm = nn.LSTM(\n            input_size=visit_embed_dim + 2,  # +2 for time features\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers &gt; 1 else 0,\n            bidirectional=bidirectional\n        )\n\n        self.dropout = nn.Dropout(dropout)\n        self.hidden_dim = hidden_dim * (2 if bidirectional else 1)\n\n    def encode_visit(self, visit_codes, visit_mask):\n        \"\"\"\n        Encode a single visit.\n\n        Args:\n            visit_codes: [batch, max_codes_per_visit]\n            visit_mask: [batch, max_codes_per_visit]\n\n        Returns:\n            visit_embedding: [batch, visit_embed_dim]\n        \"\"\"\n        # Embed codes\n        code_embeds = self.code_embeddings(visit_codes)  # [batch, codes, embed_dim]\n\n        # LSTM over codes\n        lstm_out, (hidden, _) = self.visit_lstm(code_embeds)\n\n        # Use last hidden state (or mean pooling)\n        visit_repr = hidden[-1]  # [batch, code_embed_dim]\n\n        # Project to visit space\n        return self.visit_projection(visit_repr)\n\n    def forward(self, patient_visits, time_features, visit_mask):\n        \"\"\"\n        Encode patient visit sequence.\n\n        Args:\n            patient_visits: [batch, num_visits, max_codes_per_visit]\n            time_features: [batch, num_visits, 2]\n            visit_mask: [batch, num_visits, max_codes_per_visit]\n\n        Returns:\n            sequence_output: [batch, num_visits, hidden_dim]\n            final_hidden: [num_layers, batch, hidden_dim]\n        \"\"\"\n        batch_size, num_visits, max_codes = patient_visits.shape\n\n        # Encode each visit\n        visit_embeds = []\n        for i in range(num_visits):\n            visit_embed = self.encode_visit(\n                patient_visits[:, i, :],\n                visit_mask[:, i, :]\n            )\n            visit_embeds.append(visit_embed)\n\n        visit_embeds = torch.stack(visit_embeds, dim=1)  # [batch, visits, visit_dim]\n\n        # Concatenate time features\n        visit_embeds_with_time = torch.cat([visit_embeds, time_features], dim=-1)\n\n        # LSTM over visit sequence\n        sequence_output, (final_hidden, final_cell) = self.sequence_lstm(\n            visit_embeds_with_time\n        )\n\n        return sequence_output, final_hidden\n</code></pre>"},{"location":"implementation/resource-aware-models/#disease-progression-model-with-lstm","title":"Disease Progression Model with LSTM","text":"<pre><code>class LSTMProgressionModel(nn.Module):\n    \"\"\"\n    LSTM-based disease progression model.\n\n    Predicts disease stage and time to progression.\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        code_embed_dim: int = 128,\n        visit_embed_dim: int = 256,\n        hidden_dim: int = 512,\n        num_stages: int = 5,\n        num_layers: int = 2,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n\n        # Visit encoder\n        self.encoder = LSTMVisitEncoder(\n            vocab_size=vocab_size,\n            code_embed_dim=code_embed_dim,\n            visit_embed_dim=visit_embed_dim,\n            hidden_dim=hidden_dim,\n            num_layers=num_layers,\n            dropout=dropout,\n            bidirectional=False\n        )\n\n        # Prediction heads\n        self.stage_classifier = nn.Linear(hidden_dim, num_stages)\n        self.time_to_progression = nn.Linear(hidden_dim, 1)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, patient_visits, time_features, visit_mask):\n        \"\"\"\n        Predict disease progression.\n\n        Returns:\n            stage_logits: [batch, num_visits, num_stages]\n            time_pred: [batch, num_visits, 1]\n        \"\"\"\n        # Encode sequence\n        sequence_output, _ = self.encoder(patient_visits, time_features, visit_mask)\n        sequence_output = self.dropout(sequence_output)\n\n        # Predict at each visit\n        stage_logits = self.stage_classifier(sequence_output)\n        time_pred = torch.relu(self.time_to_progression(sequence_output))\n\n        return stage_logits, time_pred\n</code></pre>"},{"location":"implementation/resource-aware-models/#part-3-resource-aware-model-configurations","title":"Part 3: Resource-Aware Model Configurations","text":""},{"location":"implementation/resource-aware-models/#configuration-system","title":"Configuration System","text":"<pre><code># src/ehrsequencing/models/configs/model_configs.py\n\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass EHRModelConfig:\n    \"\"\"Configuration for EHR sequence models.\"\"\"\n\n    # Model architecture\n    code_embed_dim: int\n    visit_embed_dim: int\n    hidden_dim: int\n    num_layers: int\n    dropout: float = 0.1\n\n    # Model type\n    model_type: str = \"lstm\"  # \"lstm\" or \"transformer\"\n\n    # Transformer-specific\n    num_heads: Optional[int] = None\n    use_flash_attention: bool = False\n\n    # Vocabulary\n    vocab_size: int = 10000\n    max_visits: int = 50\n    max_codes_per_visit: int = 100\n\n    # Training\n    batch_size: int = 32\n    gradient_accumulation_steps: int = 1\n    mixed_precision: bool = True\n    use_checkpoint: bool = False\n\n    # Task-specific\n    num_stages: int = 5  # For disease progression\n\n    @property\n    def effective_batch_size(self) -&gt; int:\n        return self.batch_size * self.gradient_accumulation_steps\n\n    @property\n    def total_params_millions(self) -&gt; float:\n        \"\"\"Estimate total parameters in millions.\"\"\"\n        if self.model_type == \"lstm\":\n            # LSTM parameters\n            # Input-to-hidden: 4 * (input_size * hidden_dim + hidden_dim^2)\n            # Per layer\n            code_lstm_params = 4 * (self.code_embed_dim * self.code_embed_dim + \n                                   self.code_embed_dim ** 2)\n\n            visit_lstm_params = 4 * (self.visit_embed_dim * self.hidden_dim + \n                                     self.hidden_dim ** 2) * self.num_layers\n\n            # Embeddings\n            embed_params = self.vocab_size * self.code_embed_dim\n\n            # Projection layers\n            proj_params = self.code_embed_dim * self.visit_embed_dim\n\n            # Prediction heads\n            head_params = self.hidden_dim * (self.num_stages + 1)\n\n            total = (code_lstm_params + visit_lstm_params + embed_params + \n                    proj_params + head_params) / 1e6\n\n        elif self.model_type == \"transformer\":\n            # Transformer parameters (rough estimate)\n            attn_params = 4 * self.visit_embed_dim * self.visit_embed_dim * self.num_layers\n            ffn_params = 2 * self.visit_embed_dim * self.hidden_dim * self.num_layers\n            embed_params = self.vocab_size * self.code_embed_dim\n\n            total = (attn_params + ffn_params + embed_params) / 1e6\n\n        return round(total, 2)\n\n    def memory_estimate_gb(self, dtype_bytes: int = 4) -&gt; float:\n        \"\"\"Estimate memory usage in GB.\"\"\"\n        # Model parameters\n        model_memory = self.total_params_millions * 1e6 * dtype_bytes / 1e9\n\n        # Optimizer states (Adam: 2x)\n        optimizer_memory = model_memory * 2\n\n        # Activations (LSTM is more memory efficient than Transformer)\n        if self.model_type == \"lstm\":\n            activation_memory = (\n                self.batch_size * self.max_visits * self.hidden_dim * \n                self.num_layers * dtype_bytes / 1e9\n            )\n        else:  # transformer\n            activation_memory = (\n                self.batch_size * self.max_visits * self.max_visits * \n                self.num_heads * dtype_bytes / 1e9\n            )\n\n        # Gradients\n        gradient_memory = model_memory\n\n        total = model_memory + optimizer_memory + activation_memory + gradient_memory\n        total *= 1.2  # 20% overhead\n\n        return round(total, 2)\n</code></pre>"},{"location":"implementation/resource-aware-models/#preset-configurations","title":"Preset Configurations","text":"<pre><code># src/ehrsequencing/models/configs/presets.py\n\nfrom .model_configs import EHRModelConfig\n\n# ============================================================================\n# SMALL: M1 MacBook Pro 16GB\n# ============================================================================\n\nSMALL_LSTM_CONFIG = EHRModelConfig(\n    # Architecture\n    code_embed_dim=128,\n    visit_embed_dim=256,\n    hidden_dim=256,\n    num_layers=2,\n    dropout=0.1,\n\n    # Model type\n    model_type=\"lstm\",\n\n    # Vocabulary\n    vocab_size=10000,\n    max_visits=50,\n    max_codes_per_visit=50,\n\n    # Training (optimized for M1 16GB)\n    batch_size=4,\n    gradient_accumulation_steps=8,  # Effective batch = 32\n    mixed_precision=True,\n    use_checkpoint=True,  # Gradient checkpointing\n\n    # Task\n    num_stages=5,\n)\n\nSMALL_TRANSFORMER_CONFIG = EHRModelConfig(\n    # Architecture\n    code_embed_dim=128,\n    visit_embed_dim=256,\n    hidden_dim=512,\n    num_layers=4,\n    num_heads=4,\n    dropout=0.1,\n\n    # Model type\n    model_type=\"transformer\",\n    use_flash_attention=False,  # Not available on M1\n\n    # Vocabulary\n    vocab_size=10000,\n    max_visits=30,  # Shorter for memory\n    max_codes_per_visit=50,\n\n    # Training\n    batch_size=2,\n    gradient_accumulation_steps=16,  # Effective batch = 32\n    mixed_precision=True,\n    use_checkpoint=True,\n\n    # Task\n    num_stages=5,\n)\n\n# ============================================================================\n# MEDIUM: RunPod 24GB GPU (A10, RTX 4090)\n# ============================================================================\n\nMEDIUM_LSTM_CONFIG = EHRModelConfig(\n    # Architecture\n    code_embed_dim=256,\n    visit_embed_dim=512,\n    hidden_dim=512,\n    num_layers=3,\n    dropout=0.1,\n\n    # Model type\n    model_type=\"lstm\",\n\n    # Vocabulary\n    vocab_size=20000,\n    max_visits=100,\n    max_codes_per_visit=100,\n\n    # Training (optimized for 24GB GPU)\n    batch_size=32,\n    gradient_accumulation_steps=1,\n    mixed_precision=True,\n    use_checkpoint=False,\n\n    # Task\n    num_stages=5,\n)\n\nMEDIUM_TRANSFORMER_CONFIG = EHRModelConfig(\n    # Architecture\n    code_embed_dim=256,\n    visit_embed_dim=512,\n    hidden_dim=1024,\n    num_layers=6,\n    num_heads=8,\n    dropout=0.1,\n\n    # Model type\n    model_type=\"transformer\",\n    use_flash_attention=True,\n\n    # Vocabulary\n    vocab_size=20000,\n    max_visits=50,\n    max_codes_per_visit=100,\n\n    # Training\n    batch_size=16,\n    gradient_accumulation_steps=2,  # Effective batch = 32\n    mixed_precision=True,\n    use_checkpoint=False,\n\n    # Task\n    num_stages=5,\n)\n\n# ============================================================================\n# LARGE: Cloud 40GB+ GPU (A40, A100)\n# ============================================================================\n\nLARGE_LSTM_CONFIG = EHRModelConfig(\n    # Architecture\n    code_embed_dim=512,\n    visit_embed_dim=768,\n    hidden_dim=1024,\n    num_layers=4,\n    dropout=0.1,\n\n    # Model type\n    model_type=\"lstm\",\n\n    # Vocabulary\n    vocab_size=50000,\n    max_visits=200,\n    max_codes_per_visit=150,\n\n    # Training (optimized for 40GB+ GPU)\n    batch_size=64,\n    gradient_accumulation_steps=1,\n    mixed_precision=True,\n    use_checkpoint=False,\n\n    # Task\n    num_stages=5,\n)\n\nLARGE_TRANSFORMER_CONFIG = EHRModelConfig(\n    # Architecture\n    code_embed_dim=512,\n    visit_embed_dim=768,\n    hidden_dim=2048,\n    num_layers=12,\n    num_heads=12,\n    dropout=0.1,\n\n    # Model type\n    model_type=\"transformer\",\n    use_flash_attention=True,\n\n    # Vocabulary\n    vocab_size=50000,\n    max_visits=100,\n    max_codes_per_visit=150,\n\n    # Training\n    batch_size=32,\n    gradient_accumulation_steps=2,  # Effective batch = 64\n    mixed_precision=True,\n    use_checkpoint=False,\n\n    # Task\n    num_stages=5,\n)\n\n\ndef get_model_config(size: str = \"small\", model_type: str = \"lstm\") -&gt; EHRModelConfig:\n    \"\"\"\n    Get a preset model configuration.\n\n    Args:\n        size: \"small\", \"medium\", or \"large\"\n        model_type: \"lstm\" or \"transformer\"\n\n    Returns:\n        EHRModelConfig instance\n\n    Examples:\n        &gt;&gt;&gt; # For M1 MacBook development\n        &gt;&gt;&gt; config = get_model_config(\"small\", \"lstm\")\n        &gt;&gt;&gt; print(f\"Memory: {config.memory_estimate_gb(dtype_bytes=2)}GB\")\n\n        &gt;&gt;&gt; # For RunPod training\n        &gt;&gt;&gt; config = get_model_config(\"medium\", \"transformer\")\n    \"\"\"\n    configs = {\n        (\"small\", \"lstm\"): SMALL_LSTM_CONFIG,\n        (\"small\", \"transformer\"): SMALL_TRANSFORMER_CONFIG,\n        (\"medium\", \"lstm\"): MEDIUM_LSTM_CONFIG,\n        (\"medium\", \"transformer\"): MEDIUM_TRANSFORMER_CONFIG,\n        (\"large\", \"lstm\"): LARGE_LSTM_CONFIG,\n        (\"large\", \"transformer\"): LARGE_TRANSFORMER_CONFIG,\n    }\n\n    key = (size, model_type)\n    if key not in configs:\n        raise ValueError(\n            f\"Unknown config: size='{size}', model_type='{model_type}'. \"\n            f\"Valid sizes: small, medium, large. Valid types: lstm, transformer.\"\n        )\n\n    return configs[key]\n</code></pre>"},{"location":"implementation/resource-aware-models/#part-4-configuration-comparison","title":"Part 4: Configuration Comparison","text":""},{"location":"implementation/resource-aware-models/#memory-and-performance-estimates","title":"Memory and Performance Estimates","text":"Config Model Type Params (M) Memory (GB) Batch Max Visits Hardware Small LSTM LSTM ~5M ~3GB 4 (32) 50 M1 16GB Small Transformer Transformer ~8M ~5GB 2 (32) 30 M1 16GB Medium LSTM LSTM ~15M ~8GB 32 100 RunPod 24GB Medium Transformer Transformer ~25M ~12GB 16 (32) 50 RunPod 24GB Large LSTM LSTM ~40M ~18GB 64 200 A40/A100 40GB+ Large Transformer Transformer ~80M ~32GB 32 (64) 100 A40/A100 40GB+ <p>Notes: - Memory estimates assume fp16 mixed precision - Batch shows actual batch size (effective batch size in parentheses) - Small configs use gradient checkpointing - LSTM is 2-3x more memory efficient than Transformer</p>"},{"location":"implementation/resource-aware-models/#part-5-usage-examples","title":"Part 5: Usage Examples","text":""},{"location":"implementation/resource-aware-models/#training-script-with-config-system","title":"Training Script with Config System","text":"<pre><code># scripts/train_disease_progression.py\n\nimport argparse\nimport torch\nfrom ehrsequencing.models.configs import get_model_config\nfrom ehrsequencing.models import LSTMProgressionModel, TransformerProgressionModel\nfrom ehrsequencing.data import load_sequences\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--size\", default=\"small\", choices=[\"small\", \"medium\", \"large\"])\n    parser.add_argument(\"--model-type\", default=\"lstm\", choices=[\"lstm\", \"transformer\"])\n    parser.add_argument(\"--epochs\", type=int, default=50)\n    parser.add_argument(\"--data-path\", required=True)\n    args = parser.parse_args()\n\n    # Get configuration\n    config = get_model_config(args.size, args.model_type)\n\n    print(f\"\\n{'='*80}\")\n    print(f\"Training {args.model_type.upper()} model ({args.size} config)\")\n    print(f\"{'='*80}\")\n    print(f\"Parameters: ~{config.total_params_millions}M\")\n    print(f\"Memory estimate: ~{config.memory_estimate_gb(dtype_bytes=2)}GB (fp16)\")\n    print(f\"Effective batch size: {config.effective_batch_size}\")\n    print(f\"Max visits: {config.max_visits}\")\n    print(f\"{'='*80}\\n\")\n\n    # Load data\n    sequences = load_sequences(args.data_path)\n\n    # Create model\n    if args.model_type == \"lstm\":\n        model = LSTMProgressionModel(\n            vocab_size=config.vocab_size,\n            code_embed_dim=config.code_embed_dim,\n            visit_embed_dim=config.visit_embed_dim,\n            hidden_dim=config.hidden_dim,\n            num_stages=config.num_stages,\n            num_layers=config.num_layers,\n            dropout=config.dropout\n        )\n    else:\n        model = TransformerProgressionModel(\n            vocab_size=config.vocab_size,\n            embed_dim=config.visit_embed_dim,\n            hidden_dim=config.hidden_dim,\n            num_stages=config.num_stages,\n            num_layers=config.num_layers,\n            num_heads=config.num_heads,\n            dropout=config.dropout\n        )\n\n    # Training loop\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    # ... training code ...\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"implementation/resource-aware-models/#local-development-workflow","title":"Local Development Workflow","text":"<pre><code># On M1 MacBook: Fast iteration with small LSTM\npython scripts/train_disease_progression.py \\\n    --size small \\\n    --model-type lstm \\\n    --epochs 10 \\\n    --data-path data/ckd_sequences.pt\n\n# Verify logic works, then scale up on RunPod\n</code></pre>"},{"location":"implementation/resource-aware-models/#runpod-training-workflow","title":"RunPod Training Workflow","text":"<pre><code># On RunPod A10 24GB: Medium transformer\npython scripts/train_disease_progression.py \\\n    --size medium \\\n    --model-type transformer \\\n    --epochs 100 \\\n    --data-path /workspace/ehr-sequencing/data/ckd_sequences.pt\n</code></pre>"},{"location":"implementation/resource-aware-models/#part-6-pre-trained-model-integration","title":"Part 6: Pre-trained Model Integration","text":""},{"location":"implementation/resource-aware-models/#using-pre-trained-embeddings-with-resource-configs","title":"Using Pre-trained Embeddings with Resource Configs","text":"<pre><code>from ehrsequencing.embeddings import CEHRBERTWrapper\nfrom ehrsequencing.models.configs import get_model_config\n\n# Load pre-trained embeddings\ncehrbert = CEHRBERTWrapper.from_pretrained('cehrbert-base')\npretrained_embeddings = cehrbert.get_code_embeddings()\n\n# Get config\nconfig = get_model_config(\"small\", \"lstm\")\n\n# Create model with pre-trained embeddings\nmodel = LSTMProgressionModel(\n    vocab_size=config.vocab_size,\n    code_embed_dim=config.code_embed_dim,\n    # ... other params\n)\n\n# Replace embeddings\nmodel.encoder.code_embeddings = pretrained_embeddings\n\n# Optionally freeze embeddings\nmodel.encoder.code_embeddings.weight.requires_grad = False\n</code></pre>"},{"location":"implementation/resource-aware-models/#part-7-recommendations","title":"Part 7: Recommendations","text":""},{"location":"implementation/resource-aware-models/#implementation-priority","title":"Implementation Priority","text":"<p>Phase 1: LSTM Baseline (Week 1-2) 1. Implement <code>LSTMVisitEncoder</code> (small config) 2. Implement <code>LSTMProgressionModel</code> 3. Train on M1 MacBook with small dataset 4. Validate logic and pipeline</p> <p>Phase 2: Scale to RunPod (Week 3) 1. Use medium LSTM config 2. Train on full dataset 3. Establish baseline performance</p> <p>Phase 3: Transformer Comparison (Week 4+) 1. Implement Transformer variant 2. Compare LSTM vs Transformer 3. Decide on production model</p>"},{"location":"implementation/resource-aware-models/#why-start-with-lstm","title":"Why Start with LSTM","text":"<ol> <li>Fast Development - Works well on M1 MacBook</li> <li>Strong Baseline - Proven for EHR sequences</li> <li>Memory Efficient - 2-3x less memory than Transformer</li> <li>Interpretable - Easier to debug</li> <li>Comparison Point - Validate if Transformer complexity is needed</li> </ol>"},{"location":"implementation/resource-aware-models/#when-to-use-transformer","title":"When to Use Transformer","text":"<ul> <li>After LSTM baseline is established</li> <li>When training on RunPod (24GB+)</li> <li>When long-range dependencies are critical</li> <li>For production deployment (best performance)</li> </ul>"},{"location":"implementation/resource-aware-models/#part-8-file-structure","title":"Part 8: File Structure","text":"<pre><code>src/ehrsequencing/models/\n\u251c\u2500\u2500 configs/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 model_configs.py      # EHRModelConfig dataclass\n\u2502   \u2514\u2500\u2500 presets.py             # SMALL/MEDIUM/LARGE configs\n\u251c\u2500\u2500 lstm/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 visit_encoder.py       # LSTMVisitEncoder\n\u2502   \u2514\u2500\u2500 progression_model.py   # LSTMProgressionModel\n\u251c\u2500\u2500 transformer/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 visit_encoder.py       # TransformerVisitEncoder\n\u2502   \u2514\u2500\u2500 progression_model.py   # TransformerProgressionModel\n\u2514\u2500\u2500 __init__.py\n\nscripts/\n\u251c\u2500\u2500 train_disease_progression.py  # Main training script\n\u2514\u2500\u2500 compare_models.py              # LSTM vs Transformer comparison\n</code></pre>"},{"location":"implementation/resource-aware-models/#summary","title":"Summary","text":""},{"location":"implementation/resource-aware-models/#key-decisions","title":"Key Decisions","text":"<ol> <li>\u2705 LSTM as baseline - Excellent fit for visit-grouped sequences</li> <li>\u2705 Resource-aware configs - Small/Medium/Large for different hardware</li> <li>\u2705 M1 MacBook support - Small configs for local development</li> <li>\u2705 RunPod scaling - Medium configs for realistic training</li> <li>\u2705 Logically equivalent - Same architecture, different scales</li> </ol>"},{"location":"implementation/resource-aware-models/#expected-performance","title":"Expected Performance","text":"<p>LSTM Baseline: - M1 MacBook: ~10 min/epoch (small dataset) - RunPod 24GB: ~2 min/epoch (full dataset) - A40 40GB: ~1 min/epoch (full dataset)</p> <p>Memory Usage: - Small LSTM: ~3GB (M1 safe) - Medium LSTM: ~8GB (RunPod safe) - Large LSTM: ~18GB (A40 safe)</p>"},{"location":"implementation/resource-aware-models/#next-steps","title":"Next Steps","text":"<ol> <li>Implement model config system</li> <li>Implement LSTM baseline</li> <li>Test on M1 MacBook (small config)</li> <li>Scale to RunPod (medium config)</li> <li>Compare with Transformer (optional)</li> </ol> <p>Document Version: 1.0 Last Updated: January 20, 2026 Next Review: After LSTM baseline implementation</p>"},{"location":"implementation/visit-grouped-sequences/","title":"Visit-Grouped Sequence Representation: Implementation Plan","text":"<p>Date: January 19, 2026 Status: Implementation Roadmap Priority: Foundation for all downstream applications</p>"},{"location":"implementation/visit-grouped-sequences/#overview","title":"Overview","text":"<p>This document outlines the complete implementation plan for visit-grouped sequence representation in the EHR Sequencing project. This is the foundational pipeline that transforms raw EHR/EMR data into structured visit sequences suitable for disease progression modeling, temporal phenotyping, and patient segmentation.</p>"},{"location":"implementation/visit-grouped-sequences/#key-components","title":"Key Components","text":"<ol> <li>Data Sequencing Pipeline - Transform raw EHR \u2192 structured visits</li> <li>Data Adapters - Reuse from loinc-predictor project</li> <li>Visit Representation - How to structure codes within visits</li> <li>Pre-trained Model Integration - Leverage CEHR-BERT embeddings</li> <li>Downstream Applications - Disease progression, phenotyping, clustering</li> </ol>"},{"location":"implementation/visit-grouped-sequences/#part-1-sequencing-pipeline-architecture","title":"Part 1: Sequencing Pipeline Architecture","text":""},{"location":"implementation/visit-grouped-sequences/#11-pipeline-overview","title":"1.1 Pipeline Overview","text":"<pre><code>Raw EHR Data (CSV/Database)\n    \u2193\n[Data Adapter] - Load and normalize\n    \u2193\nEvent Stream (timestamp, patient_id, code, code_type, value)\n    \u2193\n[Visit Grouper] - Group events into visits\n    \u2193\nVisit Sequences (patient \u2192 [visit1, visit2, ...])\n    \u2193\n[Visit Encoder] - Encode visits with pre-trained embeddings\n    \u2193\nPatient Trajectories (patient \u2192 [visit_embed1, visit_embed2, ...])\n    \u2193\n[Downstream Models] - Disease progression, phenotyping, etc.\n</code></pre>"},{"location":"implementation/visit-grouped-sequences/#12-design-decisions-open-questions","title":"1.2 Design Decisions &amp; Open Questions","text":""},{"location":"implementation/visit-grouped-sequences/#decision-1-visit-definition","title":"Decision 1: Visit Definition","text":"<p>Options: - A. Same-day grouping - All events on same calendar day = one visit - B. Time-window grouping - Events within 24 hours = one visit - C. Encounter-based - Use explicit encounter IDs from EHR - D. Hybrid - Encounter IDs when available, same-day otherwise</p> <p>Recommendation: D (Hybrid) - Most EHR systems have encounter IDs - Fallback to same-day for datasets without encounters - Clinically accurate (reflects actual visits)</p>"},{"location":"implementation/visit-grouped-sequences/#decision-2-within-visit-structure","title":"Decision 2: Within-Visit Structure","text":"<p>Open Question: How to maintain structure within a visit?</p> <p>Options:</p> <p>A. Flat (Order-Agnostic) <pre><code>visit = {\n    'codes': [ICD:E11.9, LOINC:4548-4, RXNORM:860975],\n    'timestamp': '2020-01-15'\n}\n# Pros: Simple, order doesn't matter\n# Cons: Loses temporal ordering within visit\n</code></pre></p> <p>B. Typed Groups (Code-Type Aware) <pre><code>visit = {\n    'diagnoses': [ICD:E11.9],\n    'labs': [LOINC:4548-4],\n    'medications': [RXNORM:860975],\n    'timestamp': '2020-01-15'\n}\n# Pros: Preserves code types, natural clinical workflow\n# Cons: More complex, need to handle variable-length groups\n</code></pre></p> <p>C. Temporal Ordering (Within-Visit Time) <pre><code>visit = {\n    'events': [\n        (09:00, LOINC:4548-4),   # Lab drawn first\n        (09:30, ICD:E11.9),       # Diagnosis after lab\n        (10:00, RXNORM:860975)    # Medication prescribed last\n    ],\n    'date': '2020-01-15'\n}\n# Pros: Most accurate, preserves clinical workflow\n# Cons: Requires precise timestamps (often not available)\n</code></pre></p> <p>Recommendation: B (Typed Groups) with optional A fallback - Reflects clinical workflow (labs \u2192 diagnosis \u2192 treatment) - Most datasets have code types - Can aggregate to flat if needed</p>"},{"location":"implementation/visit-grouped-sequences/#decision-3-visit-sequence-length","title":"Decision 3: Visit Sequence Length","text":"<p>Open Question: How to handle variable-length patient histories?</p> <p>Options: - Fixed window - Last N visits (e.g., N=20) - Time window - Last T years (e.g., T=5) - Dynamic - All visits, pad/truncate at batch level - Hierarchical - Summarize old visits, keep recent ones detailed</p> <p>Recommendation: Dynamic with max limit - Keep all visits up to max (e.g., 50 visits) - For longer histories, use hierarchical summarization - Allows model to learn from full history</p>"},{"location":"implementation/visit-grouped-sequences/#part-2-data-adapter-integration","title":"Part 2: Data Adapter Integration","text":""},{"location":"implementation/visit-grouped-sequences/#21-reusing-loinc-predictor-data-adapters","title":"2.1 Reusing loinc-predictor Data Adapters","text":"<p>The loinc-predictor project already has data adapters for: - Synthea (synthetic EHR data) - MIMIC-III (real ICU data) - Generic CSV format</p> <p>Strategy: Duplicate adapters to ehr-sequencing</p> <pre><code>loinc-predictor/src/loinc_predictor/data/adapters/\n    \u251c\u2500\u2500 synthea_adapter.py\n    \u251c\u2500\u2500 mimic_adapter.py\n    \u2514\u2500\u2500 base_adapter.py\n\n\u2192 Copy to \u2192\n\nehr-sequencing/src/ehrsequencing/data/adapters/\n    \u251c\u2500\u2500 synthea_adapter.py\n    \u251c\u2500\u2500 mimic_adapter.py\n    \u2514\u2500\u2500 base_adapter.py\n</code></pre>"},{"location":"implementation/visit-grouped-sequences/#22-adapter-interface","title":"2.2 Adapter Interface","text":"<pre><code># src/ehrsequencing/data/adapters/base_adapter.py\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\nimport pandas as pd\n\nclass BaseEHRAdapter(ABC):\n    \"\"\"\n    Base class for EHR data adapters.\n\n    Adapters transform raw EHR data into standardized event streams.\n    \"\"\"\n\n    @abstractmethod\n    def load_patients(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Load patient demographics.\n\n        Returns:\n            DataFrame with columns: [patient_id, birth_date, gender, race, ...]\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load_events(self, patient_ids: List[str] = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Load all clinical events.\n\n        Args:\n            patient_ids: Optional list of patient IDs to filter\n\n        Returns:\n            DataFrame with columns:\n                - patient_id: str\n                - timestamp: datetime\n                - code: str (e.g., 'ICD10:E11.9', 'LOINC:4548-4')\n                - code_type: str ('ICD', 'LOINC', 'SNOMED', 'RXNORM', 'CPT')\n                - value: float (for lab values, None for diagnoses)\n                - unit: str (for lab values)\n                - encounter_id: str (visit identifier)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load_encounters(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Load encounter (visit) information.\n\n        Returns:\n            DataFrame with columns:\n                - encounter_id: str\n                - patient_id: str\n                - start_time: datetime\n                - end_time: datetime\n                - encounter_type: str ('inpatient', 'outpatient', 'emergency')\n        \"\"\"\n        pass\n\n    def get_vocabulary(self) -&gt; Dict[str, int]:\n        \"\"\"\n        Build vocabulary mapping codes to integer IDs.\n\n        Returns:\n            Dict mapping code strings to integer IDs\n        \"\"\"\n        events = self.load_events()\n        unique_codes = events['code'].unique()\n\n        vocab = {\n            '[PAD]': 0,\n            '[UNK]': 1,\n            '[MASK]': 2,\n            '[CLS]': 3,\n            '[SEP]': 4\n        }\n\n        for i, code in enumerate(sorted(unique_codes), start=5):\n            vocab[code] = i\n\n        return vocab\n</code></pre>"},{"location":"implementation/visit-grouped-sequences/#23-synthea-adapter-implementation","title":"2.3 Synthea Adapter Implementation","text":"<pre><code># src/ehrsequencing/data/adapters/synthea_adapter.py\n\nimport pandas as pd\nfrom pathlib import Path\nfrom .base_adapter import BaseEHRAdapter\n\nclass SyntheaAdapter(BaseEHRAdapter):\n    \"\"\"\n    Adapter for Synthea synthetic EHR data.\n\n    Synthea format:\n        - patients.csv\n        - conditions.csv (diagnoses)\n        - observations.csv (labs, vitals)\n        - medications.csv\n        - procedures.csv\n        - encounters.csv\n    \"\"\"\n\n    def __init__(self, data_dir: str):\n        self.data_dir = Path(data_dir)\n        self._validate_data_dir()\n\n    def _validate_data_dir(self):\n        \"\"\"Check that required files exist.\"\"\"\n        required_files = [\n            'patients.csv',\n            'conditions.csv',\n            'observations.csv',\n            'medications.csv',\n            'encounters.csv'\n        ]\n        for file in required_files:\n            if not (self.data_dir / file).exists():\n                raise FileNotFoundError(f\"Missing required file: {file}\")\n\n    def load_patients(self) -&gt; pd.DataFrame:\n        \"\"\"Load patient demographics.\"\"\"\n        df = pd.read_csv(self.data_dir / 'patients.csv')\n\n        return df.rename(columns={\n            'Id': 'patient_id',\n            'BIRTHDATE': 'birth_date',\n            'GENDER': 'gender',\n            'RACE': 'race'\n        })[['patient_id', 'birth_date', 'gender', 'race']]\n\n    def load_encounters(self) -&gt; pd.DataFrame:\n        \"\"\"Load encounter information.\"\"\"\n        df = pd.read_csv(self.data_dir / 'encounters.csv')\n\n        return pd.DataFrame({\n            'encounter_id': df['Id'],\n            'patient_id': df['PATIENT'],\n            'start_time': pd.to_datetime(df['START']),\n            'end_time': pd.to_datetime(df['STOP']),\n            'encounter_type': df['ENCOUNTERCLASS']\n        })\n\n    def load_events(self, patient_ids: List[str] = None) -&gt; pd.DataFrame:\n        \"\"\"Load all clinical events.\"\"\"\n        events = []\n\n        # Load diagnoses (conditions)\n        conditions = pd.read_csv(self.data_dir / 'conditions.csv')\n        conditions_events = pd.DataFrame({\n            'patient_id': conditions['PATIENT'],\n            'timestamp': pd.to_datetime(conditions['START']),\n            'code': 'SNOMED:' + conditions['CODE'].astype(str),\n            'code_type': 'SNOMED',\n            'value': None,\n            'unit': None,\n            'encounter_id': conditions['ENCOUNTER']\n        })\n        events.append(conditions_events)\n\n        # Load labs/observations\n        observations = pd.read_csv(self.data_dir / 'observations.csv')\n        obs_events = pd.DataFrame({\n            'patient_id': observations['PATIENT'],\n            'timestamp': pd.to_datetime(observations['DATE']),\n            'code': 'LOINC:' + observations['CODE'].astype(str),\n            'code_type': 'LOINC',\n            'value': pd.to_numeric(observations['VALUE'], errors='coerce'),\n            'unit': observations['UNITS'],\n            'encounter_id': observations['ENCOUNTER']\n        })\n        events.append(obs_events)\n\n        # Load medications\n        medications = pd.read_csv(self.data_dir / 'medications.csv')\n        med_events = pd.DataFrame({\n            'patient_id': medications['PATIENT'],\n            'timestamp': pd.to_datetime(medications['START']),\n            'code': 'RXNORM:' + medications['CODE'].astype(str),\n            'code_type': 'RXNORM',\n            'value': None,\n            'unit': None,\n            'encounter_id': medications['ENCOUNTER']\n        })\n        events.append(med_events)\n\n        # Load procedures\n        procedures = pd.read_csv(self.data_dir / 'procedures.csv')\n        proc_events = pd.DataFrame({\n            'patient_id': procedures['PATIENT'],\n            'timestamp': pd.to_datetime(procedures['DATE']),\n            'code': 'SNOMED:' + procedures['CODE'].astype(str),\n            'code_type': 'SNOMED',\n            'value': None,\n            'unit': None,\n            'encounter_id': procedures['ENCOUNTER']\n        })\n        events.append(proc_events)\n\n        # Combine all events\n        all_events = pd.concat(events, ignore_index=True)\n\n        # Filter by patient IDs if provided\n        if patient_ids is not None:\n            all_events = all_events[all_events['patient_id'].isin(patient_ids)]\n\n        # Sort by patient and time\n        all_events = all_events.sort_values(['patient_id', 'timestamp'])\n\n        return all_events\n</code></pre>"},{"location":"implementation/visit-grouped-sequences/#part-3-visit-grouping-sequencing","title":"Part 3: Visit Grouping &amp; Sequencing","text":""},{"location":"implementation/visit-grouped-sequences/#31-visit-grouper-implementation","title":"3.1 Visit Grouper Implementation","text":"<pre><code># src/ehrsequencing/data/sequences/visit_grouper.py\n\nimport pandas as pd\nfrom typing import List, Dict, Any\nfrom collections import defaultdict\n\nclass VisitGrouper:\n    \"\"\"\n    Group clinical events into visits.\n\n    Supports multiple grouping strategies:\n    - encounter-based (use explicit encounter IDs)\n    - same-day (group events on same calendar day)\n    - time-window (group events within N hours)\n    \"\"\"\n\n    def __init__(self, strategy: str = 'encounter', time_window_hours: int = 24):\n        \"\"\"\n        Args:\n            strategy: 'encounter', 'same-day', or 'time-window'\n            time_window_hours: For time-window strategy\n        \"\"\"\n        self.strategy = strategy\n        self.time_window_hours = time_window_hours\n\n    def group_events_into_visits(self, events_df: pd.DataFrame) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Group events into visits for all patients.\n\n        Args:\n            events_df: DataFrame from adapter.load_events()\n\n        Returns:\n            List of visit dicts, one per patient-visit\n        \"\"\"\n        if self.strategy == 'encounter':\n            return self._group_by_encounter(events_df)\n        elif self.strategy == 'same-day':\n            return self._group_by_same_day(events_df)\n        elif self.strategy == 'time-window':\n            return self._group_by_time_window(events_df)\n        else:\n            raise ValueError(f\"Unknown strategy: {self.strategy}\")\n\n    def _group_by_encounter(self, events_df: pd.DataFrame) -&gt; List[Dict[str, Any]]:\n        \"\"\"Group by encounter_id.\"\"\"\n        visits = []\n\n        for (patient_id, encounter_id), group in events_df.groupby(['patient_id', 'encounter_id']):\n            visit = self._create_visit_dict(patient_id, encounter_id, group)\n            visits.append(visit)\n\n        return visits\n\n    def _group_by_same_day(self, events_df: pd.DataFrame) -&gt; List[Dict[str, Any]]:\n        \"\"\"Group events on same calendar day.\"\"\"\n        events_df = events_df.copy()\n        events_df['visit_date'] = events_df['timestamp'].dt.date\n\n        visits = []\n        for (patient_id, visit_date), group in events_df.groupby(['patient_id', 'visit_date']):\n            visit = self._create_visit_dict(patient_id, f\"{patient_id}_{visit_date}\", group)\n            visits.append(visit)\n\n        return visits\n\n    def _group_by_time_window(self, events_df: pd.DataFrame) -&gt; List[Dict[str, Any]]:\n        \"\"\"Group events within time window.\"\"\"\n        visits = []\n\n        for patient_id, patient_events in events_df.groupby('patient_id'):\n            patient_events = patient_events.sort_values('timestamp')\n\n            current_visit_events = []\n            visit_start_time = None\n            visit_id = 0\n\n            for _, event in patient_events.iterrows():\n                if visit_start_time is None:\n                    visit_start_time = event['timestamp']\n                    current_visit_events.append(event)\n                else:\n                    time_diff = (event['timestamp'] - visit_start_time).total_seconds() / 3600\n\n                    if time_diff &lt;= self.time_window_hours:\n                        current_visit_events.append(event)\n                    else:\n                        visit = self._create_visit_dict(\n                            patient_id,\n                            f\"{patient_id}_visit_{visit_id}\",\n                            pd.DataFrame(current_visit_events)\n                        )\n                        visits.append(visit)\n\n                        current_visit_events = [event]\n                        visit_start_time = event['timestamp']\n                        visit_id += 1\n\n            if current_visit_events:\n                visit = self._create_visit_dict(\n                    patient_id,\n                    f\"{patient_id}_visit_{visit_id}\",\n                    pd.DataFrame(current_visit_events)\n                )\n                visits.append(visit)\n\n        return visits\n\n    def _create_visit_dict(\n        self,\n        patient_id: str,\n        visit_id: str,\n        events: pd.DataFrame\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Create structured visit dictionary.\n\n        Uses typed groups (Option B from design decisions).\n        \"\"\"\n        visit = {\n            'patient_id': patient_id,\n            'visit_id': visit_id,\n            'timestamp': events['timestamp'].min(),\n            'codes_by_type': defaultdict(list),\n            'all_codes': [],\n            'values': {}\n        }\n\n        for _, event in events.iterrows():\n            code = event['code']\n            code_type = event['code_type']\n\n            visit['codes_by_type'][code_type].append(code)\n            visit['all_codes'].append(code)\n\n            if pd.notna(event['value']):\n                visit['values'][code] = {\n                    'value': event['value'],\n                    'unit': event['unit']\n                }\n\n        return visit\n</code></pre>"},{"location":"implementation/visit-grouped-sequences/#32-patient-sequence-builder","title":"3.2 Patient Sequence Builder","text":"<pre><code># src/ehrsequencing/data/sequences/sequence_builder.py\n\nimport pandas as pd\nfrom typing import List, Dict, Any\nfrom .visit_grouper import VisitGrouper\n\nclass PatientSequenceBuilder:\n    \"\"\"\n    Build patient-level visit sequences.\n\n    Transforms visits into sequences suitable for modeling.\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab: Dict[str, int],\n        max_visits: int = 50,\n        max_codes_per_visit: int = 100\n    ):\n        self.vocab = vocab\n        self.max_visits = max_visits\n        self.max_codes_per_visit = max_codes_per_visit\n\n    def build_sequences(self, visits: List[Dict[str, Any]]) -&gt; Dict[str, List[Dict]]:\n        \"\"\"\n        Build sequences for all patients.\n\n        Args:\n            visits: List of visit dicts from VisitGrouper\n\n        Returns:\n            Dict mapping patient_id to list of visit sequences\n        \"\"\"\n        patient_sequences = defaultdict(list)\n\n        for visit in visits:\n            patient_id = visit['patient_id']\n            patient_sequences[patient_id].append(visit)\n\n        # Sort each patient's visits by time\n        for patient_id in patient_sequences:\n            patient_sequences[patient_id].sort(key=lambda v: v['timestamp'])\n\n        return dict(patient_sequences)\n\n    def encode_sequence(self, patient_visits: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Encode a patient's visit sequence into tensor-ready format.\n\n        Args:\n            patient_visits: List of visits for one patient\n\n        Returns:\n            Dict with encoded sequences and metadata\n        \"\"\"\n        num_visits = min(len(patient_visits), self.max_visits)\n\n        encoded = {\n            'patient_id': patient_visits[0]['patient_id'],\n            'num_visits': num_visits,\n            'visit_codes': [],  # List of code ID lists\n            'visit_timestamps': [],\n            'visit_time_deltas': [],  # Days since previous visit\n            'visit_codes_by_type': [],  # List of dicts\n            'visit_values': []  # List of value dicts\n        }\n\n        prev_timestamp = None\n\n        for i in range(num_visits):\n            visit = patient_visits[i]\n\n            # Encode codes\n            code_ids = [\n                self.vocab.get(code, self.vocab['[UNK]'])\n                for code in visit['all_codes'][:self.max_codes_per_visit]\n            ]\n            encoded['visit_codes'].append(code_ids)\n\n            # Encode by type\n            codes_by_type = {}\n            for code_type, codes in visit['codes_by_type'].items():\n                codes_by_type[code_type] = [\n                    self.vocab.get(code, self.vocab['[UNK]'])\n                    for code in codes[:self.max_codes_per_visit]\n                ]\n            encoded['visit_codes_by_type'].append(codes_by_type)\n\n            # Timestamps\n            encoded['visit_timestamps'].append(visit['timestamp'])\n\n            # Time deltas\n            if prev_timestamp is None:\n                time_delta = 0\n            else:\n                time_delta = (visit['timestamp'] - prev_timestamp).days\n            encoded['visit_time_deltas'].append(time_delta)\n            prev_timestamp = visit['timestamp']\n\n            # Values\n            encoded['visit_values'].append(visit['values'])\n\n        return encoded\n</code></pre>"},{"location":"implementation/visit-grouped-sequences/#part-4-pre-trained-model-integration-cehr-bert","title":"Part 4: Pre-trained Model Integration (CEHR-BERT)","text":""},{"location":"implementation/visit-grouped-sequences/#41-cehr-bert-integration-strategy","title":"4.1 CEHR-BERT Integration Strategy","text":"<p>Goal: Use pre-trained CEHR-BERT embeddings without training from scratch</p> <p>Approach: 1. Load pre-trained CEHR-BERT model 2. Extract code embeddings (frozen or fine-tunable) 3. Use as initialization for visit encoder 4. Fine-tune on downstream tasks</p>"},{"location":"implementation/visit-grouped-sequences/#42-cehr-bert-wrapper","title":"4.2 CEHR-BERT Wrapper","text":"<pre><code># src/ehrsequencing/embeddings/cehrbert_wrapper.py\n\nimport torch\nimport torch.nn as nn\nfrom typing import Dict, Optional\n\nclass CEHRBERTWrapper(nn.Module):\n    \"\"\"\n    Wrapper for pre-trained CEHR-BERT model.\n\n    Provides easy interface to extract code embeddings and\n    encode patient sequences.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        freeze_embeddings: bool = False,\n        device: str = 'cpu'\n    ):\n        super().__init__()\n        self.device = device\n\n        # Load pre-trained model\n        self.cehrbert = self._load_pretrained(model_path)\n\n        # Extract code embeddings\n        self.code_embeddings = self.cehrbert.get_code_embeddings()\n\n        if freeze_embeddings:\n            self.code_embeddings.weight.requires_grad = False\n\n    def _load_pretrained(self, model_path: str):\n        \"\"\"Load pre-trained CEHR-BERT model.\"\"\"\n        try:\n            from cehrbert import CEHRBERT\n            model = CEHRBERT.from_pretrained(model_path)\n            return model\n        except ImportError:\n            raise ImportError(\n                \"CEHR-BERT not installed. Install with: pip install cehr-bert\"\n            )\n\n    def get_code_embeddings(self) -&gt; nn.Embedding:\n        \"\"\"Get code embedding layer.\"\"\"\n        return self.code_embeddings\n\n    def encode_codes(self, code_ids: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Encode codes using pre-trained embeddings.\n\n        Args:\n            code_ids: [batch, seq_len] or [batch, num_visits, codes_per_visit]\n\n        Returns:\n            embeddings: Same shape with last dim = embed_dim\n        \"\"\"\n        return self.code_embeddings(code_ids)\n\n    def encode_patient_sequence(\n        self,\n        code_ids: torch.Tensor,\n        timestamps: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Encode full patient sequence using CEHR-BERT.\n\n        Args:\n            code_ids: [batch, seq_len]\n            timestamps: [batch, seq_len] - Days since first event\n            attention_mask: [batch, seq_len]\n\n        Returns:\n            sequence_embeddings: [batch, seq_len, embed_dim]\n        \"\"\"\n        return self.cehrbert.encode(\n            code_ids=code_ids,\n            timestamps=timestamps,\n            attention_mask=attention_mask\n        )\n</code></pre>"},{"location":"implementation/visit-grouped-sequences/#43-visit-encoder-with-pre-trained-embeddings","title":"4.3 Visit Encoder with Pre-trained Embeddings","text":"<pre><code># src/ehrsequencing/models/visit_encoder.py\n\nimport torch\nimport torch.nn as nn\nfrom ..embeddings.cehrbert_wrapper import CEHRBERTWrapper\n\nclass VisitEncoder(nn.Module):\n    \"\"\"\n    Encode visits using pre-trained code embeddings.\n\n    Two-level encoding:\n    1. Code-level: Use pre-trained CEHR-BERT embeddings\n    2. Visit-level: Aggregate codes into visit representation\n    \"\"\"\n\n    def __init__(\n        self,\n        cehrbert_wrapper: CEHRBERTWrapper,\n        code_embed_dim: int = 128,\n        visit_embed_dim: int = 256,\n        aggregation: str = 'transformer'\n    ):\n        super().__init__()\n\n        self.code_embeddings = cehrbert_wrapper.get_code_embeddings()\n        self.aggregation = aggregation\n\n        if aggregation == 'transformer':\n            self.aggregator = nn.TransformerEncoder(\n                nn.TransformerEncoderLayer(\n                    d_model=code_embed_dim,\n                    nhead=4,\n                    dim_feedforward=code_embed_dim * 4,\n                    batch_first=True\n                ),\n                num_layers=2\n            )\n        elif aggregation == 'lstm':\n            self.aggregator = nn.LSTM(\n                code_embed_dim,\n                code_embed_dim,\n                batch_first=True\n            )\n        elif aggregation == 'mean':\n            self.aggregator = None\n        else:\n            raise ValueError(f\"Unknown aggregation: {aggregation}\")\n\n        self.projection = nn.Linear(code_embed_dim, visit_embed_dim)\n\n    def forward(self, visit_codes: torch.Tensor, visit_mask: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Encode a batch of visits.\n\n        Args:\n            visit_codes: [batch, max_codes_per_visit]\n            visit_mask: [batch, max_codes_per_visit] - 1 for real, 0 for padding\n\n        Returns:\n            visit_embeddings: [batch, visit_embed_dim]\n        \"\"\"\n        # Embed codes\n        code_embeds = self.code_embeddings(visit_codes)  # [batch, codes, embed_dim]\n\n        # Aggregate\n        if self.aggregation == 'transformer':\n            attn_mask = ~visit_mask.bool()\n            aggregated = self.aggregator(\n                code_embeds,\n                src_key_padding_mask=attn_mask\n            )\n            visit_repr = self._masked_mean(aggregated, visit_mask)\n\n        elif self.aggregation == 'lstm':\n            _, (hidden, _) = self.aggregator(code_embeds)\n            visit_repr = hidden[-1]\n\n        elif self.aggregation == 'mean':\n            visit_repr = self._masked_mean(code_embeds, visit_mask)\n\n        # Project to visit space\n        return self.projection(visit_repr)\n\n    def _masked_mean(self, embeddings: torch.Tensor, mask: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute mean over non-masked elements.\"\"\"\n        mask_expanded = mask.unsqueeze(-1)\n        masked_embeds = embeddings * mask_expanded\n        return masked_embeds.sum(dim=1) / mask.sum(dim=1, keepdim=True)\n</code></pre>"},{"location":"implementation/visit-grouped-sequences/#part-5-downstream-applications","title":"Part 5: Downstream Applications","text":""},{"location":"implementation/visit-grouped-sequences/#51-application-1-disease-progression-modeling","title":"5.1 Application 1: Disease Progression Modeling","text":"<p>Use Case: Predict CKD stage progression</p> <p>Architecture: <pre><code>Visit Sequences \u2192 Visit Encoder \u2192 LSTM/Transformer \u2192 Stage Classifier\n</code></pre></p> <p>Implementation: See <code>pretrained-models-and-disease-progression.md</code></p> <p>Key Features: - Multi-task learning (stage + time-to-progression) - Survival analysis integration - Attention-based interpretability</p>"},{"location":"implementation/visit-grouped-sequences/#52-application-2-temporal-phenotyping","title":"5.2 Application 2: Temporal Phenotyping","text":"<p>Use Case: Discover disease subtypes based on temporal patterns</p> <p>Architecture: <pre><code>Visit Sequences \u2192 Visit Encoder \u2192 Patient Embeddings \u2192 Clustering\n</code></pre></p> <p>Implementation: <pre><code># src/ehrsequencing/clustering/temporal_phenotyping.py\n\nfrom sklearn.cluster import KMeans\nimport umap\n\nclass TemporalPhenotyper:\n    \"\"\"\n    Discover disease phenotypes from visit sequences.\n    \"\"\"\n\n    def __init__(self, visit_encoder, n_phenotypes: int = 5):\n        self.visit_encoder = visit_encoder\n        self.n_phenotypes = n_phenotypes\n        self.clusterer = KMeans(n_clusters=n_phenotypes)\n        self.reducer = umap.UMAP(n_components=2)\n\n    def fit(self, patient_sequences):\n        \"\"\"\n        Discover phenotypes from patient sequences.\n        \"\"\"\n        # Encode all patients\n        patient_embeddings = []\n        for seq in patient_sequences:\n            visit_embeds = self.visit_encoder(seq['visit_codes'], seq['visit_mask'])\n            patient_embed = visit_embeds.mean(dim=0)  # Aggregate visits\n            patient_embeddings.append(patient_embed)\n\n        patient_embeddings = torch.stack(patient_embeddings).detach().numpy()\n\n        # Cluster\n        phenotypes = self.clusterer.fit_predict(patient_embeddings)\n\n        # Reduce for visualization\n        embeddings_2d = self.reducer.fit_transform(patient_embeddings)\n\n        return phenotypes, embeddings_2d\n</code></pre></p>"},{"location":"implementation/visit-grouped-sequences/#53-application-3-patient-similarity-retrieval","title":"5.3 Application 3: Patient Similarity &amp; Retrieval","text":"<p>Use Case: Find similar patients for case-based reasoning</p> <p>Architecture: <pre><code>Query Patient \u2192 Visit Encoder \u2192 Patient Embedding \u2192 Nearest Neighbors\n</code></pre></p> <p>Implementation: <pre><code># src/ehrsequencing/retrieval/patient_similarity.py\n\nimport faiss\nimport numpy as np\n\nclass PatientRetrieval:\n    \"\"\"\n    Retrieve similar patients based on visit sequences.\n    \"\"\"\n\n    def __init__(self, visit_encoder, embed_dim: int = 256):\n        self.visit_encoder = visit_encoder\n        self.index = faiss.IndexFlatL2(embed_dim)\n        self.patient_ids = []\n\n    def index_patients(self, patient_sequences):\n        \"\"\"Build index from patient sequences.\"\"\"\n        embeddings = []\n\n        for seq in patient_sequences:\n            visit_embeds = self.visit_encoder(seq['visit_codes'], seq['visit_mask'])\n            patient_embed = visit_embeds.mean(dim=0).detach().numpy()\n            embeddings.append(patient_embed)\n            self.patient_ids.append(seq['patient_id'])\n\n        embeddings = np.array(embeddings)\n        self.index.add(embeddings)\n\n    def retrieve_similar(self, query_sequence, k: int = 10):\n        \"\"\"Retrieve k most similar patients.\"\"\"\n        visit_embeds = self.visit_encoder(\n            query_sequence['visit_codes'],\n            query_sequence['visit_mask']\n        )\n        query_embed = visit_embeds.mean(dim=0).detach().numpy().reshape(1, -1)\n\n        distances, indices = self.index.search(query_embed, k)\n\n        similar_patients = [\n            {'patient_id': self.patient_ids[idx], 'distance': dist}\n            for idx, dist in zip(indices[0], distances[0])\n        ]\n\n        return similar_patients\n</code></pre></p>"},{"location":"implementation/visit-grouped-sequences/#54-application-4-risk-stratification","title":"5.4 Application 4: Risk Stratification","text":"<p>Use Case: Predict adverse outcomes (readmission, mortality)</p> <p>Architecture: <pre><code>Visit Sequences \u2192 Visit Encoder \u2192 Risk Predictor \u2192 Risk Score\n</code></pre></p>"},{"location":"implementation/visit-grouped-sequences/#part-6-implementation-roadmap","title":"Part 6: Implementation Roadmap","text":""},{"location":"implementation/visit-grouped-sequences/#phase-1-foundation-weeks-1-2","title":"Phase 1: Foundation (Weeks 1-2)","text":"<p>Goal: Set up data pipeline and adapters</p> <p>Tasks: 1. \u2705 Create directory structure 2. \u2705 Document implementation plan (this document) 3. \u2b1c Refactor legacy code \u2192 <code>legacy/</code> directory 4. \u2b1c Implement <code>BaseEHRAdapter</code> 5. \u2b1c Implement <code>SyntheaAdapter</code> 6. \u2b1c Implement <code>VisitGrouper</code> 7. \u2b1c Implement <code>PatientSequenceBuilder</code> 8. \u2b1c Create unit tests for adapters</p> <p>Deliverables: - Working data pipeline: Raw data \u2192 Visit sequences - Test suite with synthetic data - Documentation</p>"},{"location":"implementation/visit-grouped-sequences/#phase-2-pre-trained-model-integration-week-3","title":"Phase 2: Pre-trained Model Integration (Week 3)","text":"<p>Goal: Integrate CEHR-BERT embeddings</p> <p>Tasks: 1. \u2b1c Install CEHR-BERT dependencies 2. \u2b1c Implement <code>CEHRBERTWrapper</code> 3. \u2b1c Implement <code>VisitEncoder</code> 4. \u2b1c Create embedding extraction scripts 5. \u2b1c Validate embeddings (nearest neighbors, clustering)</p> <p>Deliverables: - Pre-trained embeddings loaded and validated - Visit encoder working - Embedding quality metrics</p>"},{"location":"implementation/visit-grouped-sequences/#phase-3-baseline-application-week-4","title":"Phase 3: Baseline Application (Week 4)","text":"<p>Goal: Implement disease progression model</p> <p>Tasks: 1. \u2b1c Implement <code>VisitGroupedProgressionModel</code> 2. \u2b1c Create CKD dataset builder 3. \u2b1c Implement training loop 4. \u2b1c Implement evaluation metrics 5. \u2b1c Run baseline experiments</p> <p>Deliverables: - Working disease progression model - Baseline results on synthetic data - Model checkpoints</p>"},{"location":"implementation/visit-grouped-sequences/#phase-4-additional-applications-weeks-5-6","title":"Phase 4: Additional Applications (Weeks 5-6)","text":"<p>Goal: Implement phenotyping and retrieval</p> <p>Tasks: 1. \u2b1c Implement <code>TemporalPhenotyper</code> 2. \u2b1c Implement <code>PatientRetrieval</code> 3. \u2b1c Create visualization notebooks 4. \u2b1c Run phenotyping experiments</p> <p>Deliverables: - Phenotyping pipeline - Patient retrieval system - Visualization dashboards</p>"},{"location":"implementation/visit-grouped-sequences/#phase-5-evaluation-documentation-week-7","title":"Phase 5: Evaluation &amp; Documentation (Week 7+)","text":"<p>Goal: Comprehensive evaluation and documentation</p> <p>Tasks: 1. \u2b1c Evaluate all applications 2. \u2b1c Create example notebooks 3. \u2b1c Write API documentation 4. \u2b1c Create user guide</p> <p>Deliverables: - Evaluation report - Example notebooks - Complete documentation</p>"},{"location":"implementation/visit-grouped-sequences/#part-7-open-questions-future-work","title":"Part 7: Open Questions &amp; Future Work","text":""},{"location":"implementation/visit-grouped-sequences/#open-questions","title":"Open Questions","text":"<ol> <li>Visit Definition Validation</li> <li>How to validate visit grouping accuracy?</li> <li>Compare encounter-based vs same-day on real data</li> <li> <p>Clinical expert review of visit boundaries</p> </li> <li> <p>Within-Visit Structure</p> </li> <li>Does preserving code types improve performance?</li> <li> <p>Ablation study: flat vs typed vs temporal</p> </li> <li> <p>Sequence Length</p> </li> <li>Optimal max_visits parameter?</li> <li> <p>Trade-off between history length and computational cost</p> </li> <li> <p>Pre-trained Model Selection</p> </li> <li>CEHR-BERT vs Med-BERT comparison</li> <li>Domain adaptation for specific diseases</li> </ol>"},{"location":"implementation/visit-grouped-sequences/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Multi-Modal Integration</li> <li>Add clinical notes (text)</li> <li>Add imaging reports</li> <li> <p>Add genomic data</p> </li> <li> <p>Hierarchical Summarization</p> </li> <li>For very long patient histories</li> <li> <p>Summarize old visits, keep recent ones detailed</p> </li> <li> <p>Real-Time Inference</p> </li> <li>Optimize for low-latency prediction</li> <li> <p>Incremental encoding for new visits</p> </li> <li> <p>Federated Learning</p> </li> <li>Train across multiple institutions</li> <li>Privacy-preserving patient similarity</li> </ol>"},{"location":"implementation/visit-grouped-sequences/#part-8-file-structure","title":"Part 8: File Structure","text":"<pre><code>ehr-sequencing/\n\u251c\u2500\u2500 src/ehrsequencing/\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 adapters/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base_adapter.py          \u2190 Part 2.2\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 synthea_adapter.py       \u2190 Part 2.3\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 mimic_adapter.py\n\u2502   \u2502   \u2514\u2500\u2500 sequences/\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 visit_grouper.py         \u2190 Part 3.1\n\u2502   \u2502       \u2514\u2500\u2500 sequence_builder.py      \u2190 Part 3.2\n\u2502   \u251c\u2500\u2500 embeddings/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 cehrbert_wrapper.py          \u2190 Part 4.2\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 visit_encoder.py             \u2190 Part 4.3\n\u2502   \u2502   \u2514\u2500\u2500 progression_model.py         \u2190 Part 5.1\n\u2502   \u251c\u2500\u2500 clustering/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 temporal_phenotyping.py      \u2190 Part 5.2\n\u2502   \u2514\u2500\u2500 retrieval/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 patient_similarity.py        \u2190 Part 5.3\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 build_sequences.py\n\u2502   \u251c\u2500\u2500 load_pretrained_embeddings.py\n\u2502   \u2514\u2500\u2500 train_progression_model.py\n\u251c\u2500\u2500 notebooks/\n\u2502   \u251c\u2500\u2500 01_data_exploration.ipynb\n\u2502   \u251c\u2500\u2500 02_visit_grouping.ipynb\n\u2502   \u251c\u2500\u2500 03_embedding_visualization.ipynb\n\u2502   \u2514\u2500\u2500 04_disease_progression.ipynb\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 test_adapters.py\n    \u251c\u2500\u2500 test_visit_grouper.py\n    \u2514\u2500\u2500 test_sequence_builder.py\n</code></pre>"},{"location":"implementation/visit-grouped-sequences/#summary","title":"Summary","text":"<p>This implementation plan provides:</p> <ol> <li>\u2705 Complete data pipeline - Raw EHR \u2192 Visit sequences</li> <li>\u2705 Data adapter reuse - Leverage loinc-predictor adapters</li> <li>\u2705 Design decisions - Visit definition, within-visit structure</li> <li>\u2705 Pre-trained integration - CEHR-BERT embeddings</li> <li>\u2705 Downstream applications - Disease progression, phenotyping, retrieval</li> <li>\u2705 Implementation roadmap - 7-week phased plan</li> <li>\u2705 Code templates - Production-ready implementations</li> </ol> <p>Next Steps: 1. Refactor legacy code 2. Implement Phase 1 (data pipeline) 3. Integrate CEHR-BERT (Phase 2) 4. Build baseline application (Phase 3)</p> <p>Document Version: 1.0 Last Updated: January 19, 2026 Next Review: After Phase 1 completion</p>"},{"location":"methods/","title":"Methods Documentation","text":"<p>This directory contains detailed documentation on the methodological approaches used in the EHR sequencing framework.</p>"},{"location":"methods/#disease-progression-modeling","title":"Disease Progression Modeling","text":""},{"location":"methods/#causal-survival-analysis-2-part-tutorial","title":"Causal Survival Analysis (2-Part Tutorial)","text":"<p>Why this matters: Simple classification tasks like \"does this patient have diabetes?\" can often be solved with rule-based methods. The real power of sequence modeling emerges when predicting disease progression over time.</p>"},{"location":"methods/#part-1-causal-progression-labels","title":"Part 1: Causal Progression Labels","text":"<p>Topics covered: - The most dangerous data leakage pattern in temporal prediction - Why patient-level labels + visit-level inputs = temporal leakage - Three diagnostic tests to detect leakage - Designing progression labels that respect causality - Three approaches: fixed-horizon, discrete-time survival, continuous-time survival</p> <p>Key insight: If a model can \"predict\" an outcome before clinical evidence exists, you have leakage. High AUC is not a virtue\u2014causality is.</p>"},{"location":"methods/#part-2-discrete-time-survival-modeling","title":"Part 2: Discrete-Time Survival Modeling","text":"<p>Topics covered: - What discrete-time survival modeling actually means - Understanding censoring (conceptually and operationally) - Deriving the likelihood formula from first principles - PyTorch implementation of the survival loss - Why this is causal by construction</p> <p>Key insight: Visits are the natural discretization for EHR data. Discrete-time survival modeling fits perfectly with visit-based sequences.</p> <p>Implementation: - Loss function: <code>src/ehrsequencing/models/losses.py::DiscreteTimeSurvivalLoss</code> - Model: <code>src/ehrsequencing/models/survival_lstm.py::DiscreteTimeSurvivalLSTM</code> - Training script: <code>examples/train_survival_lstm.py</code></p>"},{"location":"methods/#other-methodological-topics","title":"Other Methodological Topics","text":""},{"location":"methods/#within-visit-structure","title":"Within-Visit Structure","text":"<p>How to handle multiple medical codes within a single visit: - Bag-of-codes (mean pooling) - Attention mechanisms - Hierarchical encoding - Set-based representations</p>"},{"location":"methods/#lstm-variable-length-analysis","title":"LSTM Variable-Length Analysis","text":"<p>Technical details on handling variable-length sequences in LSTMs: - Padding and masking strategies - PackedSequence optimization - Memory considerations - Batch processing</p>"},{"location":"methods/#quick-reference","title":"Quick Reference","text":""},{"location":"methods/#when-to-use-each-approach","title":"When to Use Each Approach","text":"Task Approach Why Static classification Logistic regression, simple NN No temporal dynamics needed Fixed-horizon prediction LSTM + BCE loss Simple, interpretable, requires careful censoring Disease progression LSTM + discrete-time survival Natural for visits, handles censoring, causal Irregular timing Cox-style continuous-time Flexible timing, standard in epidemiology Multiple outcomes Competing risks survival Multiple event types, only one can occur first"},{"location":"methods/#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li>Classification: AUC, precision, recall, calibration</li> <li>Survival: Concordance index (C-index), calibration, survival curves</li> <li>Temporal: Time-dependent AUC, Brier score</li> </ul>"},{"location":"methods/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Temporal leakage: Using future information in predictions</li> <li>Censoring as negative: Treating censored patients as \"no event\"</li> <li>Ignoring visit frequency: Confounding surveillance with risk</li> <li>Patient-level labels: Losing temporal resolution</li> <li>No diagnostic tests: Not verifying causality</li> </ol>"},{"location":"methods/#getting-started","title":"Getting Started","text":""},{"location":"methods/#for-researchers","title":"For Researchers","text":"<ol> <li>Read Part 1 to understand the leakage problem</li> <li>Read Part 2 for implementation details</li> <li>Review the training script: <code>examples/train_survival_lstm.py</code></li> <li>Adapt to your specific outcome and dataset</li> </ol>"},{"location":"methods/#for-practitioners","title":"For Practitioners","text":"<ol> <li>Start with the training script: <code>examples/train_survival_lstm.py</code></li> <li>Modify the <code>create_survival_labels()</code> function for your outcome</li> <li>Adjust model hyperparameters as needed</li> <li>Evaluate with C-index and calibration plots</li> </ol>"},{"location":"methods/#for-students","title":"For Students","text":"<ol> <li>Work through the tutorials in order</li> <li>Implement the loss function from scratch (good exercise!)</li> <li>Compare discrete-time vs. fixed-horizon on the same data</li> <li>Run the diagnostic tests on the model</li> </ol>"},{"location":"methods/#references","title":"References","text":""},{"location":"methods/#survival-analysis","title":"Survival Analysis","text":"<ul> <li>Singer, J. D., &amp; Willett, J. B. (2003). Applied Longitudinal Data Analysis: Modeling Change and Event Occurrence. Oxford University Press.</li> <li>Tutz, G., &amp; Schmid, M. (2016). Modeling Discrete Time-to-Event Data. Springer.</li> <li>Cox, D. R. (1972). Regression models and life-tables. Journal of the Royal Statistical Society: Series B, 34(2), 187-202.</li> </ul>"},{"location":"methods/#ehr-sequence-modeling","title":"EHR Sequence Modeling","text":"<ul> <li>Choi, E., et al. (2016). RETAIN: An interpretable predictive model for healthcare using reverse time attention mechanism. NeurIPS.</li> <li>Rajkomar, A., et al. (2018). Scalable and accurate deep learning with electronic health records. npj Digital Medicine.</li> <li>Steinberg, E., et al. (2021). Language models are an effective representation learning technique for electronic health record data. Journal of Biomedical Informatics.</li> </ul>"},{"location":"methods/#temporal-causality","title":"Temporal Causality","text":"<ul> <li>Pearl, J. (2009). Causality: Models, Reasoning, and Inference (2<sup>nd</sup> ed.). Cambridge University Press.</li> <li>Hern\u00e1n, M. A., &amp; Robins, J. M. (2020). Causal Inference: What If. Chapman &amp; Hall/CRC.</li> </ul>"},{"location":"methods/#contributing","title":"Contributing","text":"<p>Found an error or have a suggestion? Please open an issue or submit a pull request.</p> <p>When adding new methods documentation: 1. Include mathematical derivations with intuition 2. Provide PyTorch implementation examples 3. Explain when to use vs. not use the method 4. Add references to key papers 5. Include common pitfalls and debugging tips</p>"},{"location":"methods/causal-survival-analysis-1/","title":"Causal Survival Analysis for EHR Sequences (Part 1)","text":""},{"location":"methods/causal-survival-analysis-1/#why-disease-progression-modeling-is-different","title":"Why Disease Progression Modeling is Different","text":"<p>When we move from simple classification tasks (like \"does this patient have diabetes?\") to disease progression modeling, we enter the realm of causal survival analysis. This is where many EHR sequence modeling papers go wrong\u2014not because the neural networks are poorly designed, but because the prediction task itself violates causality.</p> <p>This tutorial has two parts:</p> <p>Part 1 (this document):</p> <ol> <li>Understanding the most dangerous data leakage pattern in temporal prediction</li> <li>Designing progression labels that are both causal and statistically efficient</li> </ol> <p>Part 2 (causal-survival-analysis-2.md):</p> <ol> <li>Deep dive into discrete-time survival modeling</li> <li>Deriving the likelihood formula step-by-step</li> <li>Implementation details for PyTorch</li> </ol>"},{"location":"methods/causal-survival-analysis-1/#why-this-matters-the-diabetes-example","title":"Why This Matters: The Diabetes Example","text":"<p>Consider our diabetes prediction task from the LSTM notebook:</p> <pre><code># Task: Does patient have diabetes?\nlabel = 1 if any(diabetes_code in patient_history) else 0\n</code></pre> <p>This works, but it's not temporal. The answer depends only on the presence of certain codes\u2014a rule-based system could do this. We're not really using the sequential structure.</p> <p>Better question: Can we predict when a patient will progress from pre-diabetes to diabetes? Or from CKD stage 3 to stage 4? These are progression questions that require understanding disease dynamics over time.</p> <p>This is where sequence modeling becomes essential\u2014and where most papers introduce subtle but fatal leakage.</p>"},{"location":"methods/causal-survival-analysis-1/#part-i-the-most-dangerous-leakage-pattern","title":"Part I: The Most Dangerous Leakage Pattern","text":""},{"location":"methods/causal-survival-analysis-1/#the-setup-a-seemingly-reasonable-approach","title":"The Setup: A Seemingly Reasonable Approach","text":"<p>Consider a common approach to predicting disease progression. First, define a patient-level outcome:</p> <p>Clinical Question: \"Did this patient progress to CKD stage 4 within 1 year?\"</p> <p>This is a perfectly valid clinical question. Next, you build visit-level inputs:</p> <pre><code>visit 1 \u2192 visit 2 \u2192 visit 3 \u2192 \u2026 \u2192 visit T\n</code></pre> <p>Then\u2014and this is where the mistake happens\u2014you train like this:</p> <pre><code># Patient-level label (same for all visits)\ny_patient = 1  # if progression occurred within 1 year, else 0\n\n# Visit-level representations from LSTM\nout, _ = lstm(visit_sequences)  # shape: [batch_size, num_visits, hidden_dim]\n\n# Apply the SAME label at EVERY timestep\nfor t in range(num_visits):\n    loss += BCE(prediction_head(out[:, t]), y_patient)\n</code></pre>"},{"location":"methods/causal-survival-analysis-1/#why-this-is-leakage-not-just-suboptimal","title":"Why This Is Leakage (Not Just \"Suboptimal\")","text":"<p>This approach implicitly tells the model:</p> <p>\"At visit 1, predict something that is only knowable after visit T.\"</p> <p>This violates causality.</p>"},{"location":"methods/causal-survival-analysis-1/#the-formal-problem","title":"The Formal Problem","text":"<p>At visit \\(t\\):</p> <ul> <li>Your label depends on events in the window \\([t, t+365]\\) days</li> <li>Your input includes information about the patient's entire trajectory</li> <li>The model can see that this patient has future visits (sequence length, padding patterns)</li> </ul>"},{"location":"methods/causal-survival-analysis-1/#what-the-model-actually-learns","title":"What the Model Actually Learns","text":"<p>Instead of learning disease dynamics, the LSTM learns spurious correlations:</p> <ul> <li>Number of future visits: Patients who progress tend to have longer follow-up</li> <li>Visit density: Sicker patients have more frequent visits</li> <li>Padding patterns: Sequence length reveals outcome</li> <li>Time-to-last-visit: Abrupt termination signals events</li> </ul> <p>These are future-derived signals that won't be available at prediction time.</p>"},{"location":"methods/causal-survival-analysis-1/#why-performance-looks-amazing-and-why-reviewers-miss-it","title":"Why Performance Looks Amazing (And Why Reviewers Miss It)","text":"<p>The model doesn't need to understand disease biology. It exploits dataset artifacts:</p> Artifact What Model Learns Why It's Leakage Follow-up length Patients who progress \u2192 longer follow-up Not available prospectively Sequence termination Patients who die \u2192 abrupt ending Reveals outcome Visit frequency Severe disease \u2192 more frequent visits Confounds risk with surveillance"},{"location":"methods/causal-survival-analysis-1/#the-deceptive-metrics","title":"The Deceptive Metrics","text":"<p>This is why you see:</p> <ul> <li>AUC jumps from 0.72 \u2192 0.92 (looks like a breakthrough!)</li> <li>Perfect calibration on held-out test set</li> <li>Collapse in prospective validation (the model fails in real deployment)</li> </ul> <p>The model is \"cheating\" by using information that won't exist at prediction time.</p>"},{"location":"methods/causal-survival-analysis-1/#how-to-prove-leakage-is-happening","title":"How to Prove Leakage Is Happening","text":"<p>Here are three diagnostic tests that always catch temporal leakage:</p>"},{"location":"methods/causal-survival-analysis-1/#test-1-shuffle-visits-within-patients","title":"Test 1: Shuffle Visits Within Patients","text":"<pre><code># Randomly permute visit order for each patient\nshuffled_visits = [random.shuffle(patient_visits) for patient_visits in data]\nperformance_shuffled = evaluate_model(shuffled_visits)\n</code></pre> <p>Interpretation: If performance stays high, the model is not using temporal disease dynamics. It's relying on visit-level features or sequence statistics, not temporal patterns.</p>"},{"location":"methods/causal-survival-analysis-1/#test-2-truncate-sequences-at-random-times","title":"Test 2: Truncate Sequences at Random Times","text":"<pre><code># Cut each sequence at a random point\nfor patient in data:\n    truncate_at = random.randint(1, len(patient.visits))\n    patient.visits = patient.visits[:truncate_at]\n</code></pre> <p>Interpretation: If performance drops sharply, the model was using future context (sequence length, padding, etc.).</p>"},{"location":"methods/causal-survival-analysis-1/#test-3-predict-using-only-sequence-length","title":"Test 3: Predict Using Only Sequence Length","text":"<pre><code># Simplest possible baseline\ny_hat = logistic_regression(num_visits)\n</code></pre> <p>Interpretation: If this baseline is strong (AUC &gt; 0.75), the task is contaminated by follow-up artifacts. The outcome is predictable from metadata, not clinical content.</p>"},{"location":"methods/causal-survival-analysis-1/#part-ii-designing-causal-progression-labels-the-right-way","title":"Part II: Designing Causal Progression Labels (The Right Way)","text":"<p>Now the real work begins.</p>"},{"location":"methods/causal-survival-analysis-1/#step-1-decide-what-you-are-predicting-from-when","title":"Step 1: Decide What You Are Predicting From When","text":"<p>Every prediction must answer this question:</p> <p>\"At visit \\(t\\), using information up to \\(t\\), what am I predicting about the future?\"</p> <p>Examples of well-defined questions:</p> <ul> <li>Progression within next 6 months</li> <li>Progression before next visit</li> <li>Time to next disease stage</li> <li>Hazard of progression at this moment</li> </ul> <p>If this question cannot be answered with \"from when,\" the label is ill-defined.</p>"},{"location":"methods/causal-survival-analysis-1/#option-a-discrete-time-horizon-labels-most-practical","title":"Option A: Discrete-Time Horizon Labels (Most Practical)","text":"<p>This is the workhorse approach for most applications.</p>"},{"location":"methods/causal-survival-analysis-1/#definition","title":"Definition","text":"<p>For each visit \\(t\\), define:</p> \\[ y_t = \\begin{cases} 1 &amp; \\text{if progression occurs in } (t, t + \\Delta] \\\\ 0 &amp; \\text{if no progression in } (t, t + \\Delta] \\\\ \\text{censored} &amp; \\text{if follow-up} &lt; \\Delta \\end{cases} \\] <p>Where \\(\\Delta\\) is a fixed horizon (e.g., 180 days).</p>"},{"location":"methods/causal-survival-analysis-1/#implementation-logic","title":"Implementation Logic","text":"<p>For each visit:</p> <ol> <li>Look forward \\(\\Delta\\) days from visit \\(t\\)</li> <li>If progression occurs \u2192 label = 1 (positive)</li> <li>If patient is observed beyond \\(t + \\Delta\\) with no progression \u2192 label = 0 (negative)</li> <li>Otherwise \u2192 drop or mark as censored</li> </ol>"},{"location":"methods/causal-survival-analysis-1/#why-this-is-causal","title":"Why This Is Causal","text":"<ul> <li>Label uses only future after visit \\(t\\)</li> <li>Input uses only history before visit \\(t\\)</li> <li>No information flows backward in time</li> </ul>"},{"location":"methods/causal-survival-analysis-1/#why-its-statistically-efficient","title":"Why It's Statistically Efficient","text":"<ul> <li>Each patient contributes multiple labeled visits</li> <li>You're not collapsing the trajectory into one datapoint</li> <li>More training signal from the same data</li> </ul>"},{"location":"methods/causal-survival-analysis-1/#loss-masking","title":"Loss Masking","text":"<pre><code># Only compute loss on valid (non-censored) visits\nvalid_mask = (labels != CENSORED)\nloss = BCE(logits[valid_mask], labels[valid_mask])\n</code></pre>"},{"location":"methods/causal-survival-analysis-1/#example-ckd-staging","title":"Example: CKD Staging","text":"<p>At visit \\(t\\):</p> <ul> <li>Inputs: diagnoses, labs, medications, time since last visit</li> <li>Label: Did CKD stage increase within next 180 days?</li> <li>If yes \u2192 positive</li> <li>If no but follow-up \u2265 180 days \u2192 negative</li> <li>Else \u2192 censored</li> </ul> <p>Repeat for each visit in the patient's sequence.</p>"},{"location":"methods/causal-survival-analysis-1/#option-b-discrete-time-survival-modeling-cleaner-stronger","title":"Option B: Discrete-Time Survival Modeling (Cleaner, Stronger)","text":"<p>Instead of predicting a binary event in a fixed window, you predict hazard at each visit.</p>"},{"location":"methods/causal-survival-analysis-1/#define-hazard-at-visit-t","title":"Define Hazard at Visit \\(t\\)","text":"\\[ h_t = P(\\text{progression at } t+1 \\mid \\text{no progression before } t) \\] <p>Read this as:</p> <p>\"Given the patient has not progressed up to visit \\(t\\), what is the probability they progress before the next visit?\"</p> <p>The LSTM model outputs \\(h_t \\in (0, 1)\\) for each visit.</p>"},{"location":"methods/causal-survival-analysis-1/#likelihood-function","title":"Likelihood Function","text":"<p>For each patient, the likelihood of their observed sequence is:</p> \\[ \\mathcal{L} = \\prod_{t &lt; T^*} (1 - h_t) \\times \\begin{cases} h_{T^*} &amp; \\text{if event occurred} \\\\ 1 &amp; \\text{if censored} \\end{cases} \\] <p>Where \\(T^*\\) is the event or censoring time.</p> <p>Intuition:</p> <ul> <li>\\(\\prod_{t &lt; T^*} (1 - h_t)\\): Patient survived (didn't progress) through all visits before \\(T^*\\)</li> <li>\\(h_{T^*}\\): Patient progressed at visit \\(T^*\\) (if event observed)</li> <li>No hazard term if censored (we don't know what happened after)</li> </ul>"},{"location":"methods/causal-survival-analysis-1/#why-this-is-excellent","title":"Why This Is Excellent","text":"<ul> <li>Naturally handles censoring: No need to drop data</li> <li>No arbitrary horizon: Each visit contributes based on actual timing</li> <li>Directly models disease dynamics: Hazard is the fundamental quantity</li> </ul>"},{"location":"methods/causal-survival-analysis-1/#why-its-harder","title":"Why It's Harder","text":"<ul> <li>Must implement a custom loss function</li> <li>Interpretation is subtler than binary classification</li> <li>Requires understanding of survival analysis concepts</li> </ul>"},{"location":"methods/causal-survival-analysis-1/#implementation-preview","title":"Implementation Preview","text":"<pre><code>def discrete_time_survival_loss(hazards, event_times, event_indicators):\n    \"\"\"\n    hazards: [batch_size, num_visits] - predicted hazards\n    event_times: [batch_size] - time of event or censoring\n    event_indicators: [batch_size] - 1 if event, 0 if censored\n    \"\"\"\n    log_likelihood = 0\n\n    for i in range(batch_size):\n        T = event_times[i]\n\n        # Survival through all visits before T\n        log_likelihood += torch.sum(torch.log(1 - hazards[i, :T]))\n\n        # Event at T (if observed)\n        if event_indicators[i] == 1:\n            log_likelihood += torch.log(hazards[i, T])\n\n    return -log_likelihood  # Negative log-likelihood\n</code></pre> <p>We'll implement this fully in Part 2.</p>"},{"location":"methods/causal-survival-analysis-1/#option-c-continuous-time-survival-cox-style","title":"Option C: Continuous-Time Survival (Cox-Style)","text":"<p>This is the \"epidemiologist-approved\" approach.</p> <p>The LSTM model outputs a risk score \\(r_t\\), not a probability. The hazard function is:</p> \\[ \\lambda(t \\mid x_t) = \\lambda_0(t) \\exp(r_t) \\] <p>Where:</p> <ul> <li>\\(\\lambda_0(t)\\) is the baseline hazard (learned or assumed)</li> <li>\\(r_t\\) is the risk score from the LSTM</li> <li>This is the proportional hazards assumption</li> </ul> <p>Train with partial likelihood (Cox partial likelihood).</p>"},{"location":"methods/causal-survival-analysis-1/#when-this-shines","title":"When This Shines","text":"<ul> <li>Irregular visit spacing (visits at unpredictable times)</li> <li>Strong censoring (many patients lost to follow-up)</li> <li>Long follow-up windows (years of data)</li> <li>Need to compare with classical epidemiology methods</li> </ul>"},{"location":"methods/causal-survival-analysis-1/#when-its-overkill","title":"When It's Overkill","text":"<ul> <li>Early-stage prototyping</li> <li>Small datasets (&lt; 1000 patients)</li> <li>Regular visit patterns (monthly check-ups)</li> </ul>"},{"location":"methods/causal-survival-analysis-1/#how-to-avoid-throwing-away-data-statistical-efficiency","title":"How to Avoid Throwing Away Data (Statistical Efficiency)","text":"<p>The fear people have:</p> <p>\"If I censor aggressively, I'll lose half my visits.\"</p> <p>The fix is visit-level supervision, not patient-level.</p>"},{"location":"methods/causal-survival-analysis-1/#key-principle","title":"Key Principle","text":"<p>One patient = many training examples One visit = one prediction moment</p> <p>This is why LSTMs + visit-level labels are powerful when done correctly.</p>"},{"location":"methods/causal-survival-analysis-1/#comparison-patient-level-vs-visit-level","title":"Comparison: Patient-Level vs. Visit-Level","text":"Approach Training Examples Information Used Patient-level 1 per patient Entire trajectory collapsed Visit-level \\(T\\) per patient Each visit independently <p>With 1000 patients averaging 10 visits each:</p> <ul> <li>Patient-level: 1,000 training examples</li> <li>Visit-level: 10,000 training examples</li> </ul> <p>10x more data from the same patients!</p>"},{"location":"methods/causal-survival-analysis-1/#a-final-sanity-rule","title":"A Final Sanity Rule","text":"<p>If a model can \"predict\" an outcome before the clinical evidence exists, you have leakage.</p> <p>High AUC is not a virtue. Causality is.</p>"},{"location":"methods/causal-survival-analysis-1/#the-gold-standard-test","title":"The Gold Standard Test","text":"<p>Can you deploy this model prospectively?</p> <ul> <li>At visit \\(t\\), can you make a prediction using only data up to \\(t\\)?</li> <li>Does the prediction make sense clinically?</li> <li>Would a clinician trust it?</li> </ul> <p>If the answer to any of these is \"no,\" you have a problem.</p>"},{"location":"methods/causal-survival-analysis-1/#where-this-naturally-leads-next","title":"Where This Naturally Leads Next","text":"<p>Advanced topics for further exploration include:</p> <ul> <li>Joint modeling of visit frequency + disease progression</li> <li>Separating surveillance intensity from biological risk</li> <li>Counterfactual progression curves (\"what if no ACE inhibitor?\")</li> <li>Multi-state models (progression through multiple disease stages)</li> </ul> <p>That's where EHR sequencing becomes scientific, not just predictive.</p>"},{"location":"methods/causal-survival-analysis-1/#summary","title":"Summary","text":"<p>The Problem:</p> <ul> <li>Patient-level labels + visit-level inputs = temporal leakage</li> <li>Models learn dataset artifacts, not disease dynamics</li> </ul> <p>The Solution:</p> <ul> <li>Visit-level labels that respect causality</li> <li>Three options: fixed-horizon, discrete-time survival, continuous-time survival</li> <li>Choose based on the data characteristics and clinical question</li> </ul> <p>Next Steps:</p> <ul> <li>Part 2 dives deep into discrete-time survival modeling</li> <li>Derives the likelihood, implements the loss, and shows examples</li> <li>Demonstrates why this is the cleanest approach for visit-based EHR sequences</li> </ul> <p>Continue to: Part 2: Discrete-Time Survival Modeling</p>"},{"location":"methods/causal-survival-analysis-2/","title":"Causal Survival Analysis for EHR Sequences (Part 2)","text":""},{"location":"methods/causal-survival-analysis-2/#discrete-time-survival-modeling-from-intuition-to-implementation","title":"Discrete-Time Survival Modeling: From Intuition to Implementation","text":"<p>Previous: Part 1: Causal Progression Labels</p> <p>In Part 1, we identified the leakage problem and introduced three approaches to causal progression modeling. This tutorial focuses on discrete-time survival modeling\u2014the most natural fit for visit-based EHR sequences.</p> <p>We'll cover:</p> <ol> <li>What \"discrete-time survival\" actually means</li> <li>What censoring is, conceptually and operationally</li> <li>Deriving the likelihood formula step by step</li> <li>Implementing the loss function in PyTorch</li> </ol> <p>No symbols without intuition. Every equation will be built from first principles.</p>"},{"location":"methods/causal-survival-analysis-2/#1-what-discrete-time-survival-modeling-really-is","title":"1. What Discrete-Time Survival Modeling Really Is","text":""},{"location":"methods/causal-survival-analysis-2/#the-natural-discretization-visits","title":"The Natural Discretization: Visits","text":"<p>You already have a natural discretization of time: visits.</p> <p>Instead of modeling time as continuous (like Cox models with hazards per day), we say:</p> <p>Time advances in discrete steps: visit 1 \u2192 visit 2 \u2192 visit 3 \u2192 \u2026</p> <p>At each visit \\(t\\), the patient is in one of two states:</p> <ul> <li>Event-free (alive / not yet progressed)</li> <li>Event occurred (e.g., disease progressed, died, readmitted)</li> </ul> <p>We model the probability that the event happens at the next step, given it has not happened before.</p> <p>That conditional probability is the hazard.</p>"},{"location":"methods/causal-survival-analysis-2/#why-this-makes-sense-for-ehr-data","title":"Why This Makes Sense for EHR Data","text":"<p>EHR data is naturally discrete:</p> <ul> <li>Patients are observed at specific visits, not continuously</li> <li>Between visits, we have no information</li> <li>The \"next visit\" is the natural prediction horizon</li> </ul> <p>This is fundamentally different from continuous-time survival analysis (Cox models), which assumes you can observe events at any moment in time.</p>"},{"location":"methods/causal-survival-analysis-2/#2-the-hazard-in-discrete-time","title":"2. The Hazard in Discrete Time","text":""},{"location":"methods/causal-survival-analysis-2/#definition","title":"Definition","text":"<p>At visit \\(t\\), the hazard is:</p> \\[ h_t = P(T = t \\mid T \\geq t) = P(\\text{event at time } t \\mid \\text{survived to } t) \\] <p>Equivalently, in terms of visit intervals:</p> \\[ h_t = P(\\text{event in interval } (t-1, t] \\mid \\text{no event through } t-1) \\] <p>Read this as:</p> <p>\"Given the patient has not progressed through visit \\(t-1\\), what is the probability they progress by visit \\(t\\)?\"</p> <p>Important: The hazard \\(h_t\\) represents the risk for the current time point \\(t\\) (or equivalently, the interval since the last observation), not for some future time.</p>"},{"location":"methods/causal-survival-analysis-2/#key-properties","title":"Key Properties","text":"<p>This is NOT:</p> <ul> <li>Probability of ever progressing</li> <li>Cumulative risk over all time</li> <li>A patient-level summary statistic</li> </ul> <p>This IS:</p> <ul> <li>Local in time: specific to this moment</li> <li>Conditional: depends on survival up to now</li> <li>Causal: uses only history up to \\(t\\)</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#lstm-output-and-prediction-timing","title":"LSTM Output and Prediction Timing","text":"<p>Your LSTM outputs \\(h_t \\in (0, 1)\\) at each visit:</p> <pre><code># LSTM forward pass\nhidden_states, _ = lstm(visit_sequences)  # [batch, num_visits, hidden_dim]\n\n# Map to hazard (one value per visit)\nhazards = torch.sigmoid(hazard_head(hidden_states))  # [batch, num_visits]\n</code></pre> <p>The sigmoid ensures \\(0 &lt; h_t &lt; 1\\) (valid probability).</p> <p>Interpretation for prediction: - At visit \\(t\\), the LSTM has seen data through visit \\(t\\) - The hazard \\(h_t\\) represents: \"What is the risk that the event occurred by this visit?\" - For forward prediction, use \\(h_{t+1}\\) (the hazard at the next visit) to predict future risk - This maintains causality: predictions at time \\(t\\) use only data through time \\(t\\)</p>"},{"location":"methods/causal-survival-analysis-2/#3-what-censoring-is-and-why-its-unavoidable","title":"3. What Censoring Is (And Why It's Unavoidable)","text":""},{"location":"methods/causal-survival-analysis-2/#the-core-problem","title":"The Core Problem","text":"<p>In real data, you often don't know what eventually happened.</p> <p>Examples:</p> <ul> <li>Patient moved to another hospital</li> <li>Study ended before event occurred</li> <li>Patient still stable at last observation</li> <li>Lost to follow-up</li> </ul> <p>You only know:</p> <p>\"Up to this point, no event has occurred.\"</p> <p>That is censoring.</p>"},{"location":"methods/causal-survival-analysis-2/#formal-definition","title":"Formal Definition","text":"<p>A patient is right-censored if:</p> <ul> <li>They have no observed event</li> <li>But follow-up ends at time \\(T_c\\)</li> </ul> <p>Crucially:</p> <ul> <li>Censoring is not a negative outcome</li> <li>It means \"unknown beyond this point\"</li> <li>We cannot say the event didn't happen\u2014we just don't know</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#why-censoring-matters","title":"Why Censoring Matters","text":"<p>If you treat censored patients as \"no event,\" you're making a false assumption. This biases your model to underestimate risk.</p> <p>Correct approach: Use censoring information in the likelihood, but don't force a label.</p>"},{"location":"methods/causal-survival-analysis-2/#4-timeline-picture-mental-model","title":"4. Timeline Picture (Mental Model)","text":""},{"location":"methods/causal-survival-analysis-2/#patient-1-event-observed","title":"Patient 1: Event Observed","text":"<pre><code>Visit:    1 ---- 2 ---- 3 ---- 4 ---- 5\nEvent:                     \u2191\n                           progression\n</code></pre> <ul> <li>Visits 1-3: No event (contributes survival information)</li> <li>Visit 4: Event occurs (contributes hazard information)</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#patient-2-censored","title":"Patient 2: Censored","text":"<pre><code>Visit:    1 ---- 2 ---- 3\nEvent:          (study ends here)\n</code></pre> <ul> <li>Visits 1-3: No event (contributes survival information)</li> <li>After visit 3: Unknown (no contribution)</li> </ul> <p>Both patients contribute information, but differently.</p>"},{"location":"methods/causal-survival-analysis-2/#5-survival-probability-and-hazard-are-linked","title":"5. Survival Probability and Hazard Are Linked","text":""},{"location":"methods/causal-survival-analysis-2/#definitions","title":"Definitions","text":"<ul> <li>\\(h_t\\): hazard at visit \\(t\\)</li> <li>\\(S_t\\): probability of surviving (no event) through visit \\(t\\)</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#relationship","title":"Relationship","text":"\\[ S_t = \\prod_{k=1}^{t} (1 - h_k) \\]"},{"location":"methods/causal-survival-analysis-2/#interpretation","title":"Interpretation","text":"<p>To survive through time \\(t\\) (i.e., no event at times \\(1, 2, \\ldots, t\\)), you must not have the event at any step:</p> <ul> <li>No event at time 1: probability \\((1 - h_1)\\)</li> <li>No event at time 2: probability \\((1 - h_2)\\)</li> <li>...</li> <li>No event at time \\(t\\): probability \\((1 - h_t)\\)</li> </ul> <p>The overall survival probability \\(S_t = P(T &gt; t)\\) is the product of all these \"didn't happen\" factors.</p> <p>Note: \\(S_t\\) means \"survived through time \\(t\\)\" (equivalently, \"event-free through visit \\(t\\)\"), so \\(S_t = P(T &gt; t)\\).</p>"},{"location":"methods/causal-survival-analysis-2/#example","title":"Example","text":"<p>If hazards are constant at \\(h_t = 0.1\\) for all \\(t\\):</p> <ul> <li>\\(S_1 = 0.9\\)</li> <li>\\(S_2 = 0.9 \\times 0.9 = 0.81\\)</li> <li>\\(S_3 = 0.9 \\times 0.9 \\times 0.9 = 0.729\\)</li> <li>...</li> </ul> <p>Survival probability decreases over time, even with constant hazard.</p>"},{"location":"methods/causal-survival-analysis-2/#6-deriving-the-likelihood-formula","title":"6. Deriving the Likelihood Formula","text":"<p>Let's derive the likelihood for one patient from first principles.</p>"},{"location":"methods/causal-survival-analysis-2/#case-a-event-occurs-at-time-t","title":"Case A: Event Occurs at Time \\(T^*\\)","text":"<p>What must be true?</p> <ol> <li>No event at times \\(1, 2, \\ldots, T^*-1\\)</li> <li>Event occurs at time \\(T^*\\)</li> </ol> <p>Probability:</p> \\[ P = \\underbrace{(1 - h_1)(1 - h_2)\\cdots(1 - h_{T^*-1})}_{\\text{survived through time } T^*-1} \\times \\underbrace{h_{T^*}}_{\\text{event at time } T^*} \\] <p>Compactly:</p> \\[ \\mathcal{L} = \\left[\\prod_{t=1}^{T^*-1} (1 - h_t)\\right] \\cdot h_{T^*} \\] <p>Intuition: The patient \"rolled the dice\" at each time point and didn't have the event, until time \\(T^*\\) when the event occurred.</p> <p>Note: We write \\(\\prod_{t=1}^{T^*-1}\\) to make explicit that we're taking the product over times \\(1\\) through \\(T^*-1\\), not including \\(T^*\\).</p>"},{"location":"methods/causal-survival-analysis-2/#case-b-patient-is-censored-at-time-t_c","title":"Case B: Patient Is Censored at Time \\(T_c\\)","text":"<p>What do we know?</p> <ul> <li>No event at times \\(1, 2, \\ldots, T_c\\)</li> <li>After that: unknown</li> </ul> <p>Probability of observed data:</p> \\[ \\mathcal{L} = \\prod_{t=1}^{T_c} (1 - h_t) \\] <p>Notice:</p> <ul> <li>We only count survival through \\(T_c\\)</li> <li>No event hazard term (we don't know if/when the event happened after)</li> <li>We never claim the event didn't happen beyond \\(T_c\\)</li> <li>We just stop accumulating likelihood contributions</li> </ul> <p>Intuition: The patient survived through all observed time points. We don't know what happened after.</p> <p>Subtlety: Some formulations write this as \\(\\prod_{t=1}^{T_c-1} (1-h_t)\\) if censoring occurs \"just before\" time \\(T_c\\). Here we use the convention that censoring at \\(T_c\\) means the patient was observed event-free through time \\(T_c\\).</p>"},{"location":"methods/causal-survival-analysis-2/#case-c-unified-likelihood-formula","title":"Case C: Unified Likelihood Formula","text":"<p>For patient \\(i\\), the likelihood is:</p> \\[ \\mathcal{L}_i = \\left[\\prod_{t=1}^{T_i - 1} (1 - h_{it})\\right] \\times \\left[h_{iT_i}\\right]^{\\delta_i} \\] <p>Where:</p> <ul> <li>\\(T_i\\) is the observed time for patient \\(i\\) (event or censoring)</li> <li>\\(h_{it}\\) is the hazard at time \\(t\\) for patient \\(i\\)</li> <li>\\(\\delta_i\\) is the event indicator: \\(\\delta_i = 1\\) if event observed, \\(\\delta_i = 0\\) if censored</li> </ul> <p>Explanation: - The first term \\(\\prod_{t=1}^{T_i-1} (1 - h_{it})\\) is the probability of surviving through time \\(T_i - 1\\) - The second term \\(h_{iT_i}^{\\delta_i}\\) equals:   - \\(h_{iT_i}\\) if \\(\\delta_i = 1\\) (event observed)   - \\(1\\) if \\(\\delta_i = 0\\) (censored, no contribution from event hazard)</p> <p>That's the full likelihood. No magic. Just logic.</p>"},{"location":"methods/causal-survival-analysis-2/#7-why-this-formulation-is-powerful","title":"7. Why This Formulation Is Powerful","text":""},{"location":"methods/causal-survival-analysis-2/#1-censoring-is-handled-correctly","title":"1. Censoring Is Handled Correctly","text":"<ul> <li>Censored patients still contribute survival information</li> <li>They are not mislabeled as \"no event\"</li> <li>No data is thrown away</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#2-multiple-training-signals-per-patient","title":"2. Multiple Training Signals Per Patient","text":"<p>Each visit contributes:</p> <ul> <li>Either a \"did not progress yet\" factor: \\((1 - h_t)\\)</li> <li>Or an \"event happened now\" factor: \\(h_t\\)</li> </ul> <p>This is far more data-efficient than patient-level labels.</p> <p>With 1000 patients averaging 10 visits each:</p> <ul> <li>Patient-level: 1,000 training examples</li> <li>Visit-level (survival): ~10,000 training signals</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#3-natural-temporal-structure","title":"3. Natural Temporal Structure","text":"<p>The likelihood respects the sequential nature of the data:</p> <ul> <li>Earlier visits affect later probabilities through the survival product</li> <li>The model learns disease dynamics, not just static risk</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#8-turning-this-into-a-loss-function","title":"8. Turning This Into a Loss Function","text":"<p>Take logs (because products are numerically unstable):</p>"},{"location":"methods/causal-survival-analysis-2/#for-an-event-patient-event-at-time-t","title":"For an Event Patient (Event at Time \\(T^*\\))","text":"\\[ \\log \\mathcal{L} = \\sum_{t=1}^{T^*-1} \\log(1 - h_t) + \\log(h_{T^*}) \\]"},{"location":"methods/causal-survival-analysis-2/#for-a-censored-patient-censored-at-time-t_c","title":"For a Censored Patient (Censored at Time \\(T_c\\))","text":"\\[ \\log \\mathcal{L} = \\sum_{t=1}^{T_c} \\log(1 - h_t) \\]"},{"location":"methods/causal-survival-analysis-2/#unified-log-likelihood","title":"Unified Log-Likelihood","text":"\\[ \\log \\mathcal{L}_i = \\sum_{t=1}^{T_i - 1} \\log(1 - h_{it}) + \\delta_i \\log(h_{iT_i}) \\]"},{"location":"methods/causal-survival-analysis-2/#negative-log-likelihood-what-you-minimize","title":"Negative Log-Likelihood (What You Minimize)","text":"<p>For a batch of patients:</p> \\[ \\text{Loss} = -\\sum_{i=1}^{N} \\log \\mathcal{L}_i \\]"},{"location":"methods/causal-survival-analysis-2/#pseudocode","title":"Pseudocode","text":"<pre><code>loss = 0\nfor patient in batch:\n    for t in range(patient.num_visits):\n        if event_at_visit[t]:\n            loss -= log(hazard[t])\n            break  # Stop after event\n        else:\n            loss -= log(1 - hazard[t])\n</code></pre> <p>Mask appropriately across batch for variable-length sequences.</p>"},{"location":"methods/causal-survival-analysis-2/#9-pytorch-implementation","title":"9. PyTorch Implementation","text":"<p>Here's a complete implementation:</p> <pre><code>import torch\nimport torch.nn as nn\n\ndef discrete_time_survival_loss(hazards, event_times, event_indicators, sequence_mask):\n    \"\"\"\n    Discrete-time survival analysis loss function.\n\n    Args:\n        hazards: [batch_size, max_visits] - predicted hazards at each visit\n        event_times: [batch_size] - index of event or censoring (0-indexed)\n        event_indicators: [batch_size] - 1 if event observed, 0 if censored\n        sequence_mask: [batch_size, max_visits] - 1 for real visits, 0 for padding\n\n    Returns:\n        Negative log-likelihood (scalar)\n    \"\"\"\n    batch_size, max_visits = hazards.shape\n\n    # Clamp hazards to avoid log(0)\n    hazards = torch.clamp(hazards, min=1e-7, max=1 - 1e-7)\n\n    # Initialize log-likelihood\n    log_likelihood = torch.zeros(batch_size, device=hazards.device)\n\n    for i in range(batch_size):\n        T = event_times[i].item()  # Event or censoring time\n\n        # Contribution from survival up to time T\n        # Sum log(1 - h_t) for all t &lt; T\n        if T &gt; 0:\n            survival_log_prob = torch.sum(\n                torch.log(1 - hazards[i, :T]) * sequence_mask[i, :T]\n            )\n            log_likelihood[i] += survival_log_prob\n\n        # Contribution from event at time T (if observed)\n        if event_indicators[i] == 1 and T &lt; max_visits:\n            event_log_prob = torch.log(hazards[i, T])\n            log_likelihood[i] += event_log_prob\n\n    # Return negative log-likelihood (to minimize)\n    return -torch.mean(log_likelihood)\n\n\n# Alternative: Vectorized implementation (more efficient)\ndef discrete_time_survival_loss_vectorized(hazards, event_times, event_indicators, sequence_mask):\n    \"\"\"\n    Vectorized version for better performance.\n    \"\"\"\n    batch_size, max_visits = hazards.shape\n\n    # Clamp hazards\n    hazards = torch.clamp(hazards, min=1e-7, max=1 - 1e-7)\n\n    # Create time index tensor\n    time_idx = torch.arange(max_visits, device=hazards.device).unsqueeze(0)  # [1, max_visits]\n    event_times_expanded = event_times.unsqueeze(1)  # [batch_size, 1]\n\n    # Mask for visits before event/censoring\n    before_event_mask = (time_idx &lt; event_times_expanded).float() * sequence_mask\n\n    # Mask for event visit\n    at_event_mask = (time_idx == event_times_expanded).float() * sequence_mask\n\n    # Log-likelihood from survival (all visits before event)\n    survival_ll = torch.sum(\n        torch.log(1 - hazards) * before_event_mask,\n        dim=1\n    )\n\n    # Log-likelihood from event (only if event observed)\n    event_ll = torch.sum(\n        torch.log(hazards) * at_event_mask,\n        dim=1\n    ) * event_indicators\n\n    # Total log-likelihood\n    log_likelihood = survival_ll + event_ll\n\n    return -torch.mean(log_likelihood)\n</code></pre>"},{"location":"methods/causal-survival-analysis-2/#usage-example","title":"Usage Example","text":"<pre><code># Model output\nhazards = model(visit_sequences)  # [batch_size, num_visits]\n\n# Ground truth\nevent_times = torch.tensor([5, 3, 10, 7])  # Visit index of event/censoring\nevent_indicators = torch.tensor([1, 1, 0, 1])  # 1=event, 0=censored\nsequence_mask = create_sequence_mask(visit_sequences)  # [batch, num_visits]\n\n# Compute loss\nloss = discrete_time_survival_loss_vectorized(\n    hazards, event_times, event_indicators, sequence_mask\n)\n\n# Backprop\nloss.backward()\noptimizer.step()\n</code></pre>"},{"location":"methods/causal-survival-analysis-2/#10-how-the-lstm-fits-in","title":"10. How the LSTM Fits In","text":""},{"location":"methods/causal-survival-analysis-2/#architecture","title":"Architecture","text":"<pre><code>class DiscreteTimeSurvivalLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.hazard_head = nn.Linear(hidden_dim, 1)\n\n    def forward(self, visit_codes, visit_mask, sequence_mask):\n        \"\"\"\n        Args:\n            visit_codes: [batch, num_visits, max_codes_per_visit]\n            visit_mask: [batch, num_visits, max_codes_per_visit]\n            sequence_mask: [batch, num_visits]\n\n        Returns:\n            hazards: [batch, num_visits] - hazard at each visit\n        \"\"\"\n        # Embed codes\n        embeddings = self.embedding(visit_codes)  # [B, V, C, E]\n\n        # Aggregate codes within each visit (mean pooling)\n        visit_mask_expanded = visit_mask.unsqueeze(-1)  # [B, V, C, 1]\n        visit_vectors = (embeddings * visit_mask_expanded).sum(dim=2) / \\\n                       visit_mask.sum(dim=2, keepdim=True).clamp(min=1)  # [B, V, E]\n\n        # LSTM over visits\n        lstm_out, _ = self.lstm(visit_vectors)  # [B, V, H]\n\n        # Map to hazard (sigmoid for valid probability)\n        hazards = torch.sigmoid(self.hazard_head(lstm_out)).squeeze(-1)  # [B, V]\n\n        return hazards\n</code></pre>"},{"location":"methods/causal-survival-analysis-2/#training-loop","title":"Training Loop","text":"<pre><code>model = DiscreteTimeSurvivalLSTM(vocab_size=5000, embedding_dim=128, hidden_dim=256)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # Forward pass\n        hazards = model(\n            batch['visit_codes'],\n            batch['visit_mask'],\n            batch['sequence_mask']\n        )\n\n        # Compute loss\n        loss = discrete_time_survival_loss_vectorized(\n            hazards,\n            batch['event_times'],\n            batch['event_indicators'],\n            batch['sequence_mask']\n        )\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"methods/causal-survival-analysis-2/#11-why-this-is-causal-by-construction","title":"11. Why This Is Causal by Construction","text":"<p>At time \\(t\\):</p> <ul> <li>Input to the LSTM uses only history through time \\(t\\) (due to LSTM causality)</li> <li>Hazard \\(h_t\\) represents the risk at time \\(t\\) (given history through \\(t-1\\))</li> <li>For prediction at time \\(t\\): use \\(h_{t+1}, h_{t+2}, \\ldots\\) to forecast future risk</li> <li>No future leakage is possible (assuming correct masking)</li> </ul> <p>This is the cleanest way to do progression modeling with sequences.</p>"},{"location":"methods/causal-survival-analysis-2/#clarifying-prediction-vs-estimation","title":"Clarifying Prediction vs. Estimation","text":"<p>During training: - At each time \\(t\\), we estimate \\(h_t\\) using data through time \\(t\\) - This is appropriate for likelihood computation (we're modeling the data generation process)</p> <p>During inference/prediction: - If we want to predict future risk from visit \\(t\\), we look at \\(h_{t+1}, h_{t+2}, \\ldots\\) - These represent \"what will happen next\" from the perspective of time \\(t\\) - Each \\(h_{t+k}\\) uses only information through time \\(t+k\\), maintaining causality</p>"},{"location":"methods/causal-survival-analysis-2/#verifying-causality","title":"Verifying Causality","text":"<pre><code># Test: Shuffle future visits (shouldn't affect prediction at time t)\ndef test_causality(model, sequence, t):\n    # Prediction at time t\n    pred_original = model(sequence[:t+1])\n\n    # Shuffle visits after t\n    shuffled = sequence.copy()\n    shuffled[t+1:] = np.random.permutation(shuffled[t+1:])\n\n    # Prediction should be identical\n    pred_shuffled = model(shuffled[:t+1])\n\n    assert torch.allclose(pred_original, pred_shuffled)\n</code></pre>"},{"location":"methods/causal-survival-analysis-2/#12-common-misunderstandings","title":"12. Common Misunderstandings","text":""},{"location":"methods/causal-survival-analysis-2/#isnt-this-just-many-binary-classifiers","title":"\"Isn't this just many binary classifiers?\"","text":"<p>No.</p> <ul> <li>The hazards are coupled through survival: \\(S_t = \\prod_{k=1}^t (1 - h_k)\\)</li> <li>Earlier predictions affect later likelihood</li> <li>The model learns temporal dependencies, not independent classifications</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#does-censoring-mean-negative","title":"\"Does censoring mean negative?\"","text":"<p>No.</p> <ul> <li>Negative = observed no event in a specific window</li> <li>Censored = unknown beyond this point</li> <li>They are fundamentally different concepts</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#why-not-just-use-cox-regression","title":"\"Why not just use Cox regression?\"","text":"<p>You can\u2014but:</p> <ul> <li>Cox assumes proportional hazards (hazard ratios constant over time)</li> <li>Discrete-time lets the network learn time-varying risk flexibly</li> <li>Easier to integrate with visit-based LSTMs</li> <li>No need to estimate baseline hazard</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#13-a-final-intuition-anchor","title":"13. A Final Intuition Anchor","text":"<p>Think of each time point as asking:</p> <p>\"Did the event occur at this time (in the interval since the last observation)?\"</p> <ul> <li>Most of the time, the answer is \"not yet\" \u2192 contributes \\((1 - h_t)\\)</li> <li>Once (for event patients), the answer is \"yes\" \u2192 contributes \\(h_t\\)</li> <li>Sometimes, you stop asking \u2192 censored (no more likelihood contributions)</li> </ul> <p>That's discrete-time survival.</p>"},{"location":"methods/causal-survival-analysis-2/#reconciling-with-prediction","title":"Reconciling with \"Prediction\"","text":"<p>If you think in terms of \"predicting the future\": - At time \\(t\\) (having seen data through \\(t\\)), you've computed \\(h_t\\) - To predict \"what happens next,\" you'd look at \\(h_{t+1}\\) (which uses data through \\(t+1\\)) - But during training, we're not predicting\u2014we're estimating the hazard function that generated the observed data</p>"},{"location":"methods/causal-survival-analysis-2/#14-practical-considerations","title":"14. Practical Considerations","text":""},{"location":"methods/causal-survival-analysis-2/#handling-irregular-visit-spacing","title":"Handling Irregular Visit Spacing","text":"<p>If visits are irregularly spaced, you can:</p> <ol> <li> <p>Include time delta as a feature:    <pre><code>time_since_last_visit = compute_time_deltas(visit_timestamps)\nlstm_input = torch.cat([visit_vectors, time_deltas], dim=-1)\n</code></pre></p> </li> <li> <p>Use time-dependent hazard:    <pre><code>hazard = sigmoid(W_h @ hidden + W_t @ time_delta + b)\n</code></pre></p> </li> </ol>"},{"location":"methods/causal-survival-analysis-2/#competing-risks","title":"Competing Risks","text":"<p>If multiple event types can occur (e.g., progression vs. death):</p> <pre><code># Multi-output hazard head\nhazards = softmax(hazard_head(lstm_out))  # [B, V, num_event_types]\n\n# Cause-specific likelihood\nfor event_type in range(num_event_types):\n    if observed_event == event_type:\n        ll += log(hazards[:, :, event_type])\n</code></pre>"},{"location":"methods/causal-survival-analysis-2/#recurrent-events","title":"Recurrent Events","text":"<p>For events that can happen multiple times:</p> <ul> <li>Reset after each event</li> <li>Model inter-event times</li> <li>Use counting process formulation</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#15-next-steps","title":"15. Next Steps","text":""},{"location":"methods/causal-survival-analysis-2/#implementation-checklist","title":"Implementation Checklist","text":"<ul> <li> Implement discrete-time survival loss</li> <li> Create data loader with event times and indicators</li> <li> Train LSTM with survival objective</li> <li> Evaluate with concordance index (C-index)</li> <li> Compare with fixed-horizon baseline</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#evaluation-metrics","title":"Evaluation Metrics","text":"<ul> <li>Concordance Index (C-index): Measures ranking of predicted risks</li> <li>Calibration plots: Are predicted hazards well-calibrated?</li> <li>Survival curves: Plot \\(S_t\\) for different risk groups</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#extensions","title":"Extensions","text":"<ul> <li>Attention mechanisms: Learn which visits are most important</li> <li>Multi-task learning: Joint prediction of multiple outcomes</li> <li>Counterfactual analysis: \"What if treatment was different?\"</li> </ul>"},{"location":"methods/causal-survival-analysis-2/#summary","title":"Summary","text":"<p>What We Learned:</p> <ol> <li>Discrete-time survival is natural for visit-based EHR data</li> <li>Hazard is a conditional probability at each visit</li> <li>Censoring is handled correctly in the likelihood</li> <li>The loss function is derived from first principles</li> <li>Implementation in PyTorch is straightforward</li> </ol> <p>Why This Matters:</p> <ul> <li>Respects causality (no temporal leakage)</li> <li>Efficient use of data (multiple signals per patient)</li> <li>Flexible modeling of time-varying risk</li> <li>Clinically interpretable (hazard = \"risk right now\")</li> </ul> <p>Next:</p> <ul> <li>Implement this in a notebook</li> <li>Apply to real EHR data (CKD progression, readmission, etc.)</li> <li>Compare with simpler baselines</li> </ul> <p>Previous: Part 1: Causal Progression Labels</p>"},{"location":"methods/lstm-variable-length-analysis/","title":"Analysis: How Our LSTM Baseline Handles Variable-Length Sequences","text":"<p>Date: January 21, 2026 Model: <code>src/ehrsequencing/models/lstm_baseline.py</code></p>"},{"location":"methods/lstm-variable-length-analysis/#summary","title":"Summary","text":"<p>Our current LSTM baseline implementation correctly handles variable-length sequences using <code>pack_padded_sequence</code>. Here's the detailed analysis.</p>"},{"location":"methods/lstm-variable-length-analysis/#implementation-review","title":"Implementation Review","text":""},{"location":"methods/lstm-variable-length-analysis/#code-location-lines-256-278","title":"Code Location: Lines 256-278","text":"<pre><code># Apply LSTM across visits\nif sequence_mask is not None:\n    # Pack padded sequence for efficiency\n    lengths = sequence_mask.sum(dim=1).cpu()\n    packed_input = nn.utils.rnn.pack_padded_sequence(\n        visit_vectors,\n        lengths,\n        batch_first=True,\n        enforce_sorted=False\n    )\n    packed_output, (hidden, cell) = self.lstm(packed_input)\n    lstm_output, _ = nn.utils.rnn.pad_packed_sequence(\n        packed_output,\n        batch_first=True\n    )\nelse:\n    lstm_output, (hidden, cell) = self.lstm(visit_vectors)\n\n# Use last hidden state for prediction\nif self.bidirectional:\n    # Concatenate forward and backward final hidden states\n    final_hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\nelse:\n    final_hidden = hidden[-1]\n</code></pre>"},{"location":"methods/lstm-variable-length-analysis/#what-we-do-correctly","title":"What We Do Correctly \u2705","text":""},{"location":"methods/lstm-variable-length-analysis/#1-use-packed-sequences","title":"1. Use Packed Sequences","text":"<pre><code>packed_input = nn.utils.rnn.pack_padded_sequence(\n    visit_vectors,\n    lengths,\n    batch_first=True,\n    enforce_sorted=False  # \u2705 Allows unsorted batch\n)\n</code></pre> <p>Why this is correct: - LSTM never processes padding - Recurrence stops at each patient's true last visit - Computationally efficient</p>"},{"location":"methods/lstm-variable-length-analysis/#2-extract-true-lengths-from-mask","title":"2. Extract True Lengths from Mask","text":"<pre><code>lengths = sequence_mask.sum(dim=1).cpu()\n</code></pre> <p>Input: <code>sequence_mask</code> is <code>[batch_size, num_visits]</code> with 1 for real visits, 0 for padding</p> <p>Example: <pre><code>sequence_mask = torch.tensor([\n    [1, 1, 1, 0, 0],  # Patient 0: 3 visits\n    [1, 1, 1, 1, 1],  # Patient 1: 5 visits\n    [1, 1, 0, 0, 0],  # Patient 2: 2 visits\n])\nlengths = tensor([3, 5, 2])  # \u2705 Correct\n</code></pre></p>"},{"location":"methods/lstm-variable-length-analysis/#3-use-correct-final-hidden-state","title":"3. Use Correct Final Hidden State","text":"<pre><code>final_hidden = hidden[-1]  # \u2705 NOT lstm_output[:, -1, :]\n</code></pre> <p>Why this is correct: - <code>hidden[-1]</code> contains the hidden state at each patient's true last visit - This is guaranteed by <code>pack_padded_sequence</code> - No contamination from padding</p>"},{"location":"methods/lstm-variable-length-analysis/#4-handle-bidirectional-lstm-correctly","title":"4. Handle Bidirectional LSTM Correctly","text":"<pre><code>if self.bidirectional:\n    final_hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n</code></pre> <p>Why this is correct: - <code>hidden[-2]</code> = forward direction final state - <code>hidden[-1]</code> = backward direction final state - Concatenating gives full bidirectional representation</p>"},{"location":"methods/lstm-variable-length-analysis/#5-support-optional-packing","title":"5. Support Optional Packing","text":"<pre><code>if sequence_mask is not None:\n    # Use packed sequences\nelse:\n    # Fall back to regular LSTM\n    lstm_output, (hidden, cell) = self.lstm(visit_vectors)\n</code></pre> <p>Why this is good: - Allows usage without masks (e.g., for debugging) - Graceful degradation</p>"},{"location":"methods/lstm-variable-length-analysis/#two-level-masking-strategy","title":"Two-Level Masking Strategy","text":"<p>Our implementation uses two levels of masking, which is the correct approach:</p>"},{"location":"methods/lstm-variable-length-analysis/#level-1-visit-level-masking","title":"Level 1: Visit-Level Masking","text":"<pre><code>visit_mask: [batch_size, num_visits, max_codes_per_visit]\n</code></pre> <p>Purpose: Mask padding codes within each visit</p> <p>Handled by: <code>VisitEncoder</code> (lines 246-252)</p> <pre><code>visit_vectors = self.visit_encoder(code_embeddings_flat, visit_mask_flat)\n</code></pre>"},{"location":"methods/lstm-variable-length-analysis/#level-2-sequence-level-masking","title":"Level 2: Sequence-Level Masking","text":"<pre><code>sequence_mask: [batch_size, num_visits]\n</code></pre> <p>Purpose: Mask padding visits in the sequence</p> <p>Handled by: <code>pack_padded_sequence</code> (lines 256-269)</p> <p>This is the correct architecture because: 1. Visits can have variable numbers of codes 2. Patients can have variable numbers of visits 3. Both need separate masking</p>"},{"location":"methods/lstm-variable-length-analysis/#what-could-be-improved","title":"What Could Be Improved \ud83d\udd27","text":""},{"location":"methods/lstm-variable-length-analysis/#issue-1-no-explicit-loss-masking-for-visit-level-predictions","title":"Issue 1: No Explicit Loss Masking for Visit-Level Predictions","text":"<p>Current implementation: Only supports patient-level prediction (many-to-one)</p> <p>What's missing: If we want visit-level predictions (many-to-many), we need to mask the loss:</p> <pre><code># This is NOT currently in the model\ndef compute_visit_level_loss(self, logits, labels, lengths):\n    \"\"\"\n    Compute loss only on real visits, not padding.\n\n    Args:\n        logits: [batch_size, max_visits, num_classes]\n        labels: [batch_size, max_visits]\n        lengths: [batch_size] - number of real visits\n    \"\"\"\n    batch_size, max_visits = logits.shape[:2]\n\n    # Create mask\n    mask = torch.arange(max_visits)[None, :] &lt; lengths[:, None]\n\n    # Masked loss\n    loss = self.criterion(logits[mask], labels[mask])\n    return loss\n</code></pre> <p>Recommendation: Add this to the <code>Trainer</code> class or as a utility function.</p>"},{"location":"methods/lstm-variable-length-analysis/#issue-2-no-validation-of-lengths","title":"Issue 2: No Validation of Lengths","text":"<p>Current code: <pre><code>lengths = sequence_mask.sum(dim=1).cpu()\n</code></pre></p> <p>Potential issue: If all sequences have length 0, <code>pack_padded_sequence</code> will fail</p> <p>Suggested fix: <pre><code>lengths = sequence_mask.sum(dim=1).cpu()\nif (lengths == 0).any():\n    raise ValueError(\"Found sequences with zero length. All sequences must have at least 1 visit.\")\n</code></pre></p>"},{"location":"methods/lstm-variable-length-analysis/#issue-3-no-explicit-documentation-of-masking-requirements","title":"Issue 3: No Explicit Documentation of Masking Requirements","text":"<p>Current docstring: <pre><code>def forward(\n    self,\n    visit_codes: torch.Tensor,\n    visit_mask: Optional[torch.Tensor] = None,\n    sequence_mask: Optional[torch.Tensor] = None,\n    ...\n):\n</code></pre></p> <p>Recommendation: Add explicit documentation:</p> <pre><code>\"\"\"\nArgs:\n    visit_codes: [batch_size, num_visits, max_codes_per_visit]\n        Medical codes for each visit\n    visit_mask: [batch_size, num_visits, max_codes_per_visit]\n        Binary mask: 1 for real codes, 0 for padding codes\n        IMPORTANT: Must be provided for variable-length visits\n    sequence_mask: [batch_size, num_visits]\n        Binary mask: 1 for real visits, 0 for padding visits\n        IMPORTANT: Must be provided for variable-length sequences\n        If not provided, assumes all visits are real (no padding)\n\"\"\"\n</code></pre>"},{"location":"methods/lstm-variable-length-analysis/#comparison-with-best-practices","title":"Comparison with Best Practices","text":"<p>Let's check against the best practices from the tutorial:</p> Best Practice Our Implementation Status Use <code>pack_padded_sequence</code> \u2705 Lines 259-264 \u2705 Pass Use <code>h_n[-1]</code> for final state \u2705 Line 278 \u2705 Pass Mask losses for visit-level \u274c Not implemented \u26a0\ufe0f Missing Define labels causally N/A (model-level) - Never process padding \u2705 Via packing \u2705 Pass Validate lengths &gt; 0 \u274c Not checked \u26a0\ufe0f Missing <p>Overall: \u00be implemented correctly, 2 minor improvements needed</p>"},{"location":"methods/lstm-variable-length-analysis/#test-coverage","title":"Test Coverage","text":"<p>Our tests verify variable-length handling:</p>"},{"location":"methods/lstm-variable-length-analysis/#test-test_variable_length_sequences","title":"Test: <code>test_variable_length_sequences</code>","text":"<pre><code>def test_variable_length_sequences(self, model_config):\n    \"\"\"Test model handles variable-length sequences.\"\"\"\n    # Create variable-length masks\n    sequence_mask[0, :5] = 1   # 5 visits\n    sequence_mask[1, :8] = 1   # 8 visits\n    sequence_mask[2, :3] = 1   # 3 visits\n    sequence_mask[3, :6] = 1   # 6 visits\n\n    output = model(visit_codes, visit_mask, sequence_mask)\n\n    assert output['logits'].shape == (batch_size, 1)\n    assert not torch.isnan(output['logits']).any()\n</code></pre> <p>Status: \u2705 Passing</p> <p>This confirms that: - Variable lengths are handled without errors - No NaN values are produced - Output shape is correct</p>"},{"location":"methods/lstm-variable-length-analysis/#recommendations","title":"Recommendations","text":""},{"location":"methods/lstm-variable-length-analysis/#priority-1-add-loss-masking-utility","title":"Priority 1: Add Loss Masking Utility","text":"<p>Add to <code>src/ehrsequencing/training/trainer.py</code>:</p> <pre><code>def compute_masked_loss(\n    criterion: nn.Module,\n    predictions: torch.Tensor,\n    labels: torch.Tensor,\n    lengths: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute loss only on real timesteps, masking padding.\n\n    Args:\n        criterion: Loss function\n        predictions: [batch_size, max_timesteps, ...] predictions\n        labels: [batch_size, max_timesteps, ...] labels\n        lengths: [batch_size] number of real timesteps per sequence\n\n    Returns:\n        Masked loss (scalar)\n    \"\"\"\n    batch_size, max_timesteps = predictions.shape[:2]\n\n    # Create mask: [batch_size, max_timesteps]\n    mask = torch.arange(max_timesteps, device=predictions.device)[None, :] &lt; lengths[:, None]\n\n    # Apply mask and compute loss\n    if predictions.dim() == 3:\n        # Multi-class: [B, T, K]\n        mask = mask.unsqueeze(-1).expand_as(predictions)\n\n    masked_predictions = predictions[mask]\n    masked_labels = labels[mask]\n\n    return criterion(masked_predictions, masked_labels)\n</code></pre>"},{"location":"methods/lstm-variable-length-analysis/#priority-2-add-length-validation","title":"Priority 2: Add Length Validation","text":"<p>Add to <code>LSTMBaseline.forward()</code>:</p> <pre><code>if sequence_mask is not None:\n    lengths = sequence_mask.sum(dim=1).cpu()\n\n    # Validate\n    if (lengths == 0).any():\n        raise ValueError(\n            f\"Found {(lengths == 0).sum()} sequences with zero length. \"\n            \"All sequences must have at least 1 visit.\"\n        )\n\n    packed_input = nn.utils.rnn.pack_padded_sequence(...)\n</code></pre>"},{"location":"methods/lstm-variable-length-analysis/#priority-3-enhance-documentation","title":"Priority 3: Enhance Documentation","text":"<p>Update docstrings to explicitly document masking requirements and behavior.</p>"},{"location":"methods/lstm-variable-length-analysis/#conclusion","title":"Conclusion","text":"<p>Our LSTM baseline correctly handles variable-length sequences using the industry-standard <code>pack_padded_sequence</code> approach. The implementation:</p> <p>\u2705 Strengths: - Proper use of packed sequences - Correct extraction of final hidden states - Two-level masking (codes and visits) - Bidirectional LSTM support - Tested with variable lengths</p> <p>\u26a0\ufe0f Minor gaps: - No visit-level loss masking (only needed for many-to-many tasks) - No length validation - Documentation could be more explicit</p> <p>Overall assessment: Production-ready for patient-level prediction tasks. Needs minor additions for visit-level prediction tasks.</p> <p>Related Documents: - Tutorial: <code>docs/methods/variable-length-sequences.md</code> - Implementation: <code>src/ehrsequencing/models/lstm_baseline.py</code> - Tests: <code>tests/test_lstm_model.py</code></p>"},{"location":"methods/modern-code-embeddings/","title":"Modern Medical Code Embeddings for EHR Sequencing","text":"<p>Date: January 19, 2026 Status: Methodology &amp; Implementation Guide</p>"},{"location":"methods/modern-code-embeddings/#overview","title":"Overview","text":"<p>This document presents modern approaches to learning embeddings for medical codes (ICD, LOINC, SNOMED, RxNorm) that capture: - Semantic relationships - Similar codes have similar embeddings - Temporal dependencies - Account for ordering and time gaps - Long-range dependencies - Chronic conditions spanning years - Hierarchical structure - Medical ontologies and code hierarchies</p> <p>Key Evolution from Word2Vec: - Word2Vec (2013): Static embeddings, no temporal awareness - Modern approaches (2024-2026): Temporal, hierarchical, context-aware</p>"},{"location":"methods/modern-code-embeddings/#part-1-problem-formulation","title":"Part 1: Problem Formulation","text":""},{"location":"methods/modern-code-embeddings/#input-patient-medical-history","title":"Input: Patient Medical History","text":"<pre><code>Patient P001 Timeline:\n  2020-01-15 09:00:00 | LOINC:4548-4    | Hemoglobin A1c\n  2020-01-15 09:00:00 | SNOMED:44054006 | Diabetes mellitus type 2\n  2020-01-15 09:00:00 | RXNORM:860975   | Metformin 500 MG\n  2020-06-15 10:30:00 | LOINC:4548-4    | Hemoglobin A1c\n  2020-12-15 14:00:00 | ICD10:E11.9     | Type 2 diabetes without complications\n  2021-06-15 09:15:00 | LOINC:2339-0    | Glucose\n</code></pre>"},{"location":"methods/modern-code-embeddings/#goal-learn-embedding-function","title":"Goal: Learn Embedding Function","text":"<pre><code>f: Code \u2192 \u211d^d\n\nExamples:\n  f(ICD10:E11.9)    \u2192 [0.23, -0.45, 0.67, ..., 0.12]  # 128-dim vector\n  f(LOINC:4548-4)   \u2192 [0.19, -0.41, 0.71, ..., 0.08]  # Similar to E11.9 (diabetes)\n  f(RXNORM:860975)  \u2192 [0.21, -0.43, 0.69, ..., 0.10]  # Metformin (diabetes drug)\n</code></pre> <p>Desired Properties: 1. Semantic similarity: Diabetes-related codes cluster together 2. Temporal awareness: Codes appearing in sequence have meaningful relationships 3. Long-range dependencies: Chronic conditions maintain coherence over time 4. Hierarchy preservation: Parent-child relationships in ontologies</p>"},{"location":"methods/modern-code-embeddings/#part-2-modern-embedding-approaches","title":"Part 2: Modern Embedding Approaches","text":""},{"location":"methods/modern-code-embeddings/#approach-1-temporal-skip-gram-med2vec","title":"Approach 1: Temporal Skip-gram (Med2Vec++)","text":"<p>Evolution from Word2Vec: - Original Med2Vec: Context window ignores time gaps - Modern: Exponential decay based on time distance</p>"},{"location":"methods/modern-code-embeddings/#architecture","title":"Architecture","text":"<pre><code>import torch\nimport torch.nn as nn\n\nclass TemporalMed2Vec(nn.Module):\n    \"\"\"\n    Med2Vec with temporal awareness via time-distance weighting.\n    \"\"\"\n    def __init__(self, vocab_size, embed_dim=128, time_decay=0.1):\n        super().__init__()\n        self.code_embeddings = nn.Embedding(vocab_size, embed_dim)\n        self.context_embeddings = nn.Embedding(vocab_size, embed_dim)\n        self.time_decay = time_decay\n\n    def forward(self, target_code, context_codes, time_deltas):\n        \"\"\"\n        Args:\n            target_code: [batch_size] - Target code indices\n            context_codes: [batch_size, context_size] - Context code indices\n            time_deltas: [batch_size, context_size] - Time gaps in days\n\n        Returns:\n            loss: Negative log-likelihood with temporal weighting\n        \"\"\"\n        # Get embeddings\n        target_embed = self.code_embeddings(target_code)  # [batch, embed_dim]\n        context_embed = self.context_embeddings(context_codes)  # [batch, context, embed_dim]\n\n        # Compute similarity scores\n        scores = torch.einsum('be,bce-&gt;bc', target_embed, context_embed)  # [batch, context]\n\n        # Apply temporal decay: weight = exp(-\u03bb * \u0394t)\n        temporal_weights = torch.exp(-self.time_decay * time_deltas)  # [batch, context]\n\n        # Weighted loss\n        log_probs = torch.log_softmax(scores, dim=-1)\n        weighted_loss = -(log_probs * temporal_weights).sum() / temporal_weights.sum()\n\n        return weighted_loss\n</code></pre>"},{"location":"methods/modern-code-embeddings/#training-procedure","title":"Training Procedure","text":"<pre><code>def create_temporal_training_pairs(patient_sequences, window_size=5):\n    \"\"\"\n    Create (target, context, time_delta) triplets with temporal information.\n\n    Args:\n        patient_sequences: List of [(timestamp, code), ...] per patient\n        window_size: Maximum context window\n\n    Returns:\n        triplets: [(target_code, context_codes, time_deltas), ...]\n    \"\"\"\n    triplets = []\n\n    for sequence in patient_sequences:\n        for i, (target_time, target_code) in enumerate(sequence):\n            context_codes = []\n            time_deltas = []\n\n            # Look backward and forward in time\n            for j in range(max(0, i - window_size), min(len(sequence), i + window_size + 1)):\n                if i == j:  # Skip the target code itself\n                    continue\n\n                context_time, context_code = sequence[j]\n                time_delta = abs((target_time - context_time).days)\n\n                context_codes.append(context_code)\n                time_deltas.append(time_delta)\n\n            if context_codes:\n                triplets.append((target_code, context_codes, time_deltas))\n\n    return triplets\n\n# Training loop\nmodel = TemporalMed2Vec(vocab_size=10000, embed_dim=128, time_decay=0.01)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nfor epoch in range(50):\n    for target, context, deltas in dataloader:\n        loss = model(target, context, deltas)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n</code></pre> <p>Pros: - Simple extension of familiar Word2Vec - Explicitly models temporal decay - Fast training</p> <p>Cons: - Still limited context window - No true long-range dependencies - Treats all code types equally</p>"},{"location":"methods/modern-code-embeddings/#approach-2-hierarchical-temporal-embeddings","title":"Approach 2: Hierarchical Temporal Embeddings","text":"<p>Key Idea: Different embedding strategies for different sequence representations.</p>"},{"location":"methods/modern-code-embeddings/#for-visit-grouped-sequences","title":"For Visit-Grouped Sequences","text":"<pre><code>class VisitEmbedding(nn.Module):\n    \"\"\"\n    Embed entire visits, then aggregate to patient level.\n    \"\"\"\n    def __init__(self, vocab_size, code_embed_dim=64, visit_embed_dim=128):\n        super().__init__()\n        self.code_embeddings = nn.Embedding(vocab_size, code_embed_dim)\n        self.visit_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=code_embed_dim, nhead=4),\n            num_layers=2\n        )\n        self.visit_projection = nn.Linear(code_embed_dim, visit_embed_dim)\n\n    def forward(self, visit_codes):\n        \"\"\"\n        Args:\n            visit_codes: [batch, max_codes_per_visit] - Codes in a visit\n\n        Returns:\n            visit_embedding: [batch, visit_embed_dim]\n        \"\"\"\n        # Embed individual codes\n        code_embeds = self.code_embeddings(visit_codes)  # [batch, codes, code_dim]\n\n        # Aggregate codes within visit using Transformer\n        visit_repr = self.visit_encoder(code_embeds.transpose(0, 1)).transpose(0, 1)\n\n        # Pool to single visit embedding (mean pooling)\n        visit_embed = visit_repr.mean(dim=1)  # [batch, code_dim]\n\n        # Project to visit embedding space\n        return self.visit_projection(visit_embed)  # [batch, visit_dim]\n</code></pre>"},{"location":"methods/modern-code-embeddings/#for-hierarchical-sequences-code-type-aware","title":"For Hierarchical Sequences (Code Type Aware)","text":"<pre><code>class HierarchicalCodeEmbedding(nn.Module):\n    \"\"\"\n    Separate embeddings for different code types, then combine.\n    \"\"\"\n    def __init__(self, vocab_sizes, embed_dim=128):\n        super().__init__()\n        # Separate embeddings for each code system\n        self.icd_embeddings = nn.Embedding(vocab_sizes['ICD'], embed_dim)\n        self.loinc_embeddings = nn.Embedding(vocab_sizes['LOINC'], embed_dim)\n        self.snomed_embeddings = nn.Embedding(vocab_sizes['SNOMED'], embed_dim)\n        self.rxnorm_embeddings = nn.Embedding(vocab_sizes['RXNORM'], embed_dim)\n\n        # Cross-code-type attention\n        self.cross_attention = nn.MultiheadAttention(embed_dim, num_heads=4)\n\n    def forward(self, codes_by_type):\n        \"\"\"\n        Args:\n            codes_by_type: Dict[str, Tensor] - Codes grouped by type\n                {'ICD': [batch, n_icd], 'LOINC': [batch, n_loinc], ...}\n\n        Returns:\n            unified_embeddings: [batch, total_codes, embed_dim]\n        \"\"\"\n        embeds = []\n\n        # Embed each code type separately\n        if 'ICD' in codes_by_type:\n            embeds.append(self.icd_embeddings(codes_by_type['ICD']))\n        if 'LOINC' in codes_by_type:\n            embeds.append(self.loinc_embeddings(codes_by_type['LOINC']))\n        if 'SNOMED' in codes_by_type:\n            embeds.append(self.snomed_embeddings(codes_by_type['SNOMED']))\n        if 'RXNORM' in codes_by_type:\n            embeds.append(self.rxnorm_embeddings(codes_by_type['RXNORM']))\n\n        # Concatenate all embeddings\n        all_embeds = torch.cat(embeds, dim=1)  # [batch, total_codes, embed_dim]\n\n        # Apply cross-attention to capture relationships across code types\n        attended, _ = self.cross_attention(\n            all_embeds.transpose(0, 1),\n            all_embeds.transpose(0, 1),\n            all_embeds.transpose(0, 1)\n        )\n\n        return attended.transpose(0, 1)\n</code></pre>"},{"location":"methods/modern-code-embeddings/#approach-3-transformer-based-contextualized-embeddings-recommended","title":"Approach 3: Transformer-based Contextualized Embeddings (RECOMMENDED)","text":"<p>Key Insight: Don't learn static embeddings - learn contextualized representations that depend on the entire patient history.</p>"},{"location":"methods/modern-code-embeddings/#architecture-medical-code-bert-medcodebert","title":"Architecture: Medical Code BERT (MedCodeBERT)","text":"<pre><code>class MedCodeBERT(nn.Module):\n    \"\"\"\n    BERT-style model for medical codes with temporal encoding.\n\n    Similar to BEHRT but focused on code embeddings.\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size,\n        embed_dim=128,\n        num_layers=4,\n        num_heads=4,\n        max_seq_len=512,\n        max_age=100,\n        dropout=0.1\n    ):\n        super().__init__()\n\n        # Code embeddings\n        self.code_embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n\n        # Temporal embeddings\n        self.age_embeddings = nn.Embedding(max_age * 12, embed_dim)  # Age in months\n        self.position_embeddings = nn.Embedding(max_seq_len, embed_dim)\n        self.time_delta_projection = nn.Linear(1, embed_dim)  # Continuous time gaps\n\n        # Code type embeddings (segment embeddings)\n        self.code_type_embeddings = nn.Embedding(5, embed_dim)  # ICD, LOINC, SNOMED, RXNORM, OTHER\n\n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=num_heads,\n            dim_feedforward=embed_dim * 4,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # Layer norm\n        self.layer_norm = nn.LayerNorm(embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, code_ids, ages, positions, time_deltas, code_types, attention_mask=None):\n        \"\"\"\n        Args:\n            code_ids: [batch, seq_len] - Code vocabulary indices\n            ages: [batch, seq_len] - Patient age in months at each event\n            positions: [batch, seq_len] - Position in sequence (0, 1, 2, ...)\n            time_deltas: [batch, seq_len] - Days since previous event\n            code_types: [batch, seq_len] - Code system type (0=ICD, 1=LOINC, etc.)\n            attention_mask: [batch, seq_len] - Mask for padding\n\n        Returns:\n            contextualized_embeddings: [batch, seq_len, embed_dim]\n        \"\"\"\n        # Get base embeddings\n        code_embeds = self.code_embeddings(code_ids)\n        age_embeds = self.age_embeddings(ages)\n        pos_embeds = self.position_embeddings(positions)\n        time_embeds = self.time_delta_projection(time_deltas.unsqueeze(-1))\n        type_embeds = self.code_type_embeddings(code_types)\n\n        # Combine all embeddings\n        embeddings = code_embeds + age_embeds + pos_embeds + time_embeds + type_embeds\n        embeddings = self.layer_norm(embeddings)\n        embeddings = self.dropout(embeddings)\n\n        # Apply transformer\n        if attention_mask is not None:\n            # Convert mask to transformer format (True = ignore)\n            attention_mask = ~attention_mask.bool()\n\n        output = self.transformer(embeddings, src_key_padding_mask=attention_mask)\n\n        return output\n</code></pre>"},{"location":"methods/modern-code-embeddings/#pre-training-objectives","title":"Pre-training Objectives","text":"<p>1. Masked Language Modeling (MLM)</p> <pre><code>def masked_language_modeling_loss(model, batch):\n    \"\"\"\n    Randomly mask 15% of codes and predict them.\n    \"\"\"\n    code_ids = batch['code_ids'].clone()\n    labels = code_ids.clone()\n\n    # Create random mask (15% of tokens)\n    mask_prob = torch.rand(code_ids.shape)\n    mask = (mask_prob &lt; 0.15) &amp; (code_ids != 0)  # Don't mask padding\n\n    # Replace masked tokens with [MASK] token (vocab_size - 1)\n    code_ids[mask] = model.code_embeddings.num_embeddings - 1\n\n    # Get predictions\n    embeddings = model(\n        code_ids,\n        batch['ages'],\n        batch['positions'],\n        batch['time_deltas'],\n        batch['code_types'],\n        batch['attention_mask']\n    )\n\n    # Predict original codes\n    logits = torch.matmul(embeddings, model.code_embeddings.weight.T)\n\n    # Compute loss only on masked tokens\n    loss = F.cross_entropy(\n        logits[mask],\n        labels[mask],\n        ignore_index=0\n    )\n\n    return loss\n</code></pre> <p>2. Next Visit Prediction</p> <pre><code>def next_visit_prediction_loss(model, batch):\n    \"\"\"\n    Predict codes in next visit given current history.\n    \"\"\"\n    # Split sequence into history and target\n    history_len = batch['visit_boundaries'][-2]  # Second to last visit\n\n    history_embeds = model(\n        batch['code_ids'][:, :history_len],\n        batch['ages'][:, :history_len],\n        batch['positions'][:, :history_len],\n        batch['time_deltas'][:, :history_len],\n        batch['code_types'][:, :history_len],\n        batch['attention_mask'][:, :history_len]\n    )\n\n    # Pool history to single vector\n    history_repr = history_embeds.mean(dim=1)  # [batch, embed_dim]\n\n    # Predict next visit codes (multi-label classification)\n    next_visit_codes = batch['code_ids'][:, history_len:]\n    logits = torch.matmul(history_repr, model.code_embeddings.weight.T)\n\n    # Binary cross-entropy for multi-label\n    targets = torch.zeros_like(logits)\n    for i, codes in enumerate(next_visit_codes):\n        targets[i, codes[codes != 0]] = 1\n\n    loss = F.binary_cross_entropy_with_logits(logits, targets)\n\n    return loss\n</code></pre> <p>3. Contrastive Learning for Similar Patients</p> <pre><code>def contrastive_patient_similarity_loss(model, batch1, batch2, labels):\n    \"\"\"\n    Learn embeddings where similar patients are close, dissimilar are far.\n\n    Args:\n        batch1, batch2: Two patient sequences\n        labels: 1 if similar (same disease), 0 if dissimilar\n    \"\"\"\n    # Get patient-level embeddings\n    embeds1 = model(batch1['code_ids'], ...).mean(dim=1)  # [batch, embed_dim]\n    embeds2 = model(batch2['code_ids'], ...).mean(dim=1)\n\n    # Cosine similarity\n    similarity = F.cosine_similarity(embeds1, embeds2)\n\n    # Contrastive loss\n    loss = labels * (1 - similarity) + (1 - labels) * torch.clamp(similarity - 0.2, min=0)\n\n    return loss.mean()\n</code></pre>"},{"location":"methods/modern-code-embeddings/#training-strategy","title":"Training Strategy","text":"<pre><code># Pre-training phase (unsupervised)\nmodel = MedCodeBERT(vocab_size=10000, embed_dim=256, num_layers=6)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\nfor epoch in range(100):\n    for batch in pretrain_dataloader:\n        # Multi-task pre-training\n        mlm_loss = masked_language_modeling_loss(model, batch)\n        nvp_loss = next_visit_prediction_loss(model, batch)\n\n        loss = mlm_loss + 0.5 * nvp_loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n# Save pre-trained embeddings\ntorch.save(model.code_embeddings.state_dict(), 'pretrained_code_embeddings.pt')\n</code></pre>"},{"location":"methods/modern-code-embeddings/#approach-4-graph-enhanced-embeddings","title":"Approach 4: Graph-Enhanced Embeddings","text":"<p>Key Idea: Leverage medical ontology structure (ICD hierarchy, LOINC parts, SNOMED relationships).</p> <pre><code>import torch_geometric as pyg\n\nclass GraphEnhancedCodeEmbedding(nn.Module):\n    \"\"\"\n    Combine sequence-based embeddings with graph structure.\n    \"\"\"\n    def __init__(self, vocab_size, embed_dim=128, num_gnn_layers=3):\n        super().__init__()\n\n        # Base embeddings (from MedCodeBERT)\n        self.base_embeddings = nn.Embedding(vocab_size, embed_dim)\n\n        # Graph neural network for ontology structure\n        self.gnn_layers = nn.ModuleList([\n            pyg.nn.GATConv(embed_dim, embed_dim, heads=4, concat=False)\n            for _ in range(num_gnn_layers)\n        ])\n\n    def forward(self, code_ids, ontology_graph):\n        \"\"\"\n        Args:\n            code_ids: [batch, seq_len] - Code indices\n            ontology_graph: PyG Data object with edges representing relationships\n\n        Returns:\n            enhanced_embeddings: [batch, seq_len, embed_dim]\n        \"\"\"\n        # Get base embeddings\n        base_embeds = self.base_embeddings(code_ids)\n\n        # Enhance with graph structure\n        node_features = self.base_embeddings.weight  # All code embeddings\n\n        for gnn_layer in self.gnn_layers:\n            node_features = gnn_layer(node_features, ontology_graph.edge_index)\n            node_features = F.relu(node_features)\n\n        # Look up enhanced embeddings for input codes\n        enhanced_embeds = node_features[code_ids]\n\n        return enhanced_embeds\n</code></pre>"},{"location":"methods/modern-code-embeddings/#part-3-recommendations-for-your-use-case","title":"Part 3: Recommendations for Your Use Case","text":""},{"location":"methods/modern-code-embeddings/#for-icd-code-e119-type-2-diabetes","title":"For ICD Code E11.9 (Type 2 Diabetes)","text":"<p>Recommended Approach: MedCodeBERT (Approach 3)</p> <p>Rationale: 1. Temporal awareness: Diabetes is chronic - need long-range dependencies 2. Context-dependent: E11.9 meaning changes based on patient history 3. Multi-code relationships: Often co-occurs with LOINC (HbA1c), RxNorm (Metformin) 4. Scalability: Transformer handles variable-length sequences well</p>"},{"location":"methods/modern-code-embeddings/#for-loinc-code-4548-4-hemoglobin-a1c","title":"For LOINC Code 4548-4 (Hemoglobin A1c)","text":"<p>Recommended Approach: Hierarchical + MedCodeBERT</p> <p>Rationale: 1. LOINC structure: 6-part hierarchy (Component, Property, Time, System, Scale, Method) 2. Temporal patterns: HbA1c measured quarterly for diabetes monitoring 3. Value-aware: Embedding should reflect numeric values (high vs normal)</p> <pre><code>class LOINCEmbedding(nn.Module):\n    \"\"\"\n    LOINC-specific embedding with structure awareness.\n    \"\"\"\n    def __init__(self, loinc_vocab_size, embed_dim=128):\n        super().__init__()\n\n        # Base LOINC code embedding\n        self.code_embedding = nn.Embedding(loinc_vocab_size, embed_dim)\n\n        # LOINC part embeddings\n        self.component_embedding = nn.Embedding(1000, embed_dim // 6)\n        self.property_embedding = nn.Embedding(100, embed_dim // 6)\n        self.time_embedding = nn.Embedding(50, embed_dim // 6)\n        self.system_embedding = nn.Embedding(200, embed_dim // 6)\n        self.scale_embedding = nn.Embedding(20, embed_dim // 6)\n        self.method_embedding = nn.Embedding(500, embed_dim // 6)\n\n        # Value encoder (for numeric lab values)\n        self.value_encoder = nn.Linear(1, embed_dim // 4)\n\n        # Combine all\n        self.projection = nn.Linear(embed_dim + embed_dim // 4, embed_dim)\n\n    def forward(self, loinc_code, loinc_parts, value):\n        \"\"\"\n        Args:\n            loinc_code: LOINC code ID\n            loinc_parts: Dict with component, property, time, system, scale, method IDs\n            value: Numeric lab value (normalized)\n        \"\"\"\n        # Base embedding\n        code_embed = self.code_embedding(loinc_code)\n\n        # Part embeddings\n        part_embeds = torch.cat([\n            self.component_embedding(loinc_parts['component']),\n            self.property_embedding(loinc_parts['property']),\n            self.time_embedding(loinc_parts['time']),\n            self.system_embedding(loinc_parts['system']),\n            self.scale_embedding(loinc_parts['scale']),\n            self.method_embedding(loinc_parts['method'])\n        ], dim=-1)\n\n        # Value embedding\n        value_embed = self.value_encoder(value.unsqueeze(-1))\n\n        # Combine\n        combined = torch.cat([code_embed + part_embeds, value_embed], dim=-1)\n        return self.projection(combined)\n</code></pre>"},{"location":"methods/modern-code-embeddings/#part-4-implementation-roadmap","title":"Part 4: Implementation Roadmap","text":""},{"location":"methods/modern-code-embeddings/#phase-1-data-preparation-week-1","title":"Phase 1: Data Preparation (Week 1)","text":"<p>Goal: Prepare sequences in multiple representations</p> <pre><code># scripts/prepare_sequences.py\nfrom ehrsequencing.data.adapters import SyntheaAdapter\nfrom ehrsequencing.data.sequences import SequenceBuilder\n\n# Load data\nadapter = SyntheaAdapter('data/synthea/')\nevents = adapter.load_events()\n\n# Create different representations\nbuilder = SequenceBuilder()\n\n# Flat sequence\nflat_sequences = builder.build(events, strategy='flat')\n# Output: [code1, code2, code3, ...]\n\n# Visit-grouped\nvisit_sequences = builder.build(events, strategy='visit-grouped')\n# Output: [[visit1_codes], [visit2_codes], ...]\n\n# Hierarchical\nhierarchical_sequences = builder.build(events, strategy='hierarchical')\n# Output: {\n#   'ICD': [code1, code2, ...],\n#   'LOINC': [code3, code4, ...],\n#   'RXNORM': [code5, code6, ...]\n# }\n\n# Save\ntorch.save({\n    'flat': flat_sequences,\n    'visit': visit_sequences,\n    'hierarchical': hierarchical_sequences\n}, 'data/processed/sequences.pt')\n</code></pre>"},{"location":"methods/modern-code-embeddings/#phase-2-baseline-embeddings-week-2","title":"Phase 2: Baseline Embeddings (Week 2)","text":"<p>Goal: Implement Temporal Skip-gram as baseline</p> <pre><code># src/ehrsequencing/embeddings/temporal_skipgram.py\n# (Implementation from Approach 1 above)\n\n# examples/train_temporal_skipgram.py\nfrom ehrsequencing.embeddings.temporal_skipgram import TemporalMed2Vec\n\nmodel = TemporalMed2Vec(vocab_size=10000, embed_dim=128)\n# Train on flat sequences\n# Evaluate: nearest neighbors, clustering\n</code></pre>"},{"location":"methods/modern-code-embeddings/#phase-3-medcodebert-implementation-week-3-4","title":"Phase 3: MedCodeBERT Implementation (Week 3-4)","text":"<p>Goal: Implement transformer-based contextualized embeddings</p> <pre><code># src/ehrsequencing/embeddings/medcodebert.py\n# (Implementation from Approach 3 above)\n\n# examples/pretrain_medcodebert.py\nfrom ehrsequencing.embeddings.medcodebert import MedCodeBERT\n\nmodel = MedCodeBERT(vocab_size=10000, embed_dim=256, num_layers=6)\n# Pre-train with MLM + Next Visit Prediction\n# Save pre-trained model\n</code></pre>"},{"location":"methods/modern-code-embeddings/#phase-4-evaluation-week-5","title":"Phase 4: Evaluation (Week 5)","text":"<p>Goal: Compare embedding quality</p> <pre><code># src/ehrsequencing/evaluation/embedding_eval.py\n\ndef evaluate_embeddings(embeddings, test_data):\n    \"\"\"\n    Evaluate embedding quality.\n    \"\"\"\n    metrics = {}\n\n    # 1. Nearest neighbor accuracy\n    # For diabetes codes, are nearest neighbors also diabetes-related?\n    metrics['nn_accuracy'] = nearest_neighbor_accuracy(embeddings, test_data)\n\n    # 2. Clustering quality\n    # Do similar diseases cluster together?\n    metrics['silhouette'] = clustering_quality(embeddings, test_data)\n\n    # 3. Downstream task performance\n    # Use embeddings for diagnosis prediction\n    metrics['diagnosis_auc'] = diagnosis_prediction_auc(embeddings, test_data)\n\n    return metrics\n</code></pre>"},{"location":"methods/modern-code-embeddings/#phase-5-fine-tuning-for-applications-week-6","title":"Phase 5: Fine-tuning for Applications (Week 6+)","text":"<p>Goal: Use embeddings for downstream tasks</p> <pre><code># Disease progression modeling\nfrom ehrsequencing.models import DiseaseProgressionModel\n\nmodel = DiseaseProgressionModel(\n    code_embeddings=pretrained_medcodebert.code_embeddings,\n    hidden_dim=256\n)\nmodel.train(sequences, targets)\n\n# Patient segmentation\nfrom ehrsequencing.clustering import PatientSegmentation\n\nsegmentation = PatientSegmentation(embedding_model=pretrained_medcodebert)\nclusters = segmentation.fit_predict(patient_sequences)\n</code></pre>"},{"location":"methods/modern-code-embeddings/#part-5-comparison-table","title":"Part 5: Comparison Table","text":"Approach Temporal Aware Long-Range Deps Hierarchical Training Time Best For Temporal Skip-gram \u2705 (decay) \u274c (window) \u274c Fast (hours) Baseline, quick experiments Hierarchical Embeddings \u26a0\ufe0f (limited) \u274c \u2705 Medium (days) Visit-grouped sequences MedCodeBERT \u2705\u2705 (full) \u2705\u2705 (transformer) \u26a0\ufe0f (via types) Slow (weeks) Production, best quality Graph-Enhanced \u26a0\ufe0f \u2705 (via graph) \u2705\u2705 Slow (weeks) When ontology is critical"},{"location":"methods/modern-code-embeddings/#part-6-practical-recommendations","title":"Part 6: Practical Recommendations","text":""},{"location":"methods/modern-code-embeddings/#start-simple-iterate","title":"Start Simple, Iterate","text":"<p>Week 1-2: Temporal Skip-gram - Quick to implement and train - Establishes baseline - Validates data pipeline</p> <p>Week 3-4: MedCodeBERT - State-of-the-art quality - Handles all sequence types - Pre-train once, use for all tasks</p> <p>Week 5+: Task-Specific Fine-tuning - Disease progression - Patient segmentation - Phenotype discovery</p>"},{"location":"methods/modern-code-embeddings/#handling-different-sequence-representations","title":"Handling Different Sequence Representations","text":"<p>For Flat Sequences: <pre><code># Use MedCodeBERT directly\nembeddings = medcodebert(flat_sequence)\n</code></pre></p> <p>For Visit-Grouped: <pre><code># Embed each visit, then sequence of visits\nvisit_embeds = [medcodebert(visit).mean(dim=0) for visit in visits]\npatient_sequence = torch.stack(visit_embeds)\n</code></pre></p> <p>For Hierarchical: <pre><code># Embed each code type separately, then combine\nicd_embeds = medcodebert(icd_codes)\nloinc_embeds = loinc_embedding(loinc_codes, loinc_parts, values)\ncombined = cross_attention(icd_embeds, loinc_embeds)\n</code></pre></p>"},{"location":"methods/modern-code-embeddings/#part-7-code-examples","title":"Part 7: Code Examples","text":""},{"location":"methods/modern-code-embeddings/#complete-training-script","title":"Complete Training Script","text":"<pre><code># examples/train_code_embeddings.py\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom ehrsequencing.data import load_sequences\nfrom ehrsequencing.embeddings import MedCodeBERT\nfrom ehrsequencing.evaluation import evaluate_embeddings\n\ndef main():\n    # Load data\n    sequences = load_sequences('data/processed/sequences.pt')\n    train_loader = DataLoader(sequences['train'], batch_size=32, shuffle=True)\n    val_loader = DataLoader(sequences['val'], batch_size=32)\n\n    # Initialize model\n    model = MedCodeBERT(\n        vocab_size=len(sequences['vocab']),\n        embed_dim=256,\n        num_layers=6,\n        num_heads=8\n    )\n\n    # Pre-training\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n    for epoch in range(100):\n        # Train\n        model.train()\n        for batch in train_loader:\n            loss = masked_language_modeling_loss(model, batch)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # Evaluate\n        if epoch % 10 == 0:\n            model.eval()\n            metrics = evaluate_embeddings(model, val_loader)\n            print(f\"Epoch {epoch}: {metrics}\")\n\n    # Save\n    torch.save(model.state_dict(), 'checkpoints/medcodebert_pretrained.pt')\n\n    # Extract static embeddings for downstream use\n    code_embeddings = model.code_embeddings.weight.detach()\n    torch.save(code_embeddings, 'checkpoints/code_embeddings.pt')\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"methods/modern-code-embeddings/#references","title":"References","text":"<ol> <li>Med2Vec - Choi et al., \"Multi-layer Representation Learning for Medical Concepts\" (2016)</li> <li>BEHRT - Li et al., \"BEHRT: Transformer for Electronic Health Records\" (2020)</li> <li>Med-BERT - Rasmy et al., \"Med-BERT: Pre-trained Contextualized Embeddings\" (2021)</li> <li>CEHR-BERT - Pang et al., \"CEHR-BERT: Incorporating Temporal Information\" (2021)</li> <li>GraphCare - Choi et al., \"Learning the Graphical Structure of EHR\" (2020)</li> </ol> <p>Document Version: 1.0 Last Updated: January 19, 2026 Next Review: After Phase 3 implementation</p>"},{"location":"methods/pretrained-models-and-disease-progression/","title":"Pre-trained Models &amp; Disease Progression Modeling","text":"<p>Date: January 19, 2026 Focus: Using pre-trained foundation models + Visit-grouped sequences for disease staging</p>"},{"location":"methods/pretrained-models-and-disease-progression/#part-1-pre-trained-foundation-models-for-ehr","title":"Part 1: Pre-trained Foundation Models for EHR","text":""},{"location":"methods/pretrained-models-and-disease-progression/#available-pre-trained-models-2024-2026","title":"Available Pre-trained Models (2024-2026)","text":"<p>You're absolutely right - don't train from scratch. Use these pre-trained models:</p>"},{"location":"methods/pretrained-models-and-disease-progression/#1-behrt-bert-for-ehr","title":"1. BEHRT (BERT for EHR)","text":"<p>Paper: Li et al., \"BEHRT: Transformer for Electronic Health Records\" (2020) Pre-trained on: MIMIC-III (40K+ patients) Available: GitHub</p> <pre><code># Load pre-trained BEHRT\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(\"bvanaken/clinical-bert\")\ntokenizer = AutoTokenizer.from_pretrained(\"bvanaken/clinical-bert\")\n\n# Fine-tune for your task\n# (Much cheaper than training from scratch)\n</code></pre> <p>Architecture: - Vocabulary: ~30K medical codes (ICD, procedures) - Embedding: 768-dim - Layers: 12 transformer layers - Pre-training: MLM + Next Visit Prediction</p> <p>Pros: - \u2705 Pre-trained on real EHR data - \u2705 Handles temporal sequences - \u2705 Age + visit + position embeddings built-in</p> <p>Cons: - \u26a0\ufe0f Limited to codes in MIMIC-III - \u26a0\ufe0f No LOINC codes (mostly ICD + procedures)</p>"},{"location":"methods/pretrained-models-and-disease-progression/#2-med-bert","title":"2. Med-BERT","text":"<p>Paper: Rasmy et al., \"Med-BERT: Pre-trained Contextualized Embeddings\" (2021) Pre-trained on: 28M patients from Cerner Health Facts Available: GitHub</p> <pre><code>from medbert import MedBERT\n\n# Load pre-trained\nmodel = MedBERT.from_pretrained('medbert-base')\n\n# Fine-tune on your CKD cohort\nmodel.fine_tune(ckd_sequences, task='disease_progression')\n</code></pre> <p>Architecture: - Vocabulary: 50K+ codes (ICD-9/10, NDC, CPT) - Embedding: 256-dim - Layers: 6 transformer layers - Pre-training: Prolonged Length of Stay prediction</p> <p>Pros: - \u2705 Largest pre-training dataset - \u2705 Includes medication codes (NDC) - \u2705 Proven on disease progression tasks</p> <p>Cons: - \u26a0\ufe0f Still limited LOINC coverage</p>"},{"location":"methods/pretrained-models-and-disease-progression/#3-cehr-bert","title":"3. CEHR-BERT","text":"<p>Paper: Pang et al., \"CEHR-BERT: Incorporating Temporal Information\" (2021) Pre-trained on: Columbia University Medical Center (4M+ patients) Available: GitHub</p> <pre><code>from cehrbert import CEHRBERT\n\n# Load pre-trained\nmodel = CEHRBERT.from_pretrained('cehrbert-base')\n\n# Supports continuous time encoding\nembeddings = model.encode(\n    codes=patient_codes,\n    timestamps=patient_timestamps  # \u2190 Key feature\n)\n</code></pre> <p>Architecture: - Vocabulary: 40K+ codes - Embedding: 128-dim - Layers: 4 transformer layers - Key feature: Continuous time encoding (not just position)</p> <p>Pros: - \u2705 Best temporal modeling - \u2705 Handles irregular time intervals - \u2705 Designed for longitudinal prediction</p> <p>Cons: - \u26a0\ufe0f Smaller model (fewer layers)</p>"},{"location":"methods/pretrained-models-and-disease-progression/#4-clinicalbert-text-codes","title":"4. ClinicalBERT (Text + Codes)","text":"<p>Paper: Alsentzer et al., \"Publicly Available Clinical BERT Embeddings\" (2019) Pre-trained on: MIMIC-III clinical notes + codes Available: HuggingFace <code>emilyalsentzer/Bio_ClinicalBERT</code></p> <pre><code>from transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\ntokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n\n# Use for clinical notes + codes\ntext_embedding = model(**tokenizer(\"Patient has CKD stage 3\", return_tensors=\"pt\"))\n</code></pre> <p>Pros: - \u2705 Handles both text and codes - \u2705 Easy to use (HuggingFace) - \u2705 Well-documented</p> <p>Cons: - \u26a0\ufe0f Primarily for text, not optimized for code sequences</p>"},{"location":"methods/pretrained-models-and-disease-progression/#recommendation-for-your-use-case","title":"Recommendation for Your Use Case","text":"<p>For CKD Disease Staging:</p> <p>Primary: CEHR-BERT (best temporal modeling) Backup: Med-BERT (largest pre-training, proven on progression)</p> <p>Hybrid Approach (Recommended): <pre><code># Use pre-trained for code embeddings\npretrained_model = CEHRBERT.from_pretrained('cehrbert-base')\n\n# Extract code embeddings\ncode_embeddings = pretrained_model.get_code_embeddings()\n\n# Use these as initialization for your visit-grouped model\nvisit_model = VisitGroupedProgressionModel(\n    code_embeddings=code_embeddings,  # \u2190 Pre-trained\n    visit_encoder='lstm',\n    progression_head='survival'\n)\n</code></pre></p>"},{"location":"methods/pretrained-models-and-disease-progression/#part-2-visit-grouped-sequences-for-disease-progression","title":"Part 2: Visit-Grouped Sequences for Disease Progression","text":""},{"location":"methods/pretrained-models-and-disease-progression/#why-visit-grouped-is-ideal-for-disease-staging","title":"Why Visit-Grouped is Ideal for Disease Staging","text":"<p>You're absolutely right! Visit-grouped sequences are perfect for disease progression because:</p> <ol> <li>Clinical Reality: Disease staging happens at visits (e.g., CKD diagnosed at clinic visit)</li> <li>Natural Granularity: Each visit = snapshot of patient state</li> <li>Temporal Structure: Visit intervals encode disease velocity</li> <li>Interpretability: Can explain \"at Visit 5, patient progressed due to...\"</li> </ol>"},{"location":"methods/pretrained-models-and-disease-progression/#architecture-hierarchical-visit-grouped-model","title":"Architecture: Hierarchical Visit-Grouped Model","text":"<pre><code>import torch\nimport torch.nn as nn\n\nclass VisitGroupedProgressionModel(nn.Module):\n    \"\"\"\n    Two-level hierarchy:\n    1. Code-level: Embed codes within each visit\n    2. Visit-level: Model sequence of visits for progression\n    \"\"\"\n    def __init__(\n        self,\n        pretrained_code_embeddings,  # From CEHR-BERT\n        code_embed_dim=128,\n        visit_embed_dim=256,\n        hidden_dim=512,\n        num_stages=5,  # CKD stages 1-5\n        dropout=0.1\n    ):\n        super().__init__()\n\n        # Level 1: Code embeddings (pre-trained)\n        self.code_embeddings = nn.Embedding.from_pretrained(\n            pretrained_code_embeddings,\n            freeze=False  # Allow fine-tuning\n        )\n\n        # Level 1: Within-visit aggregation\n        self.visit_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=code_embed_dim,\n                nhead=4,\n                dim_feedforward=code_embed_dim * 4,\n                dropout=dropout,\n                batch_first=True\n            ),\n            num_layers=2\n        )\n\n        # Project to visit embedding\n        self.visit_projection = nn.Linear(code_embed_dim, visit_embed_dim)\n\n        # Level 2: Visit sequence modeling\n        self.visit_lstm = nn.LSTM(\n            input_size=visit_embed_dim + 2,  # +2 for time features\n            hidden_size=hidden_dim,\n            num_layers=2,\n            dropout=dropout,\n            batch_first=True,\n            bidirectional=False  # Causal for prediction\n        )\n\n        # Progression prediction head\n        self.stage_classifier = nn.Linear(hidden_dim, num_stages)\n        self.time_to_progression = nn.Linear(hidden_dim, 1)  # Days until next stage\n\n        self.dropout = nn.Dropout(dropout)\n\n    def encode_visit(self, visit_codes, visit_mask):\n        \"\"\"\n        Encode a single visit into a fixed-size embedding.\n\n        Args:\n            visit_codes: [batch, max_codes_per_visit]\n            visit_mask: [batch, max_codes_per_visit] - 1 for real codes, 0 for padding\n\n        Returns:\n            visit_embedding: [batch, visit_embed_dim]\n        \"\"\"\n        # Embed codes\n        code_embeds = self.code_embeddings(visit_codes)  # [batch, codes, code_dim]\n\n        # Aggregate codes within visit using Transformer\n        # Mask padding tokens\n        attn_mask = ~visit_mask.bool()  # True = ignore\n        visit_repr = self.visit_encoder(\n            code_embeds,\n            src_key_padding_mask=attn_mask\n        )  # [batch, codes, code_dim]\n\n        # Pool to single visit embedding (mean over non-padding)\n        visit_mask_expanded = visit_mask.unsqueeze(-1)  # [batch, codes, 1]\n        masked_repr = visit_repr * visit_mask_expanded\n        visit_embed = masked_repr.sum(dim=1) / visit_mask.sum(dim=1, keepdim=True)\n\n        # Project to visit space\n        return self.visit_projection(visit_embed)  # [batch, visit_dim]\n\n    def forward(self, patient_visits, time_features, visit_mask):\n        \"\"\"\n        Predict disease progression from visit sequence.\n\n        Args:\n            patient_visits: [batch, num_visits, max_codes_per_visit]\n            time_features: [batch, num_visits, 2] - (days_since_first, days_since_prev)\n            visit_mask: [batch, num_visits, max_codes_per_visit]\n\n        Returns:\n            stage_logits: [batch, num_visits, num_stages]\n            time_to_progression: [batch, num_visits, 1]\n        \"\"\"\n        batch_size, num_visits, max_codes = patient_visits.shape\n\n        # Encode each visit\n        visit_embeds = []\n        for i in range(num_visits):\n            visit_embed = self.encode_visit(\n                patient_visits[:, i, :],\n                visit_mask[:, i, :]\n            )  # [batch, visit_dim]\n            visit_embeds.append(visit_embed)\n\n        visit_embeds = torch.stack(visit_embeds, dim=1)  # [batch, visits, visit_dim]\n\n        # Concatenate time features\n        visit_embeds_with_time = torch.cat([\n            visit_embeds,\n            time_features\n        ], dim=-1)  # [batch, visits, visit_dim + 2]\n\n        # Model visit sequence\n        lstm_out, _ = self.visit_lstm(visit_embeds_with_time)  # [batch, visits, hidden]\n        lstm_out = self.dropout(lstm_out)\n\n        # Predict stage at each visit\n        stage_logits = self.stage_classifier(lstm_out)  # [batch, visits, num_stages]\n\n        # Predict time to next stage\n        time_pred = self.time_to_progression(lstm_out)  # [batch, visits, 1]\n        time_pred = torch.relu(time_pred)  # Ensure positive\n\n        return stage_logits, time_pred\n</code></pre>"},{"location":"methods/pretrained-models-and-disease-progression/#part-3-ckd-disease-staging-complete-example","title":"Part 3: CKD Disease Staging - Complete Example","text":""},{"location":"methods/pretrained-models-and-disease-progression/#data-preparation","title":"Data Preparation","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\nclass CKDSequenceBuilder:\n    \"\"\"\n    Build visit-grouped sequences for CKD patients.\n    \"\"\"\n    def __init__(self, vocab):\n        self.vocab = vocab\n        self.ckd_codes = {\n            'CKD1': ['N18.1'],  # Stage 1\n            'CKD2': ['N18.2'],  # Stage 2\n            'CKD3': ['N18.3', 'N18.30', 'N18.31', 'N18.32'],  # Stage 3\n            'CKD4': ['N18.4'],  # Stage 4\n            'CKD5': ['N18.5', 'N18.6'],  # Stage 5 / ESRD\n        }\n\n    def extract_ckd_stage(self, codes):\n        \"\"\"Extract CKD stage from ICD codes in a visit.\"\"\"\n        for stage, stage_codes in self.ckd_codes.items():\n            if any(code in codes for code in stage_codes):\n                return int(stage[-1])  # Return stage number\n        return None\n\n    def build_patient_sequence(self, patient_df):\n        \"\"\"\n        Build visit-grouped sequence for one patient.\n\n        Args:\n            patient_df: DataFrame with columns [timestamp, code, code_type, value]\n\n        Returns:\n            sequence: {\n                'visits': List of visit dicts,\n                'stages': List of CKD stages at each visit,\n                'progression_events': List of (visit_idx, old_stage, new_stage)\n            }\n        \"\"\"\n        # Group by visit (same day = same visit)\n        patient_df['visit_date'] = pd.to_datetime(patient_df['timestamp']).dt.date\n        visits = []\n        stages = []\n\n        for visit_date, visit_df in patient_df.groupby('visit_date'):\n            # Extract codes\n            codes = visit_df['code'].tolist()\n            code_ids = [self.vocab.get(code, self.vocab['[UNK]']) for code in codes]\n\n            # Extract CKD stage\n            stage = self.extract_ckd_stage(codes)\n\n            # Time features\n            if len(visits) == 0:\n                days_since_first = 0\n                days_since_prev = 0\n            else:\n                first_date = visits[0]['date']\n                prev_date = visits[-1]['date']\n                days_since_first = (visit_date - first_date).days\n                days_since_prev = (visit_date - prev_date).days\n\n            visits.append({\n                'date': visit_date,\n                'codes': code_ids,\n                'days_since_first': days_since_first,\n                'days_since_prev': days_since_prev\n            })\n            stages.append(stage)\n\n        # Identify progression events\n        progression_events = []\n        for i in range(1, len(stages)):\n            if stages[i] is not None and stages[i-1] is not None:\n                if stages[i] &gt; stages[i-1]:\n                    progression_events.append((i, stages[i-1], stages[i]))\n\n        return {\n            'visits': visits,\n            'stages': stages,\n            'progression_events': progression_events\n        }\n\n    def build_dataset(self, all_patients_df):\n        \"\"\"Build dataset for all patients.\"\"\"\n        sequences = []\n        for patient_id, patient_df in all_patients_df.groupby('patient_id'):\n            seq = self.build_patient_sequence(patient_df)\n            if len(seq['visits']) &gt;= 3:  # Minimum 3 visits\n                sequences.append({\n                    'patient_id': patient_id,\n                    **seq\n                })\n        return sequences\n</code></pre>"},{"location":"methods/pretrained-models-and-disease-progression/#training","title":"Training","text":"<pre><code>import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CKDProgressionDataset(Dataset):\n    \"\"\"PyTorch dataset for CKD progression.\"\"\"\n    def __init__(self, sequences, max_visits=20, max_codes_per_visit=50):\n        self.sequences = sequences\n        self.max_visits = max_visits\n        self.max_codes = max_codes_per_visit\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        seq = self.sequences[idx]\n\n        # Pad/truncate visits\n        num_visits = min(len(seq['visits']), self.max_visits)\n\n        # Initialize tensors\n        visit_codes = torch.zeros(self.max_visits, self.max_codes, dtype=torch.long)\n        visit_mask = torch.zeros(self.max_visits, self.max_codes, dtype=torch.float)\n        time_features = torch.zeros(self.max_visits, 2, dtype=torch.float)\n        stage_labels = torch.full((self.max_visits,), -1, dtype=torch.long)  # -1 = no label\n\n        for i in range(num_visits):\n            visit = seq['visits'][i]\n            codes = visit['codes'][:self.max_codes]\n\n            visit_codes[i, :len(codes)] = torch.tensor(codes)\n            visit_mask[i, :len(codes)] = 1.0\n            time_features[i, 0] = visit['days_since_first'] / 365.0  # Normalize to years\n            time_features[i, 1] = visit['days_since_prev'] / 30.0  # Normalize to months\n\n            if seq['stages'][i] is not None:\n                stage_labels[i] = seq['stages'][i] - 1  # 0-indexed\n\n        return {\n            'visit_codes': visit_codes,\n            'visit_mask': visit_mask,\n            'time_features': time_features,\n            'stage_labels': stage_labels,\n            'num_visits': num_visits\n        }\n\n# Training loop\ndef train_ckd_model(model, train_loader, val_loader, num_epochs=50):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n    stage_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n    time_criterion = nn.MSELoss()\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n\n        for batch in train_loader:\n            # Forward pass\n            stage_logits, time_pred = model(\n                batch['visit_codes'],\n                batch['time_features'],\n                batch['visit_mask']\n            )\n\n            # Stage classification loss\n            stage_loss = stage_criterion(\n                stage_logits.view(-1, stage_logits.size(-1)),\n                batch['stage_labels'].view(-1)\n            )\n\n            # Time to progression loss (only for progression events)\n            # TODO: Compute actual time to next stage from data\n\n            loss = stage_loss\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        # Validation\n        if epoch % 5 == 0:\n            val_metrics = evaluate_ckd_model(model, val_loader)\n            print(f\"Epoch {epoch}: Train Loss={total_loss/len(train_loader):.4f}, \"\n                  f\"Val AUC={val_metrics['auc']:.4f}\")\n</code></pre>"},{"location":"methods/pretrained-models-and-disease-progression/#evaluation","title":"Evaluation","text":"<pre><code>from sklearn.metrics import roc_auc_score, accuracy_score\n\ndef evaluate_ckd_model(model, val_loader):\n    \"\"\"Evaluate CKD progression model.\"\"\"\n    model.eval()\n\n    all_preds = []\n    all_labels = []\n    all_probs = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            stage_logits, time_pred = model(\n                batch['visit_codes'],\n                batch['time_features'],\n                batch['visit_mask']\n            )\n\n            # Get predictions\n            probs = torch.softmax(stage_logits, dim=-1)\n            preds = torch.argmax(probs, dim=-1)\n\n            # Filter valid labels\n            valid_mask = batch['stage_labels'] != -1\n\n            all_preds.extend(preds[valid_mask].cpu().numpy())\n            all_labels.extend(batch['stage_labels'][valid_mask].cpu().numpy())\n            all_probs.extend(probs[valid_mask].cpu().numpy())\n\n    # Compute metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    # Multi-class AUC (one-vs-rest)\n    all_probs = np.array(all_probs)\n    all_labels_onehot = np.eye(5)[all_labels]\n    auc = roc_auc_score(all_labels_onehot, all_probs, average='macro', multi_class='ovr')\n\n    return {\n        'accuracy': accuracy,\n        'auc': auc\n    }\n</code></pre>"},{"location":"methods/pretrained-models-and-disease-progression/#part-4-why-visit-grouped-is-superior-for-disease-progression","title":"Part 4: Why Visit-Grouped is Superior for Disease Progression","text":""},{"location":"methods/pretrained-models-and-disease-progression/#comparison-with-other-approaches","title":"Comparison with Other Approaches","text":"Approach Pros Cons Best For Flat Sequence Simple, captures all codes No visit structure, hard to interpret General embeddings Visit-Grouped \u2705 Clinical reality, \u2705 Interpretable, \u2705 Natural granularity More complex Disease progression Hierarchical Respects code types Loses temporal ordering within visit Multi-modal analysis Time-binned Fixed intervals Artificial boundaries Population studies"},{"location":"methods/pretrained-models-and-disease-progression/#key-advantages-for-ckd-staging","title":"Key Advantages for CKD Staging","text":"<ol> <li>Clinical Alignment: CKD stages are assessed at clinic visits</li> <li>Interpretability: \"Patient progressed at Visit 5 due to elevated creatinine + proteinuria\"</li> <li>Temporal Modeling: Visit intervals encode disease velocity (rapid vs slow progression)</li> <li>Prediction Target: \"Will patient progress to next stage by next visit?\"</li> </ol>"},{"location":"methods/pretrained-models-and-disease-progression/#visit-embeddings-capture-disease-state","title":"Visit Embeddings Capture Disease State","text":"<pre><code># Visit embedding captures:\nvisit_embedding = f(\n    diagnosis_codes,      # CKD stage, comorbidities\n    lab_values,           # Creatinine, GFR, proteinuria\n    medications,          # ACE inhibitors, diuretics\n    procedures,           # Dialysis, transplant\n    time_since_last_visit # Disease velocity\n)\n\n# Sequence of visits = disease trajectory\ntrajectory = [visit1_embed, visit2_embed, ..., visitN_embed]\n\n# Predict next state\nnext_stage = progression_model(trajectory)\n</code></pre>"},{"location":"methods/pretrained-models-and-disease-progression/#part-5-practical-implementation-plan","title":"Part 5: Practical Implementation Plan","text":""},{"location":"methods/pretrained-models-and-disease-progression/#week-1-setup-with-pre-trained-model","title":"Week 1: Setup with Pre-trained Model","text":"<pre><code># Install pre-trained model\npip install cehr-bert  # or med-bert\n\n# Load pre-trained embeddings\npython scripts/load_pretrained_embeddings.py\n</code></pre>"},{"location":"methods/pretrained-models-and-disease-progression/#week-2-build-visit-grouped-sequences","title":"Week 2: Build Visit-Grouped Sequences","text":"<pre><code># scripts/build_ckd_sequences.py\nfrom ehrsequencing.data import CKDSequenceBuilder\n\nbuilder = CKDSequenceBuilder(vocab)\nsequences = builder.build_dataset(ckd_patients_df)\n\n# Save\ntorch.save(sequences, 'data/processed/ckd_sequences.pt')\n</code></pre>"},{"location":"methods/pretrained-models-and-disease-progression/#week-3-train-progression-model","title":"Week 3: Train Progression Model","text":"<pre><code># examples/train_ckd_progression.py\nfrom ehrsequencing.models import VisitGroupedProgressionModel\n\n# Load pre-trained embeddings\npretrained = load_cehrbert_embeddings()\n\n# Initialize model\nmodel = VisitGroupedProgressionModel(\n    pretrained_code_embeddings=pretrained,\n    num_stages=5\n)\n\n# Train\ntrain_ckd_model(model, train_loader, val_loader)\n</code></pre>"},{"location":"methods/pretrained-models-and-disease-progression/#week-4-evaluate-interpret","title":"Week 4: Evaluate &amp; Interpret","text":"<pre><code># Evaluate\nmetrics = evaluate_ckd_model(model, test_loader)\n\n# Interpret: Which codes drive progression?\nattention_weights = model.get_visit_attention(patient_sequence)\nimportant_codes = get_top_codes_by_attention(attention_weights)\n</code></pre>"},{"location":"methods/pretrained-models-and-disease-progression/#part-6-advanced-survival-analysis-for-time-to-progression","title":"Part 6: Advanced: Survival Analysis for Time-to-Progression","text":"<pre><code>from lifelines import CoxPHFitter\n\nclass SurvivalProgressionModel(VisitGroupedProgressionModel):\n    \"\"\"\n    Combine visit embeddings with survival analysis.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # Cox proportional hazards head\n        self.cox_head = nn.Linear(kwargs['hidden_dim'], 1)\n\n    def predict_time_to_progression(self, patient_visits, time_features, visit_mask):\n        \"\"\"\n        Predict time until progression to next stage.\n\n        Returns:\n            hazard: Risk score (higher = faster progression)\n        \"\"\"\n        # Get visit sequence representation\n        _, lstm_out = self.forward(patient_visits, time_features, visit_mask)\n\n        # Use last visit representation\n        last_visit_repr = lstm_out[:, -1, :]  # [batch, hidden]\n\n        # Predict hazard\n        hazard = self.cox_head(last_visit_repr)  # [batch, 1]\n\n        return hazard\n</code></pre>"},{"location":"methods/pretrained-models-and-disease-progression/#summary-recommendations","title":"Summary &amp; Recommendations","text":""},{"location":"methods/pretrained-models-and-disease-progression/#for-your-ckd-disease-staging-task","title":"For Your CKD Disease Staging Task","text":"<p>Recommended Architecture:</p> <ol> <li>Code Embeddings: CEHR-BERT pre-trained (don't train from scratch)</li> <li>Sequence Representation: Visit-grouped (your intuition is correct!)</li> <li>Visit Encoder: Transformer (aggregate codes within visit)</li> <li>Sequence Model: LSTM or Transformer (model visit sequence)</li> <li>Prediction Head: Multi-task (stage classification + time-to-progression)</li> </ol> <p>Why This Works:</p> <ul> <li>\u2705 Pre-trained embeddings \u2192 Captures medical knowledge without expensive training</li> <li>\u2705 Visit-grouped \u2192 Aligns with clinical reality and disease assessment</li> <li>\u2705 Hierarchical \u2192 Captures both within-visit patterns and across-visit progression</li> <li>\u2705 Interpretable \u2192 Can explain which visits/codes drive progression</li> </ul> <p>Expected Performance:</p> <ul> <li>Stage classification AUC: 0.85-0.90</li> <li>Time-to-progression C-index: 0.75-0.80</li> <li>Training time: Days (not weeks/months)</li> <li>Inference: &lt;100ms per patient</li> </ul>"},{"location":"methods/pretrained-models-and-disease-progression/#next-steps","title":"Next Steps","text":"<ol> <li>Choose pre-trained model: CEHR-BERT or Med-BERT</li> <li>Build visit-grouped sequences for the target cohort</li> <li>Fine-tune on the data (much cheaper than training from scratch)</li> <li>Evaluate on disease progression metrics</li> <li>Interpret attention weights for clinical insights</li> </ol> <p>The visit-grouped approach is ideal for disease progression modeling.</p> <p>Document Version: 1.0 Last Updated: January 19, 2026</p>"},{"location":"methods/variable-length-sequences/","title":"Handling Variable-Length Patient Histories in Deep Learning","text":"<p>A Practical Guide to Avoiding Temporal Leakage in EHR Sequence Models</p>"},{"location":"methods/variable-length-sequences/#table-of-contents","title":"Table of Contents","text":"<ol> <li>The Challenge: Variable-Length Sequences</li> <li>Why Padding is Dangerous</li> <li>The Solution: Packed Sequences</li> <li>Implementation Guide</li> <li>Common Pitfalls and How to Avoid Them</li> <li>Best Practices for EHR Modeling</li> </ol>"},{"location":"methods/variable-length-sequences/#the-challenge","title":"The Challenge: Variable-Length Sequences","text":""},{"location":"methods/variable-length-sequences/#the-problem","title":"The Problem","text":"<p>In real-world EHR data, patients have vastly different history lengths:</p> <pre><code>Patient A: 3 visits   [Visit 1] \u2192 [Visit 2] \u2192 [Visit 3]\nPatient B: 17 visits  [Visit 1] \u2192 [Visit 2] \u2192 ... \u2192 [Visit 17]\nPatient C: 42 visits  [Visit 1] \u2192 [Visit 2] \u2192 ... \u2192 [Visit 42]\n</code></pre> <p>But deep learning frameworks require fixed-size tensors:</p> <pre><code># PyTorch/TensorFlow want rectangular tensors\nbatch_tensor = torch.zeros(batch_size, max_visits, feature_dim)\n</code></pre>"},{"location":"methods/variable-length-sequences/#the-standard-solution-padding","title":"The Standard Solution: Padding","text":"<p>We pad shorter sequences with zeros to match the longest sequence:</p> <pre><code># Patient A (3 real visits, 39 padding)\n[Visit_1, Visit_2, Visit_3, PAD, PAD, PAD, ..., PAD]\n\n# Patient C (42 real visits, 0 padding)\n[Visit_1, Visit_2, Visit_3, ..., Visit_42]\n</code></pre> <p>This creates a fundamental problem:</p> <p>Padding is not data, but models don't automatically know that.</p>"},{"location":"methods/variable-length-sequences/#why-padding-is-dangerous","title":"Why Padding is Dangerous","text":""},{"location":"methods/variable-length-sequences/#problem-1-the-last-timestep-trap","title":"Problem 1: The \"Last Timestep\" Trap","text":"<p>Consider a patient with 3 real visits in a batch where <code>max_visits = 10</code>:</p> <pre><code>Index:  0        1        2        3    4    5    ...  9\nData:   Visit_1  Visit_2  Visit_3  PAD  PAD  PAD  ...  PAD\n                          \u2191 real last visit\n                                                        \u2191 tensor last position\n</code></pre> <p>Na\u00efve approach (WRONG):</p> <pre><code>lstm_output, (hidden, cell) = lstm(padded_sequences)\nlast_hidden = lstm_output[:, -1, :]  # \u274c This is the hidden state AFTER padding!\n</code></pre> <p>This hidden state is influenced by: - Zero inputs from padding - Learned biases applied to padding - The recurrent computation continuing through non-existent time</p>"},{"location":"methods/variable-length-sequences/#problem-2-temporal-information-leakage","title":"Problem 2: Temporal Information Leakage","text":"<p>The real danger emerges when computing loss over all timesteps:</p> <pre><code># Dangerous pattern\nfor t in range(max_visits):\n    loss += criterion(predictions[:, t], labels[:, t])\n</code></pre> <p>This means the model learns:</p> <pre><code>PAD \u2192 PAD \u2192 PAD \u2192 prediction\n</code></pre> <p>Result: The model learns that padding patterns predict outcomes, which is catastrophic for generalization.</p>"},{"location":"methods/variable-length-sequences/#problem-3-future-information-leakage","title":"Problem 3: Future Information Leakage","text":"<p>In disease progression tasks, this is especially dangerous:</p> <pre><code># Labels computed using full patient history\nprogression_label = did_patient_progress_within_1yr(patient)\n\n# Then used at ALL timesteps\nfor t in range(num_visits):\n    loss += criterion(model_output[t], progression_label)\n</code></pre> <p>This leaks future knowledge backwards: - Early visits \"know\" what happens years later - Model performance looks amazing in training - Model fails completely in prospective deployment</p>"},{"location":"methods/variable-length-sequences/#the-solution","title":"The Solution: Packed Sequences","text":""},{"location":"methods/variable-length-sequences/#what-pack_padded_sequence-does","title":"What <code>pack_padded_sequence</code> Does","text":"<p>PyTorch's <code>pack_padded_sequence</code> tells the LSTM:</p> <p>\"Only process real timesteps. Ignore padding entirely.\"</p> <p>It transforms the rectangular tensor:</p> <pre><code>[batch_size, max_visits, feature_dim]\n</code></pre> <p>into a compact representation:</p> <pre><code>(total_real_visits_across_batch, feature_dim)\n</code></pre> <p>plus metadata that tells the LSTM when each sequence ends.</p>"},{"location":"methods/variable-length-sequences/#key-benefits","title":"Key Benefits","text":"<ol> <li>No padding processing: LSTM never sees padding tokens</li> <li>Correct recurrence: Stops exactly at each patient's last real visit</li> <li>Computational efficiency: Skips unnecessary computations</li> <li>Correct hidden states: <code>h_n</code> contains true final states by construction</li> </ol>"},{"location":"methods/variable-length-sequences/#implementation-guide","title":"Implementation Guide","text":""},{"location":"methods/variable-length-sequences/#step-by-step-correct-usage","title":"Step-by-Step: Correct Usage","text":""},{"location":"methods/variable-length-sequences/#step-1-track-true-sequence-lengths","title":"Step 1: Track True Sequence Lengths","text":"<pre><code># Number of real visits per patient\nlengths = torch.tensor([3, 7, 10, 5])  # batch_size = 4\n</code></pre>"},{"location":"methods/variable-length-sequences/#step-2-create-padded-tensor","title":"Step 2: Create Padded Tensor","text":"<pre><code>batch_size = 4\nmax_visits = 10\nfeature_dim = 128\n\n# Padded sequences\npadded_sequences = torch.zeros(batch_size, max_visits, feature_dim)\n\n# Fill with real data\nfor i, patient_data in enumerate(batch_data):\n    real_length = lengths[i]\n    padded_sequences[i, :real_length] = patient_data\n</code></pre>"},{"location":"methods/variable-length-sequences/#step-3-pack-the-sequences","title":"Step 3: Pack the Sequences","text":"<pre><code>from torch.nn.utils.rnn import pack_padded_sequence\n\npacked_sequences = pack_padded_sequence(\n    padded_sequences,\n    lengths.cpu(),  # Must be on CPU\n    batch_first=True,\n    enforce_sorted=False  # Allows unsorted lengths\n)\n</code></pre> <p>Note: If <code>enforce_sorted=True</code>, you must sort by length descending:</p> <pre><code>lengths, perm_idx = lengths.sort(descending=True)\npadded_sequences = padded_sequences[perm_idx]\n</code></pre>"},{"location":"methods/variable-length-sequences/#step-4-run-lstm","title":"Step 4: Run LSTM","text":"<pre><code>packed_output, (h_n, c_n) = lstm(packed_sequences)\n</code></pre> <p>Now: - <code>h_n[-1, i]</code> is the hidden state at patient <code>i</code>'s true last visit - No padding was processed</p>"},{"location":"methods/variable-length-sequences/#step-5-optional-unpack-for-per-visit-outputs","title":"Step 5: (Optional) Unpack for Per-Visit Outputs","text":"<pre><code>from torch.nn.utils.rnn import pad_packed_sequence\n\nunpacked_output, output_lengths = pad_packed_sequence(\n    packed_output,\n    batch_first=True\n)\n</code></pre> <p>Important: <code>unpacked_output[i, t]</code> is only valid for <code>t &lt; lengths[i]</code></p>"},{"location":"methods/variable-length-sequences/#common-pitfalls","title":"Common Pitfalls and How to Avoid Them","text":""},{"location":"methods/variable-length-sequences/#pitfall-1-using-wrong-last-hidden-state","title":"Pitfall 1: Using Wrong \"Last\" Hidden State","text":"<p>\u274c Wrong:</p> <pre><code>lstm_output, _ = lstm(padded_sequences)\nlast_hidden = lstm_output[:, -1, :]  # After padding!\n</code></pre> <p>\u2705 Correct:</p> <pre><code>packed_input = pack_padded_sequence(padded_sequences, lengths, batch_first=True)\n_, (h_n, c_n) = lstm(packed_input)\nlast_hidden = h_n[-1]  # True last visit\n</code></pre>"},{"location":"methods/variable-length-sequences/#pitfall-2-computing-loss-over-padding","title":"Pitfall 2: Computing Loss Over Padding","text":"<p>\u274c Wrong:</p> <pre><code>predictions = model(padded_sequences)  # [B, T_max, K]\nloss = criterion(predictions, labels)  # Includes padding!\n</code></pre> <p>\u2705 Correct:</p> <pre><code># Create mask for real timesteps\nmask = torch.arange(max_visits)[None, :] &lt; lengths[:, None]  # [B, T_max]\n\n# Only compute loss on real visits\nloss = criterion(predictions[mask], labels[mask])\n</code></pre>"},{"location":"methods/variable-length-sequences/#pitfall-3-non-causal-labels","title":"Pitfall 3: Non-Causal Labels","text":"<p>\u274c Wrong:</p> <pre><code># Label uses information from entire patient history\nlabel = patient.had_outcome_ever()\n\n# Applied to all visits\nfor t in range(num_visits):\n    loss += criterion(pred[t], label)\n</code></pre> <p>\u2705 Correct:</p> <pre><code># Label is causal: only uses information available at time t\nfor t in range(num_visits):\n    # Predict outcome in next 6 months from visit t\n    label_t = patient.had_outcome_between(t, t + 6_months)\n    loss += criterion(pred[t], label_t)\n</code></pre>"},{"location":"methods/variable-length-sequences/#best-practices","title":"Best Practices for EHR Modeling","text":""},{"location":"methods/variable-length-sequences/#checklist-ehr-safe-lstm-implementation","title":"Checklist: EHR-Safe LSTM Implementation","text":"<p>Ensure all of these are true:</p> <ul> <li>\u2705 Use <code>pack_padded_sequence</code> for variable visit counts</li> <li>\u2705 Use <code>h_n[-1]</code> for patient-level representations</li> <li>\u2705 Mask losses for visit-level predictions</li> <li>\u2705 Define labels causally per visit</li> <li>\u2705 Never let padding participate in loss or recurrence</li> <li>\u2705 Validate that padding is truly ignored (check gradients)</li> </ul>"},{"location":"methods/variable-length-sequences/#pattern-a-patient-level-prediction-many-to-one","title":"Pattern A: Patient-Level Prediction (Many-to-One)","text":"<p>Task: Predict patient outcome using full history</p> <pre><code>def forward(self, visit_sequences, lengths):\n    # Pack sequences\n    packed = pack_padded_sequence(\n        visit_sequences, \n        lengths, \n        batch_first=True,\n        enforce_sorted=False\n    )\n\n    # LSTM\n    _, (h_n, c_n) = self.lstm(packed)\n\n    # Use final hidden state\n    patient_repr = h_n[-1]  # [batch_size, hidden_dim]\n\n    # Predict\n    logits = self.classifier(patient_repr)\n    return logits\n</code></pre> <p>Interpretation: \"Predict using what the patient looked like at their last real visit\"</p>"},{"location":"methods/variable-length-sequences/#pattern-b-visit-level-prediction-many-to-many","title":"Pattern B: Visit-Level Prediction (Many-to-Many)","text":"<p>Task: Predict outcome at each visit</p> <pre><code>def forward(self, visit_sequences, lengths):\n    # Pack\n    packed = pack_padded_sequence(\n        visit_sequences,\n        lengths,\n        batch_first=True,\n        enforce_sorted=False\n    )\n\n    # LSTM\n    packed_output, _ = self.lstm(packed)\n\n    # Unpack\n    lstm_output, _ = pad_packed_sequence(\n        packed_output,\n        batch_first=True\n    )  # [batch_size, max_visits, hidden_dim]\n\n    # Predict at each visit\n    logits = self.classifier(lstm_output)  # [batch_size, max_visits, num_classes]\n\n    return logits, lengths\n\ndef compute_loss(self, logits, labels, lengths):\n    batch_size, max_visits = logits.shape[:2]\n\n    # Create mask for real visits\n    mask = torch.arange(max_visits)[None, :] &lt; lengths[:, None]\n\n    # Only compute loss on real visits\n    loss = self.criterion(logits[mask], labels[mask])\n    return loss\n</code></pre>"},{"location":"methods/variable-length-sequences/#pattern-c-causal-label-definition","title":"Pattern C: Causal Label Definition","text":"<p>Critical: Labels must only use information available up to time <code>t</code></p> <pre><code>def create_causal_labels(patient_visits, prediction_horizon='6M'):\n    \"\"\"\n    Create labels that are causal with respect to each visit.\n\n    Args:\n        patient_visits: List of visits with timestamps\n        prediction_horizon: How far ahead to predict (e.g., '6M', '1Y')\n\n    Returns:\n        labels: [num_visits] - outcome occurred within horizon\n    \"\"\"\n    labels = []\n\n    for i, visit in enumerate(patient_visits):\n        visit_time = visit.timestamp\n        horizon_end = visit_time + prediction_horizon\n\n        # Check if outcome occurred in prediction window\n        # ONLY using information AFTER this visit\n        future_visits = patient_visits[i+1:]\n        outcome_in_window = any(\n            visit_time &lt; v.timestamp &lt;= horizon_end and v.has_outcome\n            for v in future_visits\n        )\n\n        labels.append(outcome_in_window)\n\n    return torch.tensor(labels)\n</code></pre>"},{"location":"methods/variable-length-sequences/#philosophical-understanding","title":"Philosophical Understanding","text":""},{"location":"methods/variable-length-sequences/#mental-model-stopped-stochastic-processes","title":"Mental Model: Stopped Stochastic Processes","text":"<p>An LSTM with packed sequences models:</p> <p>A stopped stochastic process where each patient trajectory ends at a different time.</p> <p>Key insight: Padding is not \"missing data\" - it's non-existent time.</p> <p>If padding is treated as time, the model will exploit it. This is not a bug in the code; it's a fundamental modeling error.</p>"},{"location":"methods/variable-length-sequences/#the-clinical-interpretation","title":"The Clinical Interpretation","text":"<p>When you use <code>pack_padded_sequence</code>:</p> <ul> <li>Without packing: \"Last timestep\" = <code>max_visits - 1</code> (artifact of batching)</li> <li>With packing: \"Last timestep\" = <code>lengths[i] - 1</code> (clinical event boundary)</li> </ul> <p>This distinction is everything in EHR modeling.</p>"},{"location":"methods/variable-length-sequences/#summary","title":"Summary","text":"<p>Variable-length sequences are ubiquitous in EHR data. Handling them correctly requires:</p> <ol> <li>Use packed sequences to prevent padding from influencing the model</li> <li>Extract hidden states correctly using <code>h_n</code> not <code>output[:, -1]</code></li> <li>Mask losses when making per-visit predictions</li> <li>Define labels causally to prevent future information leakage</li> <li>Consider carefully what \"time\" means in the model</li> </ol> <p>The bottom line: If you see suspiciously good results on EHR sequence modeling, check your padding and temporal leakage first.</p>"},{"location":"methods/variable-length-sequences/#further-reading","title":"Further Reading","text":"<ul> <li>Next topic: Designing progression labels that are both causal and statistically efficient (handling censoring and irregular follow-up)</li> <li>Related: How to handle missing visits vs. padding (they're different!)</li> <li>Advanced: Attention mechanisms and variable-length sequences</li> </ul> <p>Last updated: January 2026 Related code: <code>src/ehrsequencing/models/lstm_baseline.py</code></p>"},{"location":"methods/within-visit-structure/","title":"Within-Visit Structure and Multi-Level Embeddings","text":"<p>Date: January 20, 2026 Focus: How to structure codes within a visit when timestamps are unavailable</p>"},{"location":"methods/within-visit-structure/#the-problem","title":"The Problem","text":"<p>Challenge: A visit contains multiple codes (ICD, LOINC, RxNorm, etc.) that may not have individual timestamps.</p> <pre><code>Visit 1 (2023-01-15):\n  - E11.9 (Type 2 diabetes)\n  - 4548-4 (HbA1c)\n  - 38341003 (Hypertension)\n  - RxNorm:860975 (Metformin)\n\n# Question: What order should these codes be in?\n# They're all \"from the same visit\" but conceptually different\n</code></pre> <p>Key Questions: 1. How do we impose structure within each visit? 2. Does order matter if codes lack individual timestamps? 3. What does the LSTM learn at each level?</p>"},{"location":"methods/within-visit-structure/#part-1-within-visit-structure-approaches","title":"Part 1: Within-Visit Structure Approaches","text":""},{"location":"methods/within-visit-structure/#approach-1-semantic-grouping-recommended","title":"Approach 1: Semantic Grouping (Recommended)","text":"<p>Idea: Group codes by their semantic type, impose a canonical ordering.</p> <pre><code># Canonical ordering within a visit\nVISIT_CODE_ORDER = [\n    'diagnosis',      # ICD codes\n    'procedure',      # CPT, SNOMED procedures\n    'lab',           # LOINC codes\n    'medication',    # RxNorm\n    'vital',         # Vital signs\n]\n\n# Example visit after ordering\nVisit 1:\n  Diagnosis:  E11.9, 38341003\n  Lab:        4548-4\n  Medication: RxNorm:860975\n</code></pre> <p>Advantages: - \u2705 Consistent structure across all visits - \u2705 Reflects clinical workflow (diagnose \u2192 test \u2192 treat) - \u2705 Enables model to learn semantic patterns - \u2705 No arbitrary ordering</p> <p>Implementation:</p> <pre><code>from typing import List, Dict\nfrom dataclasses import dataclass\n\n@dataclass\nclass CodeWithType:\n    code: str\n    code_type: str  # 'diagnosis', 'lab', 'medication', etc.\n    value: Optional[float] = None  # For lab values\n\nclass VisitStructurer:\n    \"\"\"Structure codes within a visit by semantic type.\"\"\"\n\n    # Canonical ordering\n    TYPE_ORDER = {\n        'diagnosis': 0,\n        'procedure': 1,\n        'lab': 2,\n        'medication': 3,\n        'vital': 4,\n        'other': 5\n    }\n\n    def structure_visit(\n        self, \n        codes: List[CodeWithType]\n    ) -&gt; List[CodeWithType]:\n        \"\"\"\n        Order codes within a visit by semantic type.\n\n        Within each type, codes are sorted alphabetically for consistency.\n        \"\"\"\n        # Sort by type, then alphabetically within type\n        structured = sorted(\n            codes,\n            key=lambda c: (self.TYPE_ORDER.get(c.code_type, 999), c.code)\n        )\n        return structured\n\n# Example usage\nvisit_codes = [\n    CodeWithType('RxNorm:860975', 'medication'),\n    CodeWithType('E11.9', 'diagnosis'),\n    CodeWithType('4548-4', 'lab', value=7.2),\n    CodeWithType('38341003', 'diagnosis'),\n]\n\nstructurer = VisitStructurer()\nordered_codes = structurer.structure_visit(visit_codes)\n\n# Result:\n# [E11.9, 38341003, 4548-4, RxNorm:860975]\n# diagnosis \u2192 diagnosis \u2192 lab \u2192 medication\n</code></pre>"},{"location":"methods/within-visit-structure/#approach-2-set-based-representation-order-invariant","title":"Approach 2: Set-Based Representation (Order-Invariant)","text":"<p>Idea: Treat visit as an unordered set, use pooling to aggregate.</p> <pre><code>class SetBasedVisitEncoder(nn.Module):\n    \"\"\"\n    Encode visit as a set of codes (order-invariant).\n\n    Uses pooling (mean, max, or attention) to aggregate.\n    \"\"\"\n\n    def __init__(self, code_embed_dim: int, visit_embed_dim: int):\n        super().__init__()\n        self.code_embeddings = nn.Embedding(vocab_size, code_embed_dim)\n        self.projection = nn.Linear(code_embed_dim, visit_embed_dim)\n\n    def forward(self, visit_codes, visit_mask):\n        \"\"\"\n        Args:\n            visit_codes: [batch, max_codes_per_visit]\n            visit_mask: [batch, max_codes_per_visit]\n\n        Returns:\n            visit_embedding: [batch, visit_embed_dim]\n        \"\"\"\n        # Embed codes\n        code_embeds = self.code_embeddings(visit_codes)  # [batch, codes, dim]\n\n        # Mask padding\n        code_embeds = code_embeds * visit_mask.unsqueeze(-1)\n\n        # Aggregate (mean pooling - order invariant)\n        visit_repr = code_embeds.sum(dim=1) / visit_mask.sum(dim=1, keepdim=True)\n\n        # Project to visit space\n        return self.projection(visit_repr)\n</code></pre> <p>Advantages: - \u2705 No ordering assumptions - \u2705 Mathematically clean (permutation invariant) - \u2705 Simple implementation</p> <p>Disadvantages: - \u274c Loses potential semantic ordering information - \u274c Treats all codes equally (no clinical workflow)</p>"},{"location":"methods/within-visit-structure/#approach-3-attention-based-aggregation-best-of-both-worlds","title":"Approach 3: Attention-Based Aggregation (Best of Both Worlds)","text":"<p>Idea: Use self-attention within each visit to learn importance weights.</p> <pre><code>class AttentionVisitEncoder(nn.Module):\n    \"\"\"\n    Use self-attention to aggregate codes within a visit.\n\n    Learns which codes are most important for the visit representation.\n    \"\"\"\n\n    def __init__(\n        self, \n        code_embed_dim: int, \n        visit_embed_dim: int,\n        num_heads: int = 4\n    ):\n        super().__init__()\n        self.code_embeddings = nn.Embedding(vocab_size, code_embed_dim)\n\n        # Self-attention within visit\n        self.self_attention = nn.MultiheadAttention(\n            embed_dim=code_embed_dim,\n            num_heads=num_heads,\n            batch_first=True\n        )\n\n        # Project to visit embedding\n        self.projection = nn.Linear(code_embed_dim, visit_embed_dim)\n\n    def forward(self, visit_codes, visit_mask):\n        \"\"\"\n        Args:\n            visit_codes: [batch, max_codes_per_visit]\n            visit_mask: [batch, max_codes_per_visit]\n\n        Returns:\n            visit_embedding: [batch, visit_embed_dim]\n            attention_weights: [batch, num_heads, codes, codes]\n        \"\"\"\n        # Embed codes\n        code_embeds = self.code_embeddings(visit_codes)  # [batch, codes, dim]\n\n        # Self-attention (codes attend to each other)\n        attn_output, attn_weights = self.self_attention(\n            code_embeds, code_embeds, code_embeds,\n            key_padding_mask=~visit_mask.bool()\n        )\n\n        # Aggregate (mean over codes)\n        visit_repr = attn_output.mean(dim=1)  # [batch, code_embed_dim]\n\n        # Project to visit space\n        visit_embedding = self.projection(visit_repr)\n\n        return visit_embedding, attn_weights\n</code></pre> <p>Advantages: - \u2705 Learns importance of each code - \u2705 Order-invariant but captures relationships - \u2705 Interpretable (attention weights show which codes matter) - \u2705 Flexible (adapts to different visit types)</p> <p>Disadvantages: - \u274c More complex - \u274c Requires more computation</p>"},{"location":"methods/within-visit-structure/#approach-4-hierarchical-type-embeddings","title":"Approach 4: Hierarchical Type Embeddings","text":"<p>Idea: Add type embeddings to indicate code category.</p> <pre><code>class HierarchicalVisitEncoder(nn.Module):\n    \"\"\"\n    Add type embeddings to distinguish code categories.\n\n    Similar to position embeddings in Transformers.\n    \"\"\"\n\n    def __init__(self, code_embed_dim: int, visit_embed_dim: int):\n        super().__init__()\n\n        # Code embeddings\n        self.code_embeddings = nn.Embedding(vocab_size, code_embed_dim)\n\n        # Type embeddings (diagnosis, lab, medication, etc.)\n        self.type_embeddings = nn.Embedding(\n            num_embeddings=6,  # diagnosis, procedure, lab, med, vital, other\n            embedding_dim=code_embed_dim\n        )\n\n        # LSTM over codes\n        self.code_lstm = nn.LSTM(\n            input_size=code_embed_dim,\n            hidden_size=code_embed_dim,\n            batch_first=True\n        )\n\n        self.projection = nn.Linear(code_embed_dim, visit_embed_dim)\n\n    def forward(self, visit_codes, code_types, visit_mask):\n        \"\"\"\n        Args:\n            visit_codes: [batch, max_codes_per_visit]\n            code_types: [batch, max_codes_per_visit] (0=diagnosis, 1=procedure, etc.)\n            visit_mask: [batch, max_codes_per_visit]\n\n        Returns:\n            visit_embedding: [batch, visit_embed_dim]\n        \"\"\"\n        # Embed codes and types\n        code_embeds = self.code_embeddings(visit_codes)\n        type_embeds = self.type_embeddings(code_types)\n\n        # Combine (additive, like position embeddings)\n        combined_embeds = code_embeds + type_embeds\n\n        # LSTM over codes (now with type information)\n        lstm_out, (hidden, _) = self.code_lstm(combined_embeds)\n\n        # Use final hidden state\n        visit_repr = hidden[-1]\n\n        return self.projection(visit_repr)\n</code></pre> <p>Advantages: - \u2705 Explicitly encodes code type - \u2705 Works with sequential models (LSTM) - \u2705 Learns type-specific patterns</p>"},{"location":"methods/within-visit-structure/#part-2-what-does-lstm-learn-at-each-level","title":"Part 2: What Does LSTM Learn at Each Level?","text":""},{"location":"methods/within-visit-structure/#multi-level-embedding-learning","title":"Multi-Level Embedding Learning","text":"<p>The LSTM architecture learns three levels of embeddings:</p> <pre><code>Level 1: Code Embeddings (learned)\n    \u2193\nLevel 2: Visit Embeddings (learned)\n    \u2193\nLevel 3: Patient Embeddings (learned)\n</code></pre> <p>Let me clarify exactly what is learned at each level:</p>"},{"location":"methods/within-visit-structure/#level-1-code-embeddings","title":"Level 1: Code Embeddings","text":"<p>What: Individual medical code representations (E11.9, 4548-4, etc.)</p> <p>Learned by: <code>nn.Embedding</code> layer (or pre-trained from CEHR-BERT)</p> <pre><code>self.code_embeddings = nn.Embedding(\n    num_embeddings=vocab_size,  # e.g., 10,000 codes\n    embedding_dim=code_embed_dim  # e.g., 128\n)\n\n# Input: code ID (integer)\ncode_id = 42  # e.g., E11.9 mapped to ID 42\n\n# Output: code embedding (vector)\ncode_embedding = self.code_embeddings(code_id)  # [128]\n</code></pre> <p>What is learned: - Semantic meaning of each code - Relationships between codes (e.g., diabetes codes cluster together) - Clinical context (e.g., HbA1c associated with diabetes)</p> <p>Example: <pre><code># After training, similar codes have similar embeddings\nembedding_E11_9 = code_embeddings[42]    # Type 2 diabetes\nembedding_E11_0 = code_embeddings[43]    # Type 1 diabetes\ncosine_similarity(embedding_E11_9, embedding_E11_0)  # High similarity\n\nembedding_J45_9 = code_embeddings[100]   # Asthma\ncosine_similarity(embedding_E11_9, embedding_J45_9)  # Low similarity\n</code></pre></p> <p>Can use pre-trained: <pre><code># Option 1: Learn from scratch\nself.code_embeddings = nn.Embedding(vocab_size, code_embed_dim)\n\n# Option 2: Use pre-trained (CEHR-BERT)\npretrained_embeds = load_cehrbert_embeddings()\nself.code_embeddings = nn.Embedding.from_pretrained(pretrained_embeds, freeze=False)\n</code></pre></p>"},{"location":"methods/within-visit-structure/#level-2-visit-embeddings","title":"Level 2: Visit Embeddings","text":"<p>What: Representation of an entire visit (aggregation of codes)</p> <p>Learned by: LSTM or pooling over codes within a visit</p> <pre><code># Within-visit LSTM\nself.visit_lstm = nn.LSTM(\n    input_size=code_embed_dim,  # 128\n    hidden_size=code_embed_dim,  # 128\n    num_layers=1,\n    batch_first=True\n)\n\n# Input: sequence of code embeddings for one visit\nvisit_codes = [code_emb_1, code_emb_2, code_emb_3, ...]  # [num_codes, 128]\n\n# Output: visit embedding\nlstm_out, (hidden, _) = self.visit_lstm(visit_codes)\nvisit_embedding = hidden[-1]  # [128]\n</code></pre> <p>What is learned: - How to combine multiple codes into a visit representation - Importance of code order (if using LSTM) or relationships (if using attention) - Visit-level patterns (e.g., \"diabetes visit\" vs \"routine checkup\")</p> <p>Example: <pre><code># Visit 1: Diabetes-related\nvisit_1_codes = [E11.9, 4548-4, RxNorm:860975]  # Diabetes, HbA1c, Metformin\nvisit_1_embedding = encode_visit(visit_1_codes)  # [128]\n\n# Visit 2: Hypertension-related\nvisit_2_codes = [I10, 8480-6, RxNorm:197361]  # Hypertension, BP, Lisinopril\nvisit_2_embedding = encode_visit(visit_2_codes)  # [128]\n\n# Different visit types have different embeddings\ncosine_similarity(visit_1_embedding, visit_2_embedding)  # Moderate similarity\n</code></pre></p> <p>Projection to visit space: <pre><code># Often project to a different dimension\nself.visit_projection = nn.Linear(code_embed_dim, visit_embed_dim)\nvisit_embedding = self.visit_projection(visit_repr)  # [128] \u2192 [256]\n</code></pre></p>"},{"location":"methods/within-visit-structure/#level-3-patient-embeddings","title":"Level 3: Patient Embeddings","text":"<p>What: Representation of entire patient trajectory (sequence of visits)</p> <p>Learned by: LSTM over visit sequence</p> <pre><code># Visit sequence LSTM\nself.sequence_lstm = nn.LSTM(\n    input_size=visit_embed_dim + 2,  # 256 + 2 time features\n    hidden_size=hidden_dim,  # 512\n    num_layers=2,\n    batch_first=True\n)\n\n# Input: sequence of visit embeddings\npatient_visits = [visit_emb_1, visit_emb_2, ..., visit_emb_N]  # [N, 256]\n\n# Output: patient embedding\nsequence_out, (final_hidden, _) = self.sequence_lstm(patient_visits)\npatient_embedding = final_hidden[-1]  # [512]\n</code></pre> <p>What is learned: - Temporal patterns across visits - Disease progression trajectories - Long-term dependencies (e.g., chronic conditions) - Patient-level risk factors</p> <p>Example: <pre><code># Patient A: Stable diabetes\npatient_A_visits = [diabetes_visit_1, diabetes_visit_2, diabetes_visit_3]\npatient_A_embedding = encode_patient(patient_A_visits)  # [512]\n\n# Patient B: Progressing CKD\npatient_B_visits = [ckd_stage2_visit, ckd_stage3_visit, ckd_stage4_visit]\npatient_B_embedding = encode_patient(patient_B_visits)  # [512]\n\n# Different trajectories have different embeddings\ncosine_similarity(patient_A_embedding, patient_B_embedding)  # Low similarity\n</code></pre></p>"},{"location":"methods/within-visit-structure/#part-3-complete-architecture-with-all-three-levels","title":"Part 3: Complete Architecture with All Three Levels","text":"<pre><code>class ThreeLevelLSTMEncoder(nn.Module):\n    \"\"\"\n    Three-level LSTM encoder for EHR sequences.\n\n    Level 1: Code embeddings (learned or pre-trained)\n    Level 2: Visit embeddings (learned via LSTM/attention)\n    Level 3: Patient embeddings (learned via LSTM over visits)\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        code_embed_dim: int = 128,\n        visit_embed_dim: int = 256,\n        patient_embed_dim: int = 512,\n        num_layers: int = 2,\n        use_attention: bool = False,\n        use_type_embeddings: bool = True\n    ):\n        super().__init__()\n\n        # ====================================================================\n        # LEVEL 1: Code Embeddings\n        # ====================================================================\n        self.code_embeddings = nn.Embedding(\n            num_embeddings=vocab_size,\n            embedding_dim=code_embed_dim,\n            padding_idx=0\n        )\n\n        # Optional: Type embeddings (diagnosis, lab, medication, etc.)\n        if use_type_embeddings:\n            self.type_embeddings = nn.Embedding(\n                num_embeddings=6,  # 6 code types\n                embedding_dim=code_embed_dim\n            )\n        else:\n            self.type_embeddings = None\n\n        # ====================================================================\n        # LEVEL 2: Visit Embeddings\n        # ====================================================================\n        if use_attention:\n            # Attention-based visit encoder\n            self.visit_encoder = nn.MultiheadAttention(\n                embed_dim=code_embed_dim,\n                num_heads=4,\n                batch_first=True\n            )\n        else:\n            # LSTM-based visit encoder\n            self.visit_encoder = nn.LSTM(\n                input_size=code_embed_dim,\n                hidden_size=code_embed_dim,\n                num_layers=1,\n                batch_first=True\n            )\n\n        # Project to visit embedding space\n        self.visit_projection = nn.Linear(code_embed_dim, visit_embed_dim)\n\n        # ====================================================================\n        # LEVEL 3: Patient Embeddings\n        # ====================================================================\n        self.patient_encoder = nn.LSTM(\n            input_size=visit_embed_dim + 2,  # +2 for time features\n            hidden_size=patient_embed_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=0.1 if num_layers &gt; 1 else 0\n        )\n\n        self.use_attention = use_attention\n\n    def encode_codes(self, codes, code_types=None):\n        \"\"\"\n        LEVEL 1: Encode individual codes.\n\n        Args:\n            codes: [batch, num_codes]\n            code_types: [batch, num_codes] (optional)\n\n        Returns:\n            code_embeddings: [batch, num_codes, code_embed_dim]\n        \"\"\"\n        code_embeds = self.code_embeddings(codes)\n\n        if self.type_embeddings is not None and code_types is not None:\n            type_embeds = self.type_embeddings(code_types)\n            code_embeds = code_embeds + type_embeds\n\n        return code_embeds\n\n    def encode_visit(self, visit_codes, code_types=None, visit_mask=None):\n        \"\"\"\n        LEVEL 2: Encode a visit (aggregate codes).\n\n        Args:\n            visit_codes: [batch, max_codes_per_visit]\n            code_types: [batch, max_codes_per_visit]\n            visit_mask: [batch, max_codes_per_visit]\n\n        Returns:\n            visit_embedding: [batch, visit_embed_dim]\n        \"\"\"\n        # Get code embeddings\n        code_embeds = self.encode_codes(visit_codes, code_types)\n\n        if self.use_attention:\n            # Attention-based aggregation\n            attn_out, _ = self.visit_encoder(\n                code_embeds, code_embeds, code_embeds,\n                key_padding_mask=~visit_mask.bool() if visit_mask is not None else None\n            )\n            visit_repr = attn_out.mean(dim=1)\n        else:\n            # LSTM-based aggregation\n            lstm_out, (hidden, _) = self.visit_encoder(code_embeds)\n            visit_repr = hidden[-1]\n\n        # Project to visit space\n        return self.visit_projection(visit_repr)\n\n    def encode_patient(self, patient_visits, time_features, visit_mask):\n        \"\"\"\n        LEVEL 3: Encode patient trajectory (sequence of visits).\n\n        Args:\n            patient_visits: [batch, num_visits, max_codes_per_visit]\n            time_features: [batch, num_visits, 2]\n            visit_mask: [batch, num_visits, max_codes_per_visit]\n\n        Returns:\n            patient_embedding: [batch, patient_embed_dim]\n            visit_embeddings: [batch, num_visits, visit_embed_dim]\n        \"\"\"\n        batch_size, num_visits, max_codes = patient_visits.shape\n\n        # Encode each visit\n        visit_embeds = []\n        for i in range(num_visits):\n            visit_emb = self.encode_visit(\n                patient_visits[:, i, :],\n                visit_mask=visit_mask[:, i, :] if visit_mask is not None else None\n            )\n            visit_embeds.append(visit_emb)\n\n        visit_embeds = torch.stack(visit_embeds, dim=1)  # [batch, visits, visit_dim]\n\n        # Add time features\n        visit_embeds_with_time = torch.cat([visit_embeds, time_features], dim=-1)\n\n        # LSTM over visits\n        sequence_out, (final_hidden, _) = self.patient_encoder(visit_embeds_with_time)\n\n        patient_embedding = final_hidden[-1]  # [batch, patient_embed_dim]\n\n        return patient_embedding, visit_embeds\n</code></pre>"},{"location":"methods/within-visit-structure/#part-4-summary-and-recommendations","title":"Part 4: Summary and Recommendations","text":""},{"location":"methods/within-visit-structure/#within-visit-structure-recommended-approach","title":"Within-Visit Structure: Recommended Approach","text":"<p>Recommended primary approach:</p> <p>Semantic Grouping + Type Embeddings (Approach 1 + 4)</p> <pre><code># 1. Order codes by semantic type\nvisit_codes = structure_by_type(raw_codes)  # diagnosis \u2192 lab \u2192 medication\n\n# 2. Add type embeddings\ncode_embeds = code_embeddings(codes) + type_embeddings(types)\n\n# 3. LSTM over ordered codes\nvisit_embedding = lstm(code_embeds)\n</code></pre> <p>Why: - \u2705 Consistent, interpretable structure - \u2705 Reflects clinical workflow - \u2705 Works well with LSTM - \u2705 Can still use pre-trained code embeddings</p> <p>Alternative: Attention-based (Approach 3) - Use when order-invariance is desired - More flexible but more complex - Better for Transformer models</p>"},{"location":"methods/within-visit-structure/#what-lstm-learns-summary-table","title":"What LSTM Learns: Summary Table","text":"Level What Learned By Dimension Example Level 1 Code embeddings <code>nn.Embedding</code> [vocab_size, 128] E11.9 \u2192 [0.1, -0.3, ...] Level 2 Visit embeddings LSTM/Attention over codes [256] [diabetes visit] \u2192 [0.5, 0.2, ...] Level 3 Patient embeddings LSTM over visits [512] [patient trajectory] \u2192 [0.3, -0.1, ...] <p>Key Points: 1. All three levels are learned (or Level 1 can be pre-trained) 2. Each level captures different granularity 3. Gradients flow through all levels during training 4. Can freeze Level 1 if using pre-trained embeddings</p>"},{"location":"methods/within-visit-structure/#implementation-priority","title":"Implementation Priority","text":"<p>Phase 1: Simple Baseline - Semantic grouping (canonical order) - LSTM over codes (Level 2) - LSTM over visits (Level 3) - Learn all embeddings from scratch</p> <p>Phase 2: Enhanced - Add type embeddings - Use pre-trained code embeddings (CEHR-BERT) - Experiment with attention</p> <p>Phase 3: Advanced - Attention-based visit encoder - Hierarchical attention (code \u2192 visit \u2192 patient) - Multi-task learning</p> <p>Next: Ready to implement the data pipeline with visit structuring?</p>"},{"location":"notebooks/","title":"EHR Sequencing Notebooks","text":"<p>Educational notebooks for learning EHR sequence modeling concepts and exploring the <code>ehrsequencing</code> package.</p>"},{"location":"notebooks/#purpose","title":"Purpose","text":"<p>These notebooks serve as: - Educational tutorials for understanding EHR data processing and modeling - Exploratory analysis of Synthea synthetic EHR data - Step-by-step guides for the complete modeling pipeline - Reference implementations for common tasks</p>"},{"location":"notebooks/#structure","title":"Structure","text":"<p>Notebooks are organized by topic into subdirectories:</p> <pre><code>notebooks/\n\u251c\u2500\u2500 README.md (this file)\n\u251c\u2500\u2500 01_synthea_data_exploration/\n\u2502   \u251c\u2500\u2500 01_synthea_data_exploration.ipynb\n\u2502   \u251c\u2500\u2500 01a_lstm_data_preparation.ipynb\n\u2502   \u251c\u2500\u2500 data_shape_transformations.md\n\u2502   \u2514\u2500\u2500 README.md\n\u251c\u2500\u2500 02_survival_analysis/\n\u2502   \u251c\u2500\u2500 01_discrete_time_survival_lstm.ipynb\n\u2502   \u251c\u2500\u2500 validate_survival_model.py\n\u2502   \u2514\u2500\u2500 README.md\n\u2514\u2500\u2500 (future topics)/\n</code></pre>"},{"location":"notebooks/#available-notebooks","title":"Available Notebooks","text":""},{"location":"notebooks/#01-synthea-data-exploration","title":"01. Synthea Data Exploration","text":"<p>Directory: <code>01_synthea_data_exploration/</code></p> <p>Comprehensive exploration of Synthea synthetic EHR data and preparation for modeling.</p>"},{"location":"notebooks/#notebooks","title":"Notebooks:","text":"<ol> <li><code>01_synthea_data_exploration.ipynb</code></li> <li>Load and explore Synthea CSV files</li> <li>Understand patient demographics and clinical data</li> <li>Group events into clinical visits</li> <li>Build patient sequences</li> <li>Compute data statistics and quality metrics</li> <li> <p>Visualize temporal patterns</p> </li> <li> <p><code>01a_lstm_data_preparation.ipynb</code></p> </li> <li>Prepare visit-grouped sequences for LSTM model</li> <li>Encode sequences to integer IDs</li> <li>Create labels for prediction tasks (diabetes detection)</li> <li>Batch data with proper padding and masking</li> <li>Visualize data shape transformations</li> <li>Test LSTM model forward pass</li> </ol>"},{"location":"notebooks/#documentation","title":"Documentation:","text":"<ul> <li><code>data_shape_transformations.md</code>: Comprehensive reference for all data shape transformations from raw CSV to model predictions</li> </ul>"},{"location":"notebooks/#02-survival-analysis","title":"02. Survival Analysis","text":"<p>Directory: <code>02_survival_analysis/</code></p> <p>Discrete-time survival analysis for disease progression modeling using LSTMs.</p>"},{"location":"notebooks/#notebooks_1","title":"Notebooks:","text":"<ol> <li><code>01_discrete_time_survival_lstm.ipynb</code></li> <li>Understanding the C-index (concordance index)</li> <li>Research questions and clinical applications</li> <li>Data labeling strategies for survival outcomes</li> <li>Synthetic survival outcome generation</li> <li>Training discrete-time survival LSTM models</li> <li>Memory estimation and cloud training setup</li> <li>Handling censored data and competing risks</li> </ol>"},{"location":"notebooks/#scripts","title":"Scripts:","text":"<ul> <li><code>validate_survival_model.py</code>: Validation script for quick model testing with options for:</li> <li>Patient subsampling for local testing</li> <li>Example patient sequence display</li> <li>Adjustable model complexity</li> <li>Memory estimation</li> <li>Synthetic outcome quality checks</li> </ul>"},{"location":"notebooks/#key-features","title":"Key Features:","text":"<ul> <li>Memory management: Subsample patients for local testing (200 patients) or use full dataset on cloud GPUs</li> <li>Cloud training guide: Instructions for RunPods/Vast.ai setup with GPU recommendations</li> <li>Synthetic outcomes: Risk-based survival outcome generation with controllable censoring rates</li> <li>C-index evaluation: Proper concordance index computation for survival models</li> <li>Diagnostic tools: Correlation checks to validate synthetic outcome quality</li> </ul>"},{"location":"notebooks/#getting-started","title":"Getting Started","text":""},{"location":"notebooks/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Environment setup: <pre><code>cd /path/to/ehr-sequencing\nmamba activate ehrsequencing\n</code></pre></p> </li> <li> <p>Data setup:</p> </li> <li>Option A: Use shared Synthea data from <code>loinc-predictor</code> project</li> <li>Option B: Generate your own Synthea data (see <code>docs/datasets/SYNTHEA_SETUP.md</code>)</li> <li> <p>Option C: Use test fixtures for quick exploration</p> </li> <li> <p>Launch Jupyter: <pre><code>jupyter lab\n</code></pre></p> </li> </ol>"},{"location":"notebooks/#recommended-order","title":"Recommended Order","text":"<p>For first-time users, follow this sequence:</p> <ol> <li>Start with <code>01_synthea_data_exploration/01_synthea_data_exploration.ipynb</code></li> <li>Understand the data structure</li> <li>Learn the core APIs</li> <li> <p>See the complete pipeline</p> </li> <li> <p>Continue with <code>01_synthea_data_exploration/01a_lstm_data_preparation.ipynb</code></p> </li> <li>Learn how to prepare data for models</li> <li>Understand shape transformations</li> <li> <p>See LSTM input format</p> </li> <li> <p>Reference <code>01_synthea_data_exploration/data_shape_transformations.md</code></p> </li> <li>Detailed shape specifications</li> <li>Memory considerations</li> <li> <p>Common pitfalls</p> </li> <li> <p>Explore production code in <code>examples/</code></p> </li> <li><code>train_lstm_baseline.py</code> - Full training script</li> <li>See how notebooks translate to production</li> </ol>"},{"location":"notebooks/#notebook-philosophy","title":"Notebook Philosophy","text":""},{"location":"notebooks/#educational-focus","title":"Educational Focus","text":"<p>These notebooks prioritize: - Clarity over brevity: Explicit steps with explanations - Visualization: Plots and diagrams to aid understanding - Interactivity: Encourage experimentation - Small datasets: Fast iteration for learning</p>"},{"location":"notebooks/#differences-from-production-code","title":"Differences from Production Code","text":"Aspect Notebooks Production (<code>examples/</code>) Purpose Learning &amp; exploration Deployment &amp; scale Data size Small subsets (50-100 patients) Full datasets (1000s of patients) Code style Step-by-step, verbose Modular, reusable functions Output Rich visualizations Metrics, checkpoints, logs Error handling Minimal (for clarity) Comprehensive Performance Not optimized Optimized for speed/memory"},{"location":"notebooks/#data-requirements","title":"Data Requirements","text":""},{"location":"notebooks/#synthea-data","title":"Synthea Data","text":"<p>Most notebooks use Synthea synthetic EHR data. See <code>docs/datasets/SYNTHEA_SETUP.md</code> for: - Installation instructions - Data generation guide - Configuration options - Troubleshooting</p>"},{"location":"notebooks/#shared-data","title":"Shared Data","text":"<p>If you have access to the <code>loinc-predictor</code> project's Synthea data:</p> <pre><code>data_path = Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'all_cohorts'\n</code></pre> <p>This contains ~100 patients with realistic EHR data.</p>"},{"location":"notebooks/#test-fixtures","title":"Test Fixtures","text":"<p>For quick testing without full Synthea setup:</p> <pre><code>data_path = Path('../../tests/fixtures/mock_synthea_data')\n</code></pre> <p>Small sample dataset for unit tests and quick exploration.</p>"},{"location":"notebooks/#common-tasks","title":"Common Tasks","text":""},{"location":"notebooks/#loading-data","title":"Loading Data","text":"<pre><code>from ehrsequencing.data.adapters import SyntheaAdapter\n\nadapter = SyntheaAdapter(data_path='/path/to/synthea')\npatients = adapter.load_patients(limit=50)\nevents = adapter.load_events(patient_ids=[p.patient_id for p in patients])\n</code></pre>"},{"location":"notebooks/#grouping-visits","title":"Grouping Visits","text":"<pre><code>from ehrsequencing.data.visit_grouper import VisitGrouper\n\ngrouper = VisitGrouper(strategy='hybrid', time_window_hours=24)\npatient_visits = {}\nfor patient_id in patient_ids:\n    events = adapter.load_events(patient_ids=[patient_id])\n    visits = grouper.group_events(events, patient_id=patient_id)\n    patient_visits[patient_id] = visits\n</code></pre>"},{"location":"notebooks/#building-sequences","title":"Building Sequences","text":"<pre><code>from ehrsequencing.data.sequence_builder import PatientSequenceBuilder\n\nbuilder = PatientSequenceBuilder(\n    max_visits=50,\n    max_codes_per_visit=100,\n    use_semantic_order=True\n)\n\nvocab = builder.build_vocabulary(patient_visits, min_frequency=1)\nsequences = builder.build_sequences(patient_visits, min_visits=2)\n</code></pre>"},{"location":"notebooks/#encoding-for-models","title":"Encoding for Models","text":"<pre><code>encoded = builder.encode_sequence(sequences[0], return_tensors=True)\n# Returns dict with:\n#   - visit_codes: [num_visits, max_codes_per_visit]\n#   - visit_mask: [num_visits, max_codes_per_visit]\n#   - sequence_mask: [num_visits]\n#   - time_deltas: [num_visits-1]\n</code></pre>"},{"location":"notebooks/#tips-for-using-notebooks","title":"Tips for Using Notebooks","text":""},{"location":"notebooks/#performance","title":"Performance","text":"<ul> <li>Start small: Use <code>limit=50</code> when loading patients</li> <li>Profile memory: Monitor memory usage for large datasets</li> <li>Clear outputs: Clear cell outputs before committing to git</li> </ul>"},{"location":"notebooks/#experimentation","title":"Experimentation","text":"<ul> <li>Duplicate cells: Copy cells to try different parameters</li> <li>Add markdown: Document your findings inline</li> <li>Save checkpoints: Save processed data to avoid recomputation</li> </ul>"},{"location":"notebooks/#debugging","title":"Debugging","text":"<ul> <li>Print shapes: Always print tensor/array shapes</li> <li>Visualize data: Plot distributions and samples</li> <li>Check masks: Verify padding masks are correct</li> <li>Use small batches: Easier to inspect and debug</li> </ul>"},{"location":"notebooks/#contributing","title":"Contributing","text":"<p>When adding new notebooks:</p> <ol> <li>Create a topic directory: <code>notebooks/XX_topic_name/</code></li> <li>Follow naming convention: <code>XX_main_topic.ipynb</code>, <code>XXa_subtopic.ipynb</code></li> <li>Add README: Explain purpose and contents</li> <li>Include documentation: Add <code>.md</code> files for complex topics</li> <li>Test thoroughly: Ensure notebooks run end-to-end</li> <li>Clear outputs: Before committing to git</li> </ol>"},{"location":"notebooks/#related-resources","title":"Related Resources","text":""},{"location":"notebooks/#documentation_1","title":"Documentation","text":"<ul> <li><code>docs/datasets/</code> - Dataset setup and documentation</li> <li><code>docs/methods/</code> - Methodological details</li> <li><code>README.md</code> - Project overview</li> </ul>"},{"location":"notebooks/#source-code","title":"Source Code","text":"<ul> <li><code>src/ehrsequencing/data/</code> - Data loading and processing</li> <li><code>src/ehrsequencing/models/</code> - Model implementations</li> <li><code>src/ehrsequencing/training/</code> - Training utilities</li> </ul>"},{"location":"notebooks/#examples","title":"Examples","text":"<ul> <li><code>examples/train_lstm_baseline.py</code> - LSTM training script</li> <li><code>examples/</code> - Production-ready scripts</li> </ul>"},{"location":"notebooks/#questions","title":"Questions?","text":"<ul> <li>Check notebook markdown cells for inline documentation</li> <li>See <code>data_shape_transformations.md</code> for shape reference</li> <li>Review source code for implementation details</li> <li>Refer to <code>docs/</code> for methodological background</li> </ul>"},{"location":"notebooks/01_synthea_data_exploration/","title":"Synthea Data Exploration","text":"<p>Comprehensive exploration of Synthea synthetic EHR data and preparation for LSTM baseline modeling.</p>"},{"location":"notebooks/01_synthea_data_exploration/#overview","title":"Overview","text":"<p>This directory contains notebooks and documentation for: 1. Exploring Synthea data - Understanding the structure and content of synthetic EHR data 2. Building patient sequences - Grouping events into visits and creating temporal sequences 3. Preparing LSTM inputs - Encoding sequences for neural network models 4. Understanding data transformations - Tracking shape changes through the pipeline</p>"},{"location":"notebooks/01_synthea_data_exploration/#contents","title":"Contents","text":""},{"location":"notebooks/01_synthea_data_exploration/#notebooks","title":"Notebooks","text":""},{"location":"notebooks/01_synthea_data_exploration/#01_synthea_data_explorationipynb","title":"<code>01_synthea_data_exploration.ipynb</code>","text":"<p>Purpose: Comprehensive exploration of Synthea data and the complete data processing pipeline.</p> <p>What you'll learn: - How to load Synthea CSV files using <code>SyntheaAdapter</code> - Patient demographics and clinical data distributions - Event types (conditions, observations, medications, procedures) - Visit grouping strategies and their effects - Patient sequence construction - Data quality assessment - Temporal patterns in EHR data</p> <p>Key outputs: - Patient demographics summary - Code frequency distributions - Visit statistics (visits per patient, codes per visit, time between visits) - Sequence length distributions - Data quality metrics</p> <p>Runtime: ~5-10 minutes with 50 patients</p>"},{"location":"notebooks/01_synthea_data_exploration/#01a_lstm_data_preparationipynb","title":"<code>01a_lstm_data_preparation.ipynb</code>","text":"<p>Purpose: Detailed walkthrough of preparing visit-grouped sequences for the LSTM baseline model.</p> <p>What you'll learn: - How to encode string codes to integer IDs - Vocabulary building from patient data - Padding and masking strategies - Creating labels for prediction tasks - Batching sequences with variable lengths - Data shape transformations at each step - Running LSTM model forward pass</p> <p>Prediction task: Binary classification - predicting diabetes diagnosis from EHR sequences</p> <p>Key outputs: - Encoded sequences with proper padding - Labeled dataset (diabetes vs. no diabetes) - Batched PyTorch tensors ready for training - Model predictions on sample batch - Complete pipeline visualization</p> <p>Runtime: ~3-5 minutes with 50 patients</p>"},{"location":"notebooks/01_synthea_data_exploration/#documentation","title":"Documentation","text":""},{"location":"notebooks/01_synthea_data_exploration/#data_shape_transformationsmd","title":"<code>data_shape_transformations.md</code>","text":"<p>Purpose: Comprehensive reference for all data shape transformations in the pipeline.</p> <p>Contents: - Stage-by-stage transformation details - Shape specifications for each data structure - Memory footprint calculations - Common pitfalls and how to avoid them - Code examples for each transformation</p> <p>Use this when: - Debugging shape mismatch errors - Understanding memory requirements - Implementing custom data processing - Optimizing batch sizes</p>"},{"location":"notebooks/01_synthea_data_exploration/#getting-started","title":"Getting Started","text":""},{"location":"notebooks/01_synthea_data_exploration/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Activate environment: <pre><code>mamba activate ehrsequencing\n</code></pre></p> </li> <li> <p>Set up data:</p> </li> </ol> <p>Option A - Use shared data (recommended): <pre><code>data_path = Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'all_cohorts'\n</code></pre></p> <p>Option B - Generate your own:    See <code>../../docs/datasets/SYNTHEA_SETUP.md</code> for instructions</p> <p>Option C - Use test fixtures: <pre><code>data_path = Path('../../tests/fixtures/mock_synthea_data')\n</code></pre></p> <ol> <li>Launch Jupyter: <pre><code>cd notebooks/01_synthea_data_exploration\njupyter lab\n</code></pre></li> </ol>"},{"location":"notebooks/01_synthea_data_exploration/#recommended-order","title":"Recommended Order","text":"<ol> <li>Start here: <code>01_synthea_data_exploration.ipynb</code></li> <li>Run all cells to see the complete pipeline</li> <li>Experiment with different parameters</li> <li> <p>Understand the data structure</p> </li> <li> <p>Deep dive: <code>01a_lstm_data_preparation.ipynb</code></p> </li> <li>Focus on model input preparation</li> <li>Understand encoding and batching</li> <li> <p>See shape transformations in detail</p> </li> <li> <p>Reference: <code>data_shape_transformations.md</code></p> </li> <li>Consult when debugging</li> <li>Use as shape specification reference</li> <li>Review memory considerations</li> </ol>"},{"location":"notebooks/01_synthea_data_exploration/#key-concepts","title":"Key Concepts","text":""},{"location":"notebooks/01_synthea_data_exploration/#visit-grouping","title":"Visit Grouping","text":"<p>Problem: Raw EHR data consists of individual events (one code at a time). We need to group related events into clinical visits.</p> <p>Solution: <code>VisitGrouper</code> with multiple strategies: - <code>'encounter'</code>: Group by encounter ID (when available) - <code>'same_day'</code>: Group events on the same calendar day - <code>'time_window'</code>: Group events within N hours - <code>'hybrid'</code>: Use encounter IDs when available, fallback to time-based grouping</p> <p>Example: <pre><code>grouper = VisitGrouper(strategy='hybrid', time_window_hours=24)\nvisits = grouper.group_events(events, patient_id=patient_id)\n</code></pre></p>"},{"location":"notebooks/01_synthea_data_exploration/#patient-sequences","title":"Patient Sequences","text":"<p>Problem: Visits need to be organized into patient-level temporal sequences for modeling.</p> <p>Solution: <code>PatientSequenceBuilder</code> creates structured sequences with: - Chronologically ordered visits - Vocabulary mapping (code \u2192 integer ID) - Configurable sequence length limits - Semantic code ordering within visits</p> <p>Example: <pre><code>builder = PatientSequenceBuilder(max_visits=50, max_codes_per_visit=100)\nvocab = builder.build_vocabulary(patient_visits)\nsequences = builder.build_sequences(patient_visits, min_visits=2)\n</code></pre></p>"},{"location":"notebooks/01_synthea_data_exploration/#encoding-padding","title":"Encoding &amp; Padding","text":"<p>Problem: Neural networks require fixed-size inputs, but EHR sequences have variable lengths.</p> <p>Solution: Encode codes to IDs and pad to fixed dimensions: - Convert string codes \u2192 integer IDs via vocabulary - Pad visits to <code>max_codes_per_visit</code> - Pad sequences to <code>max_visits</code> - Create masks to track real vs. padded data</p> <p>Example: <pre><code>encoded = builder.encode_sequence(sequence, return_tensors=True)\n# Returns:\n#   visit_codes: [num_visits, max_codes_per_visit]\n#   visit_mask: [num_visits, max_codes_per_visit]\n#   sequence_mask: [num_visits]\n</code></pre></p>"},{"location":"notebooks/01_synthea_data_exploration/#batching","title":"Batching","text":"<p>Problem: Training requires batching multiple sequences together.</p> <p>Solution: <code>collate_fn</code> dynamically pads batch to max lengths: - Find max visits and max codes in current batch - Pad all sequences to these dimensions - Create batch-level masks - Return PyTorch tensors</p> <p>Example: <pre><code>batch = collate_fn(dataset_items[:32])\n# Returns:\n#   visit_codes: [32, max_visits_in_batch, max_codes_in_batch]\n#   visit_mask: [32, max_visits_in_batch, max_codes_in_batch]\n#   sequence_mask: [32, max_visits_in_batch]\n#   labels: [32, 1]\n</code></pre></p>"},{"location":"notebooks/01_synthea_data_exploration/#data-pipeline-summary","title":"Data Pipeline Summary","text":"<pre><code>Raw Synthea CSV Files\n    \u2193 SyntheaAdapter.load_events()\nList[MedicalEvent] - Individual timestamped codes\n    \u2193 VisitGrouper.group_events()\nDict[patient_id, List[Visit]] - Events grouped into visits\n    \u2193 PatientSequenceBuilder.build_sequences()\nList[PatientSequence] - Structured patient sequences\n    \u2193 PatientSequenceBuilder.encode_sequence()\nEncoded sequences with padding/masking\n    \u2193 Add labels + collate_fn()\nBatched PyTorch tensors\n    \u2193 LSTM Model\nPredictions\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/#prediction-task-diabetes-detection","title":"Prediction Task: Diabetes Detection","text":"<p>Both notebooks use diabetes detection as the example prediction task:</p> <p>Task type: Binary classification</p> <p>Label definition:  - <code>1</code> if patient has any diabetes diagnosis code in their history - <code>0</code> otherwise</p> <p>Diabetes codes (SNOMED-CT): - <code>44054006</code> - Type 2 diabetes mellitus - <code>46635009</code> - Type 1 diabetes mellitus - <code>73211009</code> - Diabetes mellitus - <code>11687002</code> - Gestational diabetes - <code>190330002</code> - Diabetes mellitus without complication - <code>190331003</code> - Diabetes mellitus with complication</p> <p>Why diabetes? - Common condition (~10-20% prevalence in typical Synthea data) - Clear diagnostic codes - Clinically meaningful - Good balance for binary classification</p> <p>Other possible tasks: - Readmission prediction (time-based) - Mortality prediction (death_date) - Disease onset prediction (temporal) - Multi-disease phenotyping (multi-class) - Risk score prediction (regression)</p>"},{"location":"notebooks/01_synthea_data_exploration/#common-parameters","title":"Common Parameters","text":""},{"location":"notebooks/01_synthea_data_exploration/#data-loading","title":"Data Loading","text":"<pre><code># Number of patients to load\nlimit = 50  # Start small for exploration\n\n# Patient filtering\npatient_ids = [p.patient_id for p in patients[:10]]\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/#visit-grouping_1","title":"Visit Grouping","text":"<pre><code>strategy = 'hybrid'           # 'encounter', 'same_day', 'time_window', 'hybrid'\ntime_window_hours = 24        # For time-based grouping\npreserve_code_types = True    # Keep semantic structure\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/#sequence-building","title":"Sequence Building","text":"<pre><code>max_visits = 50               # Maximum visits per sequence\nmax_codes_per_visit = 100     # Maximum codes per visit\nmin_visits = 2                # Minimum visits to include patient\nuse_semantic_order = True     # Order codes by type\nmin_frequency = 1             # Minimum code frequency for vocabulary\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/#model-configuration","title":"Model Configuration","text":"<pre><code>vocab_size = len(vocab)       # Determined by data\nembedding_dim = 128           # Code embedding dimension\nhidden_dim = 256              # LSTM hidden dimension\nnum_layers = 1                # Number of LSTM layers\nbatch_size = 32               # Training batch size\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"notebooks/01_synthea_data_exploration/#issue-keyerror-when-loading-data","title":"Issue: KeyError when loading data","text":"<p>Cause: CSV column names don't match expected format</p> <p>Solution: Check that you're using Synthea data (not MIMIC or other formats)</p>"},{"location":"notebooks/01_synthea_data_exploration/#issue-attributeerror-on-visit-or-patientsequence","title":"Issue: AttributeError on Visit or PatientSequence","text":"<p>Cause: Treating dataclass as dictionary</p> <p>Solution: Use attribute access (<code>.patient_id</code>) not dict access (<code>['patient_id']</code>)</p>"},{"location":"notebooks/01_synthea_data_exploration/#issue-shape-mismatch-in-model-forward-pass","title":"Issue: Shape mismatch in model forward pass","text":"<p>Cause: Incorrect tensor dimensions or missing masks</p> <p>Solution: Check <code>data_shape_transformations.md</code> for expected shapes</p>"},{"location":"notebooks/01_synthea_data_exploration/#issue-out-of-memory","title":"Issue: Out of memory","text":"<p>Cause: Too many patients or too large dimensions</p> <p>Solution:  - Reduce <code>limit</code> when loading patients - Decrease <code>max_visits</code> or <code>max_codes_per_visit</code> - Use smaller <code>batch_size</code></p>"},{"location":"notebooks/01_synthea_data_exploration/#issue-empty-sequences-after-filtering","title":"Issue: Empty sequences after filtering","text":"<p>Cause: <code>min_visits</code> threshold too high</p> <p>Solution: Lower <code>min_visits</code> or load more patients</p>"},{"location":"notebooks/01_synthea_data_exploration/#performance-tips","title":"Performance Tips","text":""},{"location":"notebooks/01_synthea_data_exploration/#for-exploration-notebooks","title":"For Exploration (Notebooks)","text":"<ul> <li>Use <code>limit=50</code> patients for fast iteration</li> <li>Clear cell outputs before committing</li> <li>Restart kernel if memory grows large</li> </ul>"},{"location":"notebooks/01_synthea_data_exploration/#for-training-production","title":"For Training (Production)","text":"<ul> <li>Use full dataset (no limit)</li> <li>Increase batch size for efficiency</li> <li>Use GPU if available</li> <li>Enable mixed precision (fp16)</li> </ul>"},{"location":"notebooks/01_synthea_data_exploration/#memory-estimates","title":"Memory Estimates","text":""},{"location":"notebooks/01_synthea_data_exploration/#per-patient-avg","title":"Per Patient (avg)","text":"<ul> <li>Events: ~100 events \u00d7 200 bytes = 20 KB</li> <li>Visits: ~10 visits \u00d7 500 bytes = 5 KB</li> <li>Sequence: ~50 KB (encoded)</li> <li>Total: ~75 KB per patient</li> </ul>"},{"location":"notebooks/01_synthea_data_exploration/#per-batch-batch_size32","title":"Per Batch (batch_size=32)","text":"<ul> <li>Tensors: ~1.5 MB</li> <li>Gradients: ~3 MB</li> <li>Total: ~5 MB per batch</li> </ul>"},{"location":"notebooks/01_synthea_data_exploration/#full-dataset","title":"Full Dataset","text":"Patients Memory Training Time 100 ~10 MB Minutes 1,000 ~100 MB Hours 10,000 ~1 GB Days"},{"location":"notebooks/01_synthea_data_exploration/#next-steps","title":"Next Steps","text":"<p>After completing these notebooks:</p> <ol> <li> <p>Run production training: <pre><code>python examples/train_lstm_baseline.py \\\n    --data_dir /path/to/synthea \\\n    --max_patients 1000 \\\n    --num_epochs 10 \\\n    --output_dir ./outputs\n</code></pre></p> </li> <li> <p>Experiment with models:</p> </li> <li>Try different LSTM configurations</li> <li>Add attention mechanisms</li> <li>Use bidirectional LSTMs</li> <li> <p>Experiment with different aggregation strategies</p> </li> <li> <p>Try different tasks:</p> </li> <li>Readmission prediction</li> <li>Mortality prediction</li> <li>Multi-disease phenotyping</li> <li> <p>Next visit prediction</p> </li> <li> <p>Explore advanced methods:</p> </li> <li>Med2Vec embeddings</li> <li>Transformer models</li> <li>Graph neural networks</li> </ol>"},{"location":"notebooks/01_synthea_data_exploration/#related-resources","title":"Related Resources","text":"<ul> <li><code>../../docs/datasets/SYNTHEA_SETUP.md</code> - Synthea data generation</li> <li><code>../../examples/train_lstm_baseline.py</code> - Production training script</li> <li><code>../../src/ehrsequencing/models/lstm_baseline.py</code> - Model implementation</li> <li><code>../../src/ehrsequencing/data/</code> - Data processing modules</li> </ul>"},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/","title":"01 synthea data exploration","text":"In\u00a0[1]: Copied! <pre>import sys\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom datetime import datetime, timedelta\n\n# Set plotting style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Import ehrsequencing package\nfrom ehrsequencing.data.adapters import SyntheaAdapter\nfrom ehrsequencing.data.visit_grouper import VisitGrouper\nfrom ehrsequencing.data.sequence_builder import PatientSequenceBuilder\n\nprint(\"\u2705 Imports successful\")\n</pre> import sys from pathlib import Path import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from collections import Counter from datetime import datetime, timedelta  # Set plotting style sns.set_style('whitegrid') plt.rcParams['figure.figsize'] = (12, 6)  # Import ehrsequencing package from ehrsequencing.data.adapters import SyntheaAdapter from ehrsequencing.data.visit_grouper import VisitGrouper from ehrsequencing.data.sequence_builder import PatientSequenceBuilder  print(\"\u2705 Imports successful\") <pre>\u2705 Imports successful\n</pre> In\u00a0[2]: Copied! <pre># Path to Synthea data\n# Option 1: Use your own Synthea data\n# data_path = Path('../../data/synthea')\ndata_path = Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'all_cohorts'\nprint(data_path)\n\n# Option 2: Use test fixtures (small sample)\n# data_path = Path('../../tests/fixtures/mock_synthea_data')\n\n# Initialize adapter\nadapter = SyntheaAdapter(data_path=str(data_path))\n\nprint(f\"\u2705 Loaded Synthea data from: {data_path}\")\nprint(f\"   Data directory exists: {data_path.exists()}\")\n</pre> # Path to Synthea data # Option 1: Use your own Synthea data # data_path = Path('../../data/synthea') data_path = Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'all_cohorts' print(data_path)  # Option 2: Use test fixtures (small sample) # data_path = Path('../../tests/fixtures/mock_synthea_data')  # Initialize adapter adapter = SyntheaAdapter(data_path=str(data_path))  print(f\"\u2705 Loaded Synthea data from: {data_path}\") print(f\"   Data directory exists: {data_path.exists()}\") <pre>/Users/pleiadian53/work/loinc-predictor/data/synthea/all_cohorts\n\u2705 Loaded Synthea data from: /Users/pleiadian53/work/loinc-predictor/data/synthea/all_cohorts\n   Data directory exists: True\n</pre> In\u00a0[3]: Copied! <pre># List available CSV files\ncsv_files = list(data_path.glob('*.csv'))\n\nprint(f\"Found {len(csv_files)} CSV files:\")\ntotal_size = 0\nfor f in sorted(csv_files):\n    size_mb = f.stat().st_size / 1024 / 1024\n    print(f\"  - {f.name:30s} ({size_mb:.2f} MB)\")\n    total_size += size_mb\nprint(f\"Total size: {total_size:.2f} MB\")\n</pre> # List available CSV files csv_files = list(data_path.glob('*.csv'))  print(f\"Found {len(csv_files)} CSV files:\") total_size = 0 for f in sorted(csv_files):     size_mb = f.stat().st_size / 1024 / 1024     print(f\"  - {f.name:30s} ({size_mb:.2f} MB)\")     total_size += size_mb print(f\"Total size: {total_size:.2f} MB\") <pre>Found 18 CSV files:\n  - allergies.csv                  (0.01 MB)\n  - careplans.csv                  (0.07 MB)\n  - claims.csv                     (3.07 MB)\n  - claims_transactions.csv        (33.52 MB)\n  - conditions.csv                 (0.45 MB)\n  - devices.csv                    (0.11 MB)\n  - encounters.csv                 (1.60 MB)\n  - imaging_studies.csv            (6.22 MB)\n  - immunizations.csv              (0.21 MB)\n  - medications.csv                (0.84 MB)\n  - observations.csv               (9.30 MB)\n  - organizations.csv              (0.04 MB)\n  - patients.csv                   (0.03 MB)\n  - payer_transitions.csv          (0.58 MB)\n  - payers.csv                     (0.00 MB)\n  - procedures.csv                 (2.86 MB)\n  - providers.csv                  (0.04 MB)\n  - supplies.csv                   (0.29 MB)\nTotal size: 59.24 MB\n</pre> In\u00a0[4]: Copied! <pre># Load patients\npatients = adapter.load_patients(limit=100)  # Load first 100 for exploration\n\nprint(f\"Total patients loaded: {len(patients)}\")\nprint(f\"\\nPatient object attributes:\")\nif patients:\n    sample_patient = patients[0]\n    print(f\"  - patient_id: {sample_patient.patient_id}\")\n    print(f\"  - birth_date: {sample_patient.birth_date}\")\n    print(f\"  - gender: {sample_patient.gender}\")\n    print(f\"  - race: {sample_patient.race}\")\n    print(f\"  - death_date: {sample_patient.death_date}\")\n    print(f\"  - metadata: {sample_patient.metadata}\")\n\n    print(f\"  - data type: {type(sample_patient)}\")\n\n# Convert to DataFrame for analysis\nimport pandas as pd\npatients_df = pd.DataFrame([\n    {\n        'patient_id': p.patient_id,\n        'birth_date': p.birth_date,\n        'gender': p.gender,\n        'race': p.race,\n        'death_date': p.death_date,\n        'city': p.metadata.get('city'),\n        'state': p.metadata.get('state'),\n        'ethnicity': p.metadata.get('ethnicity')\n    }\n    for p in patients\n])\n\nprint(f\"\\nDataFrame shape: {patients_df.shape}\")\npatients_df.head()\n</pre> # Load patients patients = adapter.load_patients(limit=100)  # Load first 100 for exploration  print(f\"Total patients loaded: {len(patients)}\") print(f\"\\nPatient object attributes:\") if patients:     sample_patient = patients[0]     print(f\"  - patient_id: {sample_patient.patient_id}\")     print(f\"  - birth_date: {sample_patient.birth_date}\")     print(f\"  - gender: {sample_patient.gender}\")     print(f\"  - race: {sample_patient.race}\")     print(f\"  - death_date: {sample_patient.death_date}\")     print(f\"  - metadata: {sample_patient.metadata}\")      print(f\"  - data type: {type(sample_patient)}\")  # Convert to DataFrame for analysis import pandas as pd patients_df = pd.DataFrame([     {         'patient_id': p.patient_id,         'birth_date': p.birth_date,         'gender': p.gender,         'race': p.race,         'death_date': p.death_date,         'city': p.metadata.get('city'),         'state': p.metadata.get('state'),         'ethnicity': p.metadata.get('ethnicity')     }     for p in patients ])  print(f\"\\nDataFrame shape: {patients_df.shape}\") patients_df.head() <pre>Total patients loaded: 100\n\nPatient object attributes:\n  - patient_id: 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0\n  - birth_date: 2024-05-05 00:00:00\n  - gender: F\n  - race: white\n  - death_date: None\n  - metadata: {'ethnicity': 'nonhispanic', 'city': 'Wilmington', 'state': 'Massachusetts'}\n  - data type: &lt;class 'ehrsequencing.data.adapters.base.PatientInfo'&gt;\n\nDataFrame shape: (100, 8)\n</pre> Out[4]: patient_id birth_date gender race death_date city state ethnicity 0 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 2024-05-05 F white NaT Wilmington Massachusetts nonhispanic 1 8f30f356-f7c7-4947-9b72-d61d60a56ac1 2021-04-08 M white NaT Billerica Massachusetts nonhispanic 2 dd6e30e4-69d6-42d1-9536-d1f43c506b15 2016-10-26 F white NaT Pittsfield Massachusetts hispanic 3 916785c0-9e53-09e5-f13c-69b0196f06fe 1995-09-14 M black NaT Boston Massachusetts nonhispanic 4 313d3648-02b3-7c0d-a381-47160df32a9d 2001-07-31 M white NaT Leicester Massachusetts nonhispanic In\u00a0[5]: Copied! <pre># Patient demographics summary\nprint(\"Patient Demographics Summary:\")\nprint(\"=\" * 50)\n\nprint(f\"\\nGender distribution:\")\nprint(patients_df['gender'].value_counts())\n\nprint(f\"\\nRace distribution:\")\nprint(patients_df['race'].value_counts())\n\nprint(f\"\\nState distribution:\")\nprint(patients_df['state'].value_counts())\n\n# Calculate age\npatients_df['birth_date'] = pd.to_datetime(patients_df['birth_date'])\npatients_df['age'] = (pd.Timestamp.now() - patients_df['birth_date']).dt.days / 365.25\n\nprint(f\"\\nAge statistics:\")\nprint(patients_df['age'].describe())\n</pre> # Patient demographics summary print(\"Patient Demographics Summary:\") print(\"=\" * 50)  print(f\"\\nGender distribution:\") print(patients_df['gender'].value_counts())  print(f\"\\nRace distribution:\") print(patients_df['race'].value_counts())  print(f\"\\nState distribution:\") print(patients_df['state'].value_counts())  # Calculate age patients_df['birth_date'] = pd.to_datetime(patients_df['birth_date']) patients_df['age'] = (pd.Timestamp.now() - patients_df['birth_date']).dt.days / 365.25  print(f\"\\nAge statistics:\") print(patients_df['age'].describe()) <pre>Patient Demographics Summary:\n==================================================\n\nGender distribution:\ngender\nF    58\nM    42\nName: count, dtype: int64\n\nRace distribution:\nrace\nwhite       85\nblack        8\nasian        4\nhawaiian     1\nother        1\nnative       1\nName: count, dtype: int64\n\nState distribution:\nstate\nMassachusetts    100\nName: count, dtype: int64\n\nAge statistics:\ncount    100.000000\nmean      36.336318\nstd       20.761935\nmin        1.308693\n25%       18.500342\n50%       36.986995\n75%       51.917180\nmax       86.086242\nName: age, dtype: float64\n</pre> In\u00a0[6]: Copied! <pre># Load encounters using raw pandas (adapter returns structured objects)\n# For exploration, we'll load CSV directly\nencounters_df = pd.read_csv(data_path / 'encounters.csv')\n\nprint(f\"Total encounters: {len(encounters_df)}\")\nprint(f\"Unique patients with encounters: {encounters_df['PATIENT'].nunique()}\")\nprint(f\"\\nEncounter types:\")\nif 'ENCOUNTERCLASS' in encounters_df.columns:\n    print(encounters_df['ENCOUNTERCLASS'].value_counts())\n\nprint(f\"\\nColumns: {list(encounters_df.columns)}\")\nencounters_df.head()\n</pre> # Load encounters using raw pandas (adapter returns structured objects) # For exploration, we'll load CSV directly encounters_df = pd.read_csv(data_path / 'encounters.csv')  print(f\"Total encounters: {len(encounters_df)}\") print(f\"Unique patients with encounters: {encounters_df['PATIENT'].nunique()}\") print(f\"\\nEncounter types:\") if 'ENCOUNTERCLASS' in encounters_df.columns:     print(encounters_df['ENCOUNTERCLASS'].value_counts())  print(f\"\\nColumns: {list(encounters_df.columns)}\") encounters_df.head() <pre>Total encounters: 5015\nUnique patients with encounters: 106\n\nEncounter types:\nENCOUNTERCLASS\nambulatory    2496\nwellness      1201\noutpatient     844\nemergency      217\nurgentcare     134\ninpatient       81\nvirtual         17\nsnf             14\nhospice         11\nName: count, dtype: int64\n\nColumns: ['Id', 'START', 'STOP', 'PATIENT', 'ORGANIZATION', 'PROVIDER', 'PAYER', 'ENCOUNTERCLASS', 'CODE', 'DESCRIPTION', 'BASE_ENCOUNTER_COST', 'TOTAL_CLAIM_COST', 'PAYER_COVERAGE', 'REASONCODE', 'REASONDESCRIPTION']\n</pre> Out[6]: Id START STOP PATIENT ORGANIZATION PROVIDER PAYER ENCOUNTERCLASS CODE DESCRIPTION BASE_ENCOUNTER_COST TOTAL_CLAIM_COST PAYER_COVERAGE REASONCODE REASONDESCRIPTION 0 9b2bcf62-ebf2-0ee3-8c94-6250d4153fd2 2024-05-05T18:38:29Z 2024-05-05T18:53:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 411f14a5-de79-3e08-81d6-301270cc2275 e00d3da0-cd4b-35a4-ae2f-ab2bf4a6e989 d31fccc3-1767-390d-966a-22a5156f4219 wellness 410620009 Well child visit (procedure) 136.8 347.38 0.00 NaN NaN 1 9b2bcf62-ebf2-0ee3-aed8-65055b81f25e 2024-06-09T18:38:29Z 2024-06-09T18:53:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 411f14a5-de79-3e08-81d6-301270cc2275 e00d3da0-cd4b-35a4-ae2f-ab2bf4a6e989 d31fccc3-1767-390d-966a-22a5156f4219 wellness 410620009 Well child visit (procedure) 136.8 272.80 0.00 NaN NaN 2 9b2bcf62-ebf2-0ee3-6a12-5d8a52c9c6f4 2024-08-11T18:38:29Z 2024-08-11T18:53:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 411f14a5-de79-3e08-81d6-301270cc2275 e00d3da0-cd4b-35a4-ae2f-ab2bf4a6e989 d31fccc3-1767-390d-966a-22a5156f4219 wellness 410620009 Well child visit (procedure) 136.8 1679.60 673.42 NaN NaN 3 9b2bcf62-ebf2-0ee3-1468-067524470bf1 2024-10-13T18:38:29Z 2024-10-13T18:53:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 411f14a5-de79-3e08-81d6-301270cc2275 e00d3da0-cd4b-35a4-ae2f-ab2bf4a6e989 d31fccc3-1767-390d-966a-22a5156f4219 wellness 410620009 Well child visit (procedure) 136.8 1437.12 1149.70 NaN NaN 4 9b2bcf62-ebf2-0ee3-f8e6-36425d47a319 2025-01-12T18:38:29Z 2025-01-12T18:53:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 411f14a5-de79-3e08-81d6-301270cc2275 e00d3da0-cd4b-35a4-ae2f-ab2bf4a6e989 d31fccc3-1767-390d-966a-22a5156f4219 wellness 410620009 Well child visit (procedure) 136.8 816.80 653.44 NaN NaN In\u00a0[7]: Copied! <pre># Load conditions\nconditions_df = pd.read_csv(data_path / 'conditions.csv')\n\nprint(f\"Total condition records: {len(conditions_df)}\")\nprint(f\"Unique condition codes: {conditions_df['CODE'].nunique() if len(conditions_df) &gt; 0 else 0}\")\nprint(f\"\\nMost common conditions:\")\nif len(conditions_df) &gt; 0:\n    top_conditions = conditions_df['DESCRIPTION'].value_counts().head(10)\n    for condition, count in top_conditions.items():\n        print(f\"  {condition:50s} {count:5d}\")\n\nconditions_df.head()\n</pre> # Load conditions conditions_df = pd.read_csv(data_path / 'conditions.csv')  print(f\"Total condition records: {len(conditions_df)}\") print(f\"Unique condition codes: {conditions_df['CODE'].nunique() if len(conditions_df) &gt; 0 else 0}\") print(f\"\\nMost common conditions:\") if len(conditions_df) &gt; 0:     top_conditions = conditions_df['DESCRIPTION'].value_counts().head(10)     for condition, count in top_conditions.items():         print(f\"  {condition:50s} {count:5d}\")  conditions_df.head() <pre>Total condition records: 3240\nUnique condition codes: 184\n\nMost common conditions:\n  Medication review due (situation)                    613\n  Stress (finding)                                     239\n  Gingivitis (disorder)                                237\n  Full-time employment (finding)                       217\n  Part-time employment (finding)                       138\n  Viral sinusitis (disorder)                           102\n  Social isolation (finding)                            84\n  Limited social contact (finding)                      76\n  Not in labor force (finding)                          74\n  Acute viral pharyngitis (disorder)                    68\n</pre> Out[7]: START STOP PATIENT ENCOUNTER SYSTEM CODE DESCRIPTION 0 2024-05-05 2024-08-11 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-8c94-6250d4153fd2 SNOMED-CT 314529007 Medication review due (situation) 1 2024-10-13 2024-10-13 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-1468-067524470bf1 SNOMED-CT 314529007 Medication review due (situation) 2 2025-01-12 2025-04-13 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-f8e6-36425d47a319 SNOMED-CT 314529007 Medication review due (situation) 3 2025-07-13 2025-10-12 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-7e48-ab988a95443c SNOMED-CT 314529007 Medication review due (situation) 4 2021-04-08 2021-04-08 8f30f356-f7c7-4947-9b72-d61d60a56ac1 8f30f356-f7c7-4947-9b40-be053883bd6c SNOMED-CT 314529007 Medication review due (situation) In\u00a0[8]: Copied! <pre># Load observations (labs)\nobservations_df = pd.read_csv(data_path / 'observations.csv')\n\nprint(f\"Total observation records: {len(observations_df)}\")\nprint(f\"Unique observation codes: {observations_df['CODE'].nunique() if len(observations_df) &gt; 0 else 0}\")\nprint(f\"\\nMost common observations:\")\nif len(observations_df) &gt; 0:\n    top_obs = observations_df['DESCRIPTION'].value_counts().head(10)\n    for obs, count in top_obs.items():\n        print(f\"  {obs:50s} {count:5d}\")\n\nobservations_df.head()\n</pre> # Load observations (labs) observations_df = pd.read_csv(data_path / 'observations.csv')  print(f\"Total observation records: {len(observations_df)}\") print(f\"Unique observation codes: {observations_df['CODE'].nunique() if len(observations_df) &gt; 0 else 0}\") print(f\"\\nMost common observations:\") if len(observations_df) &gt; 0:     top_obs = observations_df['DESCRIPTION'].value_counts().head(10)     for obs, count in top_obs.items():         print(f\"  {obs:50s} {count:5d}\")  observations_df.head() <pre>Total observation records: 56225\nUnique observation codes: 223\n\nMost common observations:\n  Pain severity - 0-10 verbal numeric rating [Score] - Reported  1783\n  Systolic Blood Pressure                             1273\n  Diastolic Blood Pressure                            1273\n  Body Weight                                         1254\n  Respiratory rate                                    1216\n  Heart rate                                          1216\n  Tobacco smoking status                              1187\n  Body Height                                         1187\n  Body mass index (BMI) [Ratio]                       1057\n  QOLS                                                 957\n</pre> Out[8]: DATE PATIENT ENCOUNTER CATEGORY CODE DESCRIPTION VALUE UNITS TYPE 0 2024-05-05T18:38:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-8c94-6250d4153fd2 vital-signs 8302-2 Body Height 54.5 cm numeric 1 2024-05-05T18:38:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-8c94-6250d4153fd2 vital-signs 72514-3 Pain severity - 0-10 verbal numeric rating [Sc... 4.0 {score} numeric 2 2024-05-05T18:38:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-8c94-6250d4153fd2 vital-signs 29463-7 Body Weight 4.2 kg numeric 3 2024-05-05T18:38:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-8c94-6250d4153fd2 vital-signs 77606-2 Weight-for-length Per age and sex 10.3 % numeric 4 2024-05-05T18:38:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-8c94-6250d4153fd2 vital-signs 8289-1 Head Occipital-frontal circumference Percentile 89.2 % numeric In\u00a0[9]: Copied! <pre># Load medications\nmedications_df = pd.read_csv(data_path / 'medications.csv')\n\nprint(f\"Total medication records: {len(medications_df)}\")\nprint(f\"Unique medication codes: {medications_df['CODE'].nunique() if len(medications_df) &gt; 0 else 0}\")\nprint(f\"\\nMost common medications:\")\nif len(medications_df) &gt; 0:\n    top_meds = medications_df['DESCRIPTION'].value_counts().head(10)\n    for med, count in top_meds.items():\n        print(f\"  {med:50s} {count:5d}\")\n\nmedications_df.head()\n</pre> # Load medications medications_df = pd.read_csv(data_path / 'medications.csv')  print(f\"Total medication records: {len(medications_df)}\") print(f\"Unique medication codes: {medications_df['CODE'].nunique() if len(medications_df) &gt; 0 else 0}\") print(f\"\\nMost common medications:\") if len(medications_df) &gt; 0:     top_meds = medications_df['DESCRIPTION'].value_counts().head(10)     for med, count in top_meds.items():         print(f\"  {med:50s} {count:5d}\")  medications_df.head() <pre>Total medication records: 3384\nUnique medication codes: 132\n\nMost common medications:\n  1 ML Epoetin Alfa 4000 UNT/ML Injection [Epogen]     348\n  Hydrochlorothiazide 25 MG Oral Tablet                329\n  lisinopril 10 MG Oral Tablet                         287\n  amLODIPine 2.5 MG Oral Tablet                        277\n  insulin isophane  human 70 UNT/ML / insulin  regular  human 30 UNT/ML Injectable Suspension [Humulin]   236\n  sodium fluoride 0.0272 MG/MG Oral Gel                228\n  Cisplatin 50 MG Injection                            197\n  PACLitaxel 100 MG Injection                          197\n  Acetaminophen 300 MG / Hydrocodone Bitartrate 5 MG Oral Tablet    78\n  10 ML Furosemide 10 MG/ML Injection                   68\n</pre> Out[9]: START STOP PATIENT PAYER ENCOUNTER CODE DESCRIPTION BASE_COST PAYER_COVERAGE DISPENSES TOTALCOST REASONCODE REASONDESCRIPTION 0 2022-03-04T07:19:52Z 2022-03-18T07:19:52Z 8f30f356-f7c7-4947-9b72-d61d60a56ac1 0133f751-9229-3cfd-815f-b6d4979bdd6a 8f30f356-f7c7-4947-0f8f-99d5db7e02a6 308192 Amoxicillin 500 MG Oral Tablet 44.67 0.0 1 44.67 NaN NaN 1 2022-03-04T07:19:52Z 2022-03-18T07:19:52Z 8f30f356-f7c7-4947-9b72-d61d60a56ac1 0133f751-9229-3cfd-815f-b6d4979bdd6a 8f30f356-f7c7-4947-0f8f-99d5db7e02a6 313820 Acetaminophen 160 MG Chewable Tablet 45.86 0.0 1 45.86 NaN NaN 2 2022-05-02T07:43:04Z 2022-05-29T07:43:04Z 8f30f356-f7c7-4947-9b72-d61d60a56ac1 0133f751-9229-3cfd-815f-b6d4979bdd6a 8f30f356-f7c7-4947-f77b-de0972cf3c07 198405 Ibuprofen 100 MG Oral Tablet 51.35 0.0 1 51.35 NaN NaN 3 2024-03-21T12:50:17Z 2024-03-21T12:50:17Z 8f30f356-f7c7-4947-9b72-d61d60a56ac1 0133f751-9229-3cfd-815f-b6d4979bdd6a 8f30f356-f7c7-4947-d7e0-36ca42b6527b 1535362 sodium fluoride 0.0272 MG/MG Oral Gel 129.94 0.0 1 129.94 103697008.0 Patient referral for dental care (procedure) 4 2024-07-19T07:43:04Z 2024-08-24T07:43:04Z 8f30f356-f7c7-4947-9b72-d61d60a56ac1 0133f751-9229-3cfd-815f-b6d4979bdd6a 8f30f356-f7c7-4947-8c30-4888a42c681d 313820 Acetaminophen 160 MG Chewable Tablet 45.86 0.0 1 45.86 NaN NaN In\u00a0[10]: Copied! <pre># Load procedures\nprocedures_df = pd.read_csv(data_path / 'procedures.csv')\n\nprint(f\"Total procedure records: {len(procedures_df)}\")\nprint(f\"Unique procedure codes: {procedures_df['CODE'].nunique() if len(procedures_df) &gt; 0 else 0}\")\nprint(f\"\\nMost common procedures:\")\nif len(procedures_df) &gt; 0:\n    top_procs = procedures_df['DESCRIPTION'].value_counts().head(10)\n    for proc, count in top_procs.items():\n        print(f\"  {proc:50s} {count:5d}\")\n\nprocedures_df.head()\n</pre> # Load procedures procedures_df = pd.read_csv(data_path / 'procedures.csv')  print(f\"Total procedure records: {len(procedures_df)}\") print(f\"Unique procedure codes: {procedures_df['CODE'].nunique() if len(procedures_df) &gt; 0 else 0}\") print(f\"\\nMost common procedures:\") if len(procedures_df) &gt; 0:     top_procs = procedures_df['DESCRIPTION'].value_counts().head(10)     for proc, count in top_procs.items():         print(f\"  {proc:50s} {count:5d}\")  procedures_df.head() <pre>Total procedure records: 14145\nUnique procedure codes: 232\n\nMost common procedures:\n  Depression screening (procedure)                    1372\n  Assessment of health and social care needs (procedure)   798\n  Medication reconciliation (procedure)                586\n  Assessment of substance use (procedure)              529\n  Dental consultation and report (procedure)           504\n  Patient referral for dental care (procedure)         496\n  Oral health education (procedure)                    490\n  Dental care (regime/therapy)                         485\n  Removal of subgingival plaque and calculus from all teeth using dental instrument (procedure)   485\n  Examination of gingivae (procedure)                  485\n</pre> Out[10]: START STOP PATIENT ENCOUNTER SYSTEM CODE DESCRIPTION BASE_COST REASONCODE REASONDESCRIPTION 0 2024-08-11T18:38:29Z 2024-08-11T18:53:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-6a12-5d8a52c9c6f4 SNOMED-CT 430193006 Medication reconciliation (procedure) 862.80 NaN NaN 1 2024-10-13T18:38:29Z 2024-10-13T18:53:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-1468-067524470bf1 SNOMED-CT 430193006 Medication reconciliation (procedure) 620.32 NaN NaN 2 2025-04-13T18:38:29Z 2025-04-13T18:53:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-e33f-3bf6ca237d80 SNOMED-CT 430193006 Medication reconciliation (procedure) 540.97 NaN NaN 3 2025-10-12T18:38:29Z 2025-10-12T18:53:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-8659-c64b2406802e SNOMED-CT 430193006 Medication reconciliation (procedure) 413.95 NaN NaN 4 2021-04-08T07:19:52Z 2021-04-08T07:34:52Z 8f30f356-f7c7-4947-9b72-d61d60a56ac1 8f30f356-f7c7-4947-9b40-be053883bd6c SNOMED-CT 430193006 Medication reconciliation (procedure) 862.80 NaN NaN In\u00a0[11]: Copied! <pre># Load medications (directly from CSV for exploration)\nmedications_df = pd.read_csv(data_path / 'medications.csv')\n\nprint(f\"Total medication records: {len(medications_df)}\")\nprint(f\"Unique medication codes: {medications_df['CODE'].nunique() if len(medications_df) &gt; 0 else 0}\")\nprint(f\"\\nMost common medications:\")\nif len(medications_df) &gt; 0:\n    top_meds = medications_df['DESCRIPTION'].value_counts().head(10)\n    for med, count in top_meds.items():\n        print(f\"  {med:50s} {count:5d}\")\n\nmedications_df.head()\n</pre> # Load medications (directly from CSV for exploration) medications_df = pd.read_csv(data_path / 'medications.csv')  print(f\"Total medication records: {len(medications_df)}\") print(f\"Unique medication codes: {medications_df['CODE'].nunique() if len(medications_df) &gt; 0 else 0}\") print(f\"\\nMost common medications:\") if len(medications_df) &gt; 0:     top_meds = medications_df['DESCRIPTION'].value_counts().head(10)     for med, count in top_meds.items():         print(f\"  {med:50s} {count:5d}\")  medications_df.head() <pre>Total medication records: 3384\nUnique medication codes: 132\n\nMost common medications:\n  1 ML Epoetin Alfa 4000 UNT/ML Injection [Epogen]     348\n  Hydrochlorothiazide 25 MG Oral Tablet                329\n  lisinopril 10 MG Oral Tablet                         287\n  amLODIPine 2.5 MG Oral Tablet                        277\n  insulin isophane  human 70 UNT/ML / insulin  regular  human 30 UNT/ML Injectable Suspension [Humulin]   236\n  sodium fluoride 0.0272 MG/MG Oral Gel                228\n  Cisplatin 50 MG Injection                            197\n  PACLitaxel 100 MG Injection                          197\n  Acetaminophen 300 MG / Hydrocodone Bitartrate 5 MG Oral Tablet    78\n  10 ML Furosemide 10 MG/ML Injection                   68\n</pre> Out[11]: START STOP PATIENT PAYER ENCOUNTER CODE DESCRIPTION BASE_COST PAYER_COVERAGE DISPENSES TOTALCOST REASONCODE REASONDESCRIPTION 0 2022-03-04T07:19:52Z 2022-03-18T07:19:52Z 8f30f356-f7c7-4947-9b72-d61d60a56ac1 0133f751-9229-3cfd-815f-b6d4979bdd6a 8f30f356-f7c7-4947-0f8f-99d5db7e02a6 308192 Amoxicillin 500 MG Oral Tablet 44.67 0.0 1 44.67 NaN NaN 1 2022-03-04T07:19:52Z 2022-03-18T07:19:52Z 8f30f356-f7c7-4947-9b72-d61d60a56ac1 0133f751-9229-3cfd-815f-b6d4979bdd6a 8f30f356-f7c7-4947-0f8f-99d5db7e02a6 313820 Acetaminophen 160 MG Chewable Tablet 45.86 0.0 1 45.86 NaN NaN 2 2022-05-02T07:43:04Z 2022-05-29T07:43:04Z 8f30f356-f7c7-4947-9b72-d61d60a56ac1 0133f751-9229-3cfd-815f-b6d4979bdd6a 8f30f356-f7c7-4947-f77b-de0972cf3c07 198405 Ibuprofen 100 MG Oral Tablet 51.35 0.0 1 51.35 NaN NaN 3 2024-03-21T12:50:17Z 2024-03-21T12:50:17Z 8f30f356-f7c7-4947-9b72-d61d60a56ac1 0133f751-9229-3cfd-815f-b6d4979bdd6a 8f30f356-f7c7-4947-d7e0-36ca42b6527b 1535362 sodium fluoride 0.0272 MG/MG Oral Gel 129.94 0.0 1 129.94 103697008.0 Patient referral for dental care (procedure) 4 2024-07-19T07:43:04Z 2024-08-24T07:43:04Z 8f30f356-f7c7-4947-9b72-d61d60a56ac1 0133f751-9229-3cfd-815f-b6d4979bdd6a 8f30f356-f7c7-4947-8c30-4888a42c681d 313820 Acetaminophen 160 MG Chewable Tablet 45.86 0.0 1 45.86 NaN NaN In\u00a0[13]: Copied! <pre># Load procedures\n# procedures_df = adapter.load_procedures()\n\n# Load procedures (directly from CSV for exploration)\nprocedures_df= pd.read_csv(data_path / 'procedures.csv')\n\nprint(f\"Total procedure records: {len(procedures_df)}\")\nprint(f\"Unique procedure codes: {procedures_df['CODE'].nunique() if len(procedures_df) &gt; 0 else 0}\")\n\nprocedures_df.head()\n</pre> # Load procedures # procedures_df = adapter.load_procedures()  # Load procedures (directly from CSV for exploration) procedures_df= pd.read_csv(data_path / 'procedures.csv')  print(f\"Total procedure records: {len(procedures_df)}\") print(f\"Unique procedure codes: {procedures_df['CODE'].nunique() if len(procedures_df) &gt; 0 else 0}\")  procedures_df.head() <pre>Total procedure records: 14145\nUnique procedure codes: 232\n</pre> Out[13]: START STOP PATIENT ENCOUNTER SYSTEM CODE DESCRIPTION BASE_COST REASONCODE REASONDESCRIPTION 0 2024-08-11T18:38:29Z 2024-08-11T18:53:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-6a12-5d8a52c9c6f4 SNOMED-CT 430193006 Medication reconciliation (procedure) 862.80 NaN NaN 1 2024-10-13T18:38:29Z 2024-10-13T18:53:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-1468-067524470bf1 SNOMED-CT 430193006 Medication reconciliation (procedure) 620.32 NaN NaN 2 2025-04-13T18:38:29Z 2025-04-13T18:53:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-e33f-3bf6ca237d80 SNOMED-CT 430193006 Medication reconciliation (procedure) 540.97 NaN NaN 3 2025-10-12T18:38:29Z 2025-10-12T18:53:29Z 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0 9b2bcf62-ebf2-0ee3-8659-c64b2406802e SNOMED-CT 430193006 Medication reconciliation (procedure) 413.95 NaN NaN 4 2021-04-08T07:19:52Z 2021-04-08T07:34:52Z 8f30f356-f7c7-4947-9b72-d61d60a56ac1 8f30f356-f7c7-4947-9b40-be053883bd6c SNOMED-CT 430193006 Medication reconciliation (procedure) 862.80 NaN NaN In\u00a0[14]: Copied! <pre># Summary statistics\nsummary = {\n    'Metric': [],\n    'Count': []\n}\n\nsummary['Metric'].extend([\n    'Total Patients',\n    'Total Encounters',\n    'Total Conditions',\n    'Total Observations',\n    'Total Medications',\n    'Total Procedures',\n    'Unique Condition Codes',\n    'Unique Observation Codes',\n    'Unique Medication Codes',\n    'Unique Procedure Codes'\n])\n\nsummary['Count'].extend([\n    len(patients_df),\n    len(encounters_df),\n    len(conditions_df),\n    len(observations_df),\n    len(medications_df),\n    len(procedures_df),\n    conditions_df['CODE'].nunique() if len(conditions_df) &gt; 0 else 0,\n    observations_df['CODE'].nunique() if len(observations_df) &gt; 0 else 0,\n    medications_df['CODE'].nunique() if len(medications_df) &gt; 0 else 0,\n    procedures_df['CODE'].nunique() if len(procedures_df) &gt; 0 else 0\n])\n\nsummary_df = pd.DataFrame(summary)\nprint(\"\\n\" + \"=\"*60)\nprint(\"DATA SUMMARY\")\nprint(\"=\"*60)\nprint(summary_df.to_string(index=False))\nprint(\"=\"*60)\n</pre> # Summary statistics summary = {     'Metric': [],     'Count': [] }  summary['Metric'].extend([     'Total Patients',     'Total Encounters',     'Total Conditions',     'Total Observations',     'Total Medications',     'Total Procedures',     'Unique Condition Codes',     'Unique Observation Codes',     'Unique Medication Codes',     'Unique Procedure Codes' ])  summary['Count'].extend([     len(patients_df),     len(encounters_df),     len(conditions_df),     len(observations_df),     len(medications_df),     len(procedures_df),     conditions_df['CODE'].nunique() if len(conditions_df) &gt; 0 else 0,     observations_df['CODE'].nunique() if len(observations_df) &gt; 0 else 0,     medications_df['CODE'].nunique() if len(medications_df) &gt; 0 else 0,     procedures_df['CODE'].nunique() if len(procedures_df) &gt; 0 else 0 ])  summary_df = pd.DataFrame(summary) print(\"\\n\" + \"=\"*60) print(\"DATA SUMMARY\") print(\"=\"*60) print(summary_df.to_string(index=False)) print(\"=\"*60) <pre>\n============================================================\nDATA SUMMARY\n============================================================\n                  Metric  Count\n          Total Patients    100\n        Total Encounters   5015\n        Total Conditions   3240\n      Total Observations  56225\n       Total Medications   3384\n        Total Procedures  14145\n  Unique Condition Codes    184\nUnique Observation Codes    223\n Unique Medication Codes    132\n  Unique Procedure Codes    232\n============================================================\n</pre> In\u00a0[15]: Copied! <pre># Encounters per patient\nencounters_per_patient = encounters_df.groupby('PATIENT').size()\n\nprint(\"Encounters per patient statistics:\")\nprint(encounters_per_patient.describe())\n\n# Plot distribution\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogram\naxes[0].hist(encounters_per_patient, bins=30, edgecolor='black', alpha=0.7)\naxes[0].set_xlabel('Number of Encounters')\naxes[0].set_ylabel('Number of Patients')\naxes[0].set_title('Distribution of Encounters per Patient')\naxes[0].axvline(encounters_per_patient.median(), color='red', linestyle='--', label=f'Median: {encounters_per_patient.median():.0f}')\naxes[0].legend()\n\n# Box plot\naxes[1].boxplot(encounters_per_patient, vert=True)\naxes[1].set_ylabel('Number of Encounters')\naxes[1].set_title('Encounters per Patient (Box Plot)')\n\nplt.tight_layout()\nplt.show()\n</pre> # Encounters per patient encounters_per_patient = encounters_df.groupby('PATIENT').size()  print(\"Encounters per patient statistics:\") print(encounters_per_patient.describe())  # Plot distribution fig, axes = plt.subplots(1, 2, figsize=(14, 5))  # Histogram axes[0].hist(encounters_per_patient, bins=30, edgecolor='black', alpha=0.7) axes[0].set_xlabel('Number of Encounters') axes[0].set_ylabel('Number of Patients') axes[0].set_title('Distribution of Encounters per Patient') axes[0].axvline(encounters_per_patient.median(), color='red', linestyle='--', label=f'Median: {encounters_per_patient.median():.0f}') axes[0].legend()  # Box plot axes[1].boxplot(encounters_per_patient, vert=True) axes[1].set_ylabel('Number of Encounters') axes[1].set_title('Encounters per Patient (Box Plot)')  plt.tight_layout() plt.show() <pre>Encounters per patient statistics:\ncount    106.000000\nmean      47.311321\nstd       55.889408\nmin        8.000000\n25%       23.250000\n50%       33.500000\n75%       50.000000\nmax      505.000000\ndtype: float64\n</pre> In\u00a0[16]: Copied! <pre># Combine all medical codes\nall_codes = []\n\nif len(conditions_df) &gt; 0:\n    all_codes.extend([('CONDITION', code) for code in conditions_df['CODE']])\nif len(observations_df) &gt; 0:\n    all_codes.extend([('OBSERVATION', code) for code in observations_df['CODE']])\nif len(medications_df) &gt; 0:\n    all_codes.extend([('MEDICATION', code) for code in medications_df['CODE']])\nif len(procedures_df) &gt; 0:\n    all_codes.extend([('PROCEDURE', code) for code in procedures_df['CODE']])\n\n# Code type distribution\ncode_types = [code_type for code_type, _ in all_codes]\ncode_type_counts = Counter(code_types)\n\nprint(\"Code type distribution:\")\nfor code_type, count in code_type_counts.items():\n    print(f\"  {code_type:15s}: {count:6d} ({count/len(all_codes)*100:.1f}%)\")\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.bar(code_type_counts.keys(), code_type_counts.values(), alpha=0.7, edgecolor='black')\nplt.xlabel('Code Type')\nplt.ylabel('Count')\nplt.title('Distribution of Medical Code Types')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n</pre> # Combine all medical codes all_codes = []  if len(conditions_df) &gt; 0:     all_codes.extend([('CONDITION', code) for code in conditions_df['CODE']]) if len(observations_df) &gt; 0:     all_codes.extend([('OBSERVATION', code) for code in observations_df['CODE']]) if len(medications_df) &gt; 0:     all_codes.extend([('MEDICATION', code) for code in medications_df['CODE']]) if len(procedures_df) &gt; 0:     all_codes.extend([('PROCEDURE', code) for code in procedures_df['CODE']])  # Code type distribution code_types = [code_type for code_type, _ in all_codes] code_type_counts = Counter(code_types)  print(\"Code type distribution:\") for code_type, count in code_type_counts.items():     print(f\"  {code_type:15s}: {count:6d} ({count/len(all_codes)*100:.1f}%)\")  # Plot plt.figure(figsize=(10, 6)) plt.bar(code_type_counts.keys(), code_type_counts.values(), alpha=0.7, edgecolor='black') plt.xlabel('Code Type') plt.ylabel('Count') plt.title('Distribution of Medical Code Types') plt.xticks(rotation=45) plt.tight_layout() plt.show() <pre>Code type distribution:\n  CONDITION      :   3240 (4.2%)\n  OBSERVATION    :  56225 (73.0%)\n  MEDICATION     :   3384 (4.4%)\n  PROCEDURE      :  14145 (18.4%)\n</pre> In\u00a0[17]: Copied! <pre># Initialize visit grouper\nvisit_grouper = VisitGrouper(\n    strategy='hybrid',  # Use encounter IDs when available, fallback to same_day\n    time_window_hours=24,\n    preserve_code_types=True\n)\n\nprint(\"\u2705 VisitGrouper initialized\")\nprint(f\"   Strategy: hybrid (encounter IDs + same-day fallback)\")\nprint(f\"   Time window: 24 hours\")\nprint(f\"   Preserve code types: True\")\n</pre> # Initialize visit grouper visit_grouper = VisitGrouper(     strategy='hybrid',  # Use encounter IDs when available, fallback to same_day     time_window_hours=24,     preserve_code_types=True )  print(\"\u2705 VisitGrouper initialized\") print(f\"   Strategy: hybrid (encounter IDs + same-day fallback)\") print(f\"   Time window: 24 hours\") print(f\"   Preserve code types: True\") <pre>\u2705 VisitGrouper initialized\n   Strategy: hybrid (encounter IDs + same-day fallback)\n   Time window: 24 hours\n   Preserve code types: True\n</pre> In\u00a0[18]: Copied! <pre># Load events and group into visits for sample patients\npatient_ids = [p.patient_id for p in patients[:10]]  # First 10 patients\n\nprint(f\"Processing {len(patient_ids)} patients...\")\n\n# Group visits for each patient\npatient_visits = {}\nfor patient_id in patient_ids:\n    # Load events for this patient\n    events = adapter.load_events(patient_ids=[patient_id])\n    \n    # Group events into visits\n    visits = visit_grouper.group_events(events, patient_id=patient_id)\n    patient_visits[patient_id] = visits\n    \n    print(f\"  Patient {patient_id[:8]}... : {len(events)} events \u2192 {len(visits)} visits\")\n\nprint(f\"\\n\u2705 Grouped visits for {len(patient_visits)} patients\")\n</pre> # Load events and group into visits for sample patients patient_ids = [p.patient_id for p in patients[:10]]  # First 10 patients  print(f\"Processing {len(patient_ids)} patients...\")  # Group visits for each patient patient_visits = {} for patient_id in patient_ids:     # Load events for this patient     events = adapter.load_events(patient_ids=[patient_id])          # Group events into visits     visits = visit_grouper.group_events(events, patient_id=patient_id)     patient_visits[patient_id] = visits          print(f\"  Patient {patient_id[:8]}... : {len(events)} events \u2192 {len(visits)} visits\")  print(f\"\\n\u2705 Grouped visits for {len(patient_visits)} patients\") <pre>Processing 10 patients...\n  Patient 9b2bcf62... : 113 events \u2192 10 visits\n  Patient 8f30f356... : 231 events \u2192 24 visits\n  Patient dd6e30e4... : 335 events \u2192 39 visits\n  Patient 916785c0... : 302 events \u2192 26 visits\n  Patient 313d3648... : 277 events \u2192 58 visits\n  Patient fd8f9438... : 1315 events \u2192 59 visits\n  Patient e97c9d12... : 185 events \u2192 18 visits\n  Patient 075171df... : 616 events \u2192 38 visits\n  Patient 7deb0d52... : 579 events \u2192 55 visits\n  Patient 76e97910... : 419 events \u2192 94 visits\n\n\u2705 Grouped visits for 10 patients\n</pre> In\u00a0[20]: Copied! <pre># Pick a patient with visits\nsample_patient_id = None\nfor pid, visits in patient_visits.items():\n    if len(visits) &gt; 0:\n        sample_patient_id = pid\n        break\n\nif sample_patient_id:\n    sample_visits = patient_visits[sample_patient_id]\n    print(f\"Sample patient: {sample_patient_id}\")\n    print(f\"Total visits: {len(sample_visits)}\")\n    print(f\"dtype(sample_visits): {type(sample_visits)}\")\n    \n    if len(sample_visits) &gt; 0:\n        print(f\"\\nFirst visit details:\")\n        first_visit = sample_visits[0]\n        print(f\"  Visit ID: {first_visit.visit_id}\")\n        print(f\"  Timestamp: {first_visit.timestamp}\")\n        print(f\"  Encounter ID: {first_visit.encounter_id}\")\n        print(f\"  Number of codes: {first_visit.num_codes()}\")\n        print(f\"  Codes by type: {list(first_visit.codes_by_type.keys())}\")\n        \n        # Show codes by type\n        for code_type, codes in first_visit.codes_by_type.items():\n            print(f\"    {code_type}: {len(codes)} codes\")\n            if codes:\n                print(f\"      First 5: {codes[:5]}\")\nelse:\n    print(\"No patients with visits found in sample\")\n</pre> # Pick a patient with visits sample_patient_id = None for pid, visits in patient_visits.items():     if len(visits) &gt; 0:         sample_patient_id = pid         break  if sample_patient_id:     sample_visits = patient_visits[sample_patient_id]     print(f\"Sample patient: {sample_patient_id}\")     print(f\"Total visits: {len(sample_visits)}\")     print(f\"dtype(sample_visits): {type(sample_visits)}\")          if len(sample_visits) &gt; 0:         print(f\"\\nFirst visit details:\")         first_visit = sample_visits[0]         print(f\"  Visit ID: {first_visit.visit_id}\")         print(f\"  Timestamp: {first_visit.timestamp}\")         print(f\"  Encounter ID: {first_visit.encounter_id}\")         print(f\"  Number of codes: {first_visit.num_codes()}\")         print(f\"  Codes by type: {list(first_visit.codes_by_type.keys())}\")                  # Show codes by type         for code_type, codes in first_visit.codes_by_type.items():             print(f\"    {code_type}: {len(codes)} codes\")             if codes:                 print(f\"      First 5: {codes[:5]}\") else:     print(\"No patients with visits found in sample\") <pre>Sample patient: 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0\nTotal visits: 10\ndtype(sample_visits): &lt;class 'list'&gt;\n\nFirst visit details:\n  Visit ID: 9b2bcf62-ebf2-0ee3-8c94-6250d4153fd2\n  Timestamp: 2024-05-05 00:00:00\n  Encounter ID: 9b2bcf62-ebf2-0ee3-8c94-6250d4153fd2\n  Number of codes: 23\n  Codes by type: ['diagnosis', 'lab']\n    diagnosis: 1 codes\n      First 5: ['314529007']\n    lab: 22 codes\n      First 5: ['8302-2', '72514-3', '29463-7', '77606-2', '8289-1']\n</pre> In\u00a0[22]: Copied! <pre># Compute visit statistics\nvisit_stats = {\n    'visits_per_patient': [],\n    'codes_per_visit': [],\n    'time_between_visits': []  # in days\n}\n\nfor patient_id, visits in patient_visits.items():\n    visit_stats['visits_per_patient'].append(len(visits))\n    \n    for visit in visits:\n        visit_stats['codes_per_visit'].append(visit.num_codes())  # Use num_codes() method\n    \n    # Time between consecutive visits\n    if len(visits) &gt; 1:\n        for i in range(1, len(visits)):\n            time_diff = (visits[i].timestamp - visits[i-1].timestamp).total_seconds() / 86400\n            visit_stats['time_between_visits'].append(time_diff)\n\n# Print statistics\nprint(\"Visit Statistics:\")\nprint(\"=\" * 60)\n\nif visit_stats['visits_per_patient']:\n    print(f\"\\nVisits per patient:\")\n    print(f\"  Mean: {np.mean(visit_stats['visits_per_patient']):.2f}\")\n    print(f\"  Median: {np.median(visit_stats['visits_per_patient']):.2f}\")\n    print(f\"  Min: {np.min(visit_stats['visits_per_patient'])}\")\n    print(f\"  Max: {np.max(visit_stats['visits_per_patient'])}\")\n\nif visit_stats['codes_per_visit']:\n    print(f\"\\nCodes per visit:\")\n    print(f\"  Mean: {np.mean(visit_stats['codes_per_visit']):.2f}\")\n    print(f\"  Median: {np.median(visit_stats['codes_per_visit']):.2f}\")\n    print(f\"  Min: {np.min(visit_stats['codes_per_visit'])}\")\n    print(f\"  Max: {np.max(visit_stats['codes_per_visit'])}\")\n\nif visit_stats['time_between_visits']:\n    print(f\"\\nTime between visits (days):\")\n    print(f\"  Mean: {np.mean(visit_stats['time_between_visits']):.2f}\")\n    print(f\"  Median: {np.median(visit_stats['time_between_visits']):.2f}\")\n    print(f\"  Min: {np.min(visit_stats['time_between_visits']):.2f}\")\n    print(f\"  Max: {np.max(visit_stats['time_between_visits']):.2f}\")\n</pre> # Compute visit statistics visit_stats = {     'visits_per_patient': [],     'codes_per_visit': [],     'time_between_visits': []  # in days }  for patient_id, visits in patient_visits.items():     visit_stats['visits_per_patient'].append(len(visits))          for visit in visits:         visit_stats['codes_per_visit'].append(visit.num_codes())  # Use num_codes() method          # Time between consecutive visits     if len(visits) &gt; 1:         for i in range(1, len(visits)):             time_diff = (visits[i].timestamp - visits[i-1].timestamp).total_seconds() / 86400             visit_stats['time_between_visits'].append(time_diff)  # Print statistics print(\"Visit Statistics:\") print(\"=\" * 60)  if visit_stats['visits_per_patient']:     print(f\"\\nVisits per patient:\")     print(f\"  Mean: {np.mean(visit_stats['visits_per_patient']):.2f}\")     print(f\"  Median: {np.median(visit_stats['visits_per_patient']):.2f}\")     print(f\"  Min: {np.min(visit_stats['visits_per_patient'])}\")     print(f\"  Max: {np.max(visit_stats['visits_per_patient'])}\")  if visit_stats['codes_per_visit']:     print(f\"\\nCodes per visit:\")     print(f\"  Mean: {np.mean(visit_stats['codes_per_visit']):.2f}\")     print(f\"  Median: {np.median(visit_stats['codes_per_visit']):.2f}\")     print(f\"  Min: {np.min(visit_stats['codes_per_visit'])}\")     print(f\"  Max: {np.max(visit_stats['codes_per_visit'])}\")  if visit_stats['time_between_visits']:     print(f\"\\nTime between visits (days):\")     print(f\"  Mean: {np.mean(visit_stats['time_between_visits']):.2f}\")     print(f\"  Median: {np.median(visit_stats['time_between_visits']):.2f}\")     print(f\"  Min: {np.min(visit_stats['time_between_visits']):.2f}\")     print(f\"  Max: {np.max(visit_stats['time_between_visits']):.2f}\") <pre>Visit Statistics:\n============================================================\n\nVisits per patient:\n  Mean: 42.10\n  Median: 38.50\n  Min: 10\n  Max: 94\n\nCodes per visit:\n  Mean: 10.38\n  Median: 3.00\n  Min: 1\n  Max: 298\n\nTime between visits (days):\n  Mean: 114.30\n  Median: 28.00\n  Min: 0.00\n  Max: 4388.57\n</pre> In\u00a0[23]: Copied! <pre># Visualize visit statistics\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Visits per patient\nif visit_stats['visits_per_patient']:\n    axes[0, 0].hist(visit_stats['visits_per_patient'], bins=20, edgecolor='black', alpha=0.7)\n    axes[0, 0].set_xlabel('Number of Visits')\n    axes[0, 0].set_ylabel('Number of Patients')\n    axes[0, 0].set_title('Visits per Patient')\n\n# Codes per visit\nif visit_stats['codes_per_visit']:\n    axes[0, 1].hist(visit_stats['codes_per_visit'], bins=30, edgecolor='black', alpha=0.7)\n    axes[0, 1].set_xlabel('Number of Codes')\n    axes[0, 1].set_ylabel('Number of Visits')\n    axes[0, 1].set_title('Codes per Visit')\n\n# Time between visits (log scale)\nif visit_stats['time_between_visits']:\n    axes[1, 0].hist(np.log10(np.array(visit_stats['time_between_visits']) + 1), bins=30, edgecolor='black', alpha=0.7)\n    axes[1, 0].set_xlabel('Log10(Days + 1)')\n    axes[1, 0].set_ylabel('Frequency')\n    axes[1, 0].set_title('Time Between Consecutive Visits (Log Scale)')\n\n# Codes per visit (box plot)\nif visit_stats['codes_per_visit']:\n    axes[1, 1].boxplot(visit_stats['codes_per_visit'], vert=True)\n    axes[1, 1].set_ylabel('Number of Codes')\n    axes[1, 1].set_title('Codes per Visit (Box Plot)')\n\nplt.tight_layout()\nplt.show()\n</pre> # Visualize visit statistics fig, axes = plt.subplots(2, 2, figsize=(14, 10))  # Visits per patient if visit_stats['visits_per_patient']:     axes[0, 0].hist(visit_stats['visits_per_patient'], bins=20, edgecolor='black', alpha=0.7)     axes[0, 0].set_xlabel('Number of Visits')     axes[0, 0].set_ylabel('Number of Patients')     axes[0, 0].set_title('Visits per Patient')  # Codes per visit if visit_stats['codes_per_visit']:     axes[0, 1].hist(visit_stats['codes_per_visit'], bins=30, edgecolor='black', alpha=0.7)     axes[0, 1].set_xlabel('Number of Codes')     axes[0, 1].set_ylabel('Number of Visits')     axes[0, 1].set_title('Codes per Visit')  # Time between visits (log scale) if visit_stats['time_between_visits']:     axes[1, 0].hist(np.log10(np.array(visit_stats['time_between_visits']) + 1), bins=30, edgecolor='black', alpha=0.7)     axes[1, 0].set_xlabel('Log10(Days + 1)')     axes[1, 0].set_ylabel('Frequency')     axes[1, 0].set_title('Time Between Consecutive Visits (Log Scale)')  # Codes per visit (box plot) if visit_stats['codes_per_visit']:     axes[1, 1].boxplot(visit_stats['codes_per_visit'], vert=True)     axes[1, 1].set_ylabel('Number of Codes')     axes[1, 1].set_title('Codes per Visit (Box Plot)')  plt.tight_layout() plt.show() In\u00a0[25]: Copied! <pre># Initialize sequence builder\n# Note: PatientSequenceBuilder works with already-grouped visits\n# We'll build vocabulary first, then create sequences\nsequence_builder = PatientSequenceBuilder(\n    vocab=None,  # Will build vocabulary from data\n    max_visits=50,\n    max_codes_per_visit=100,\n    use_semantic_order=True\n)\n\nprint(\"\u2705 PatientSequenceBuilder initialized\")\nprint(f\"   Max visits: 50\")\nprint(f\"   Max codes per visit: 100\")\nprint(f\"   Use semantic order: True\")\n</pre> # Initialize sequence builder # Note: PatientSequenceBuilder works with already-grouped visits # We'll build vocabulary first, then create sequences sequence_builder = PatientSequenceBuilder(     vocab=None,  # Will build vocabulary from data     max_visits=50,     max_codes_per_visit=100,     use_semantic_order=True )  print(\"\u2705 PatientSequenceBuilder initialized\") print(f\"   Max visits: 50\") print(f\"   Max codes per visit: 100\") print(f\"   Use semantic order: True\") <pre>\u2705 PatientSequenceBuilder initialized\n   Max visits: 50\n   Max codes per visit: 100\n   Use semantic order: True\n</pre> In\u00a0[27]: Copied! <pre># Build vocabulary from patient visits\nprint(\"Building vocabulary from patient data...\")\nsequence_builder.build_vocabulary(patient_visits, min_frequency=1)\n\nvocab_size = len(sequence_builder.vocab)\nprint(f\"\\n\u2705 Vocabulary built\")\nprint(f\"   Vocabulary size: {vocab_size}\")\nprint(f\"   Special tokens: [PAD]=0, [UNK]=1, [MASK]=2, [CLS]=3, [SEP]=4\")\n</pre> # Build vocabulary from patient visits print(\"Building vocabulary from patient data...\") sequence_builder.build_vocabulary(patient_visits, min_frequency=1)  vocab_size = len(sequence_builder.vocab) print(f\"\\n\u2705 Vocabulary built\") print(f\"   Vocabulary size: {vocab_size}\") print(f\"   Special tokens: [PAD]=0, [UNK]=1, [MASK]=2, [CLS]=3, [SEP]=4\") <pre>Building vocabulary from patient data...\n\n\u2705 Vocabulary built\n   Vocabulary size: 337\n   Special tokens: [PAD]=0, [UNK]=1, [MASK]=2, [CLS]=3, [SEP]=4\n</pre> In\u00a0[29]: Copied! <pre># Build sequences from patient visits\nprint(\"Building patient sequences...\")\nsequences = sequence_builder.build_sequences(patient_visits, min_visits=2)\n\nprint(f\"\\n\u2705 Built {len(sequences)} sequences\")\nprint(f\"   Total patients: {len(patient_visits)}\")\nprint(f\"   Filtered out: {len(patient_visits) - len(sequences)} patients (&lt; min_visits)\")\n\n# Show sequence statistics\nif sequences:\n    seq_lengths = [seq.sequence_length for seq in sequences]\n    print(f\"\\nSequence length statistics:\")\n    print(f\"   Mean: {np.mean(seq_lengths):.2f}\")\n    print(f\"   Median: {np.median(seq_lengths):.2f}\")\n    print(f\"   Min: {np.min(seq_lengths)}\")\n    print(f\"   Max: {np.max(seq_lengths)}\")\n</pre> # Build sequences from patient visits print(\"Building patient sequences...\") sequences = sequence_builder.build_sequences(patient_visits, min_visits=2)  print(f\"\\n\u2705 Built {len(sequences)} sequences\") print(f\"   Total patients: {len(patient_visits)}\") print(f\"   Filtered out: {len(patient_visits) - len(sequences)} patients (&lt; min_visits)\")  # Show sequence statistics if sequences:     seq_lengths = [seq.sequence_length for seq in sequences]     print(f\"\\nSequence length statistics:\")     print(f\"   Mean: {np.mean(seq_lengths):.2f}\")     print(f\"   Median: {np.median(seq_lengths):.2f}\")     print(f\"   Min: {np.min(seq_lengths)}\")     print(f\"   Max: {np.max(seq_lengths)}\") <pre>Building patient sequences...\n\n\u2705 Built 10 sequences\n   Total patients: 10\n   Filtered out: 0 patients (&lt; min_visits)\n\nSequence length statistics:\n   Mean: 42.10\n   Median: 38.50\n   Min: 10\n   Max: 94\n</pre> In\u00a0[31]: Copied! <pre>if sequences:\n    sample_seq = sequences[0]\n    \n    print(\"Sample sequence:\")\n    print(f\"  Patient ID: {sample_seq.patient_id}\")\n    print(f\"  Number of visits: {sample_seq.sequence_length}\")\n    print(f\"  Metadata: {sample_seq.metadata}\")\n    \n    print(f\"\\nFirst visit:\")\n    first_visit = sample_seq.visits[0]\n    print(f\"  Visit ID: {first_visit.visit_id}\")\n    print(f\"  Timestamp: {first_visit.timestamp}\")\n    print(f\"  Number of codes: {first_visit.num_codes()}\")\n    print(f\"  Codes by type: {list(first_visit.codes_by_type.keys())}\")\n    \n    # Show first few codes\n    all_codes = first_visit.get_all_codes()\n    print(f\"  All codes (first 10): {all_codes[:10]}\")\nelse:\n    print(\"No sequences built\")\n</pre> if sequences:     sample_seq = sequences[0]          print(\"Sample sequence:\")     print(f\"  Patient ID: {sample_seq.patient_id}\")     print(f\"  Number of visits: {sample_seq.sequence_length}\")     print(f\"  Metadata: {sample_seq.metadata}\")          print(f\"\\nFirst visit:\")     first_visit = sample_seq.visits[0]     print(f\"  Visit ID: {first_visit.visit_id}\")     print(f\"  Timestamp: {first_visit.timestamp}\")     print(f\"  Number of codes: {first_visit.num_codes()}\")     print(f\"  Codes by type: {list(first_visit.codes_by_type.keys())}\")          # Show first few codes     all_codes = first_visit.get_all_codes()     print(f\"  All codes (first 10): {all_codes[:10]}\") else:     print(\"No sequences built\") <pre>Sample sequence:\n  Patient ID: 9b2bcf62-ebf2-0ee3-d063-682358e3c0d0\n  Number of visits: 10\n  Metadata: {'total_codes': 113, 'avg_codes_per_visit': 11.3}\n\nFirst visit:\n  Visit ID: 9b2bcf62-ebf2-0ee3-8c94-6250d4153fd2\n  Timestamp: 2024-05-05 00:00:00\n  Number of codes: 23\n  Codes by type: ['diagnosis', 'lab']\n  All codes (first 10): ['314529007', '8302-2', '72514-3', '29463-7', '77606-2', '8289-1', '9843-4', '8462-4', '8480-6', '8867-4']\n</pre> In\u00a0[33]: Copied! <pre>if sequences:\n    sequence_lengths = [seq.sequence_length for seq in sequences]\n    \n    print(\"Sequence length statistics:\")\n    print(f\"  Mean: {np.mean(sequence_lengths):.2f}\")\n    print(f\"  Median: {np.median(sequence_lengths):.2f}\")\n    print(f\"  Min: {np.min(sequence_lengths)}\")\n    print(f\"  Max: {np.max(sequence_lengths)}\")\n    print(f\"  Std: {np.std(sequence_lengths):.2f}\")\n    \n    # Plot\n    plt.figure(figsize=(12, 5))\n    plt.hist(sequence_lengths, bins=20, edgecolor='black', alpha=0.7)\n    plt.xlabel('Sequence Length (Number of Visits)')\n    plt.ylabel('Number of Patients')\n    plt.title('Distribution of Sequence Lengths')\n    plt.axvline(np.median(sequence_lengths), color='red', linestyle='--', label=f'Median: {np.median(sequence_lengths):.0f}')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n</pre> if sequences:     sequence_lengths = [seq.sequence_length for seq in sequences]          print(\"Sequence length statistics:\")     print(f\"  Mean: {np.mean(sequence_lengths):.2f}\")     print(f\"  Median: {np.median(sequence_lengths):.2f}\")     print(f\"  Min: {np.min(sequence_lengths)}\")     print(f\"  Max: {np.max(sequence_lengths)}\")     print(f\"  Std: {np.std(sequence_lengths):.2f}\")          # Plot     plt.figure(figsize=(12, 5))     plt.hist(sequence_lengths, bins=20, edgecolor='black', alpha=0.7)     plt.xlabel('Sequence Length (Number of Visits)')     plt.ylabel('Number of Patients')     plt.title('Distribution of Sequence Lengths')     plt.axvline(np.median(sequence_lengths), color='red', linestyle='--', label=f'Median: {np.median(sequence_lengths):.0f}')     plt.legend()     plt.tight_layout()     plt.show() <pre>Sequence length statistics:\n  Mean: 42.10\n  Median: 38.50\n  Min: 10\n  Max: 94\n  Std: 23.71\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"Data Quality Assessment:\")\nprint(\"=\" * 60)\n\n# Missing data\nprint(\"\\n1. Missing Data:\")\nfor df_name, df in [\n    ('Patients', patients_df),\n    ('Encounters', encounters_df),\n    ('Conditions', conditions_df),\n    ('Observations', observations_df),\n    ('Medications', medications_df),\n    ('Procedures', procedures_df)\n]:\n    if len(df) &gt; 0:\n        missing_pct = (df.isnull().sum() / len(df) * 100)\n        if missing_pct.sum() &gt; 0:\n            print(f\"\\n  {df_name}:\")\n            for col, pct in missing_pct[missing_pct &gt; 0].items():\n                print(f\"    {col}: {pct:.1f}% missing\")\n\n# Patients with no encounters\npatients_with_encounters = set(encounters_df['PATIENT'].unique())\npatients_without_encounters = set(patients_df['patient_id']) - patients_with_encounters\nprint(f\"\\n2. Patients without encounters: {len(patients_without_encounters)} ({len(patients_without_encounters)/len(patients_df)*100:.1f}%)\")\n\n# Sequence statistics\nif sequences:\n    total_visits = sum(seq.sequence_length for seq in sequences)\n    total_codes = sum(visit.num_codes() for seq in sequences for visit in seq.visits)\n    print(f\"\\n3. Sequence coverage:\")\n    print(f\"   Total sequences: {len(sequences)}\")\n    print(f\"   Total visits: {total_visits}\")\n    print(f\"   Total codes: {total_codes}\")\n    print(f\"   Vocabulary size: {len(sequence_builder.vocab)}\")\n\nprint(\"\\n\" + \"=\" * 60)\n</pre> print(\"Data Quality Assessment:\") print(\"=\" * 60)  # Missing data print(\"\\n1. Missing Data:\") for df_name, df in [     ('Patients', patients_df),     ('Encounters', encounters_df),     ('Conditions', conditions_df),     ('Observations', observations_df),     ('Medications', medications_df),     ('Procedures', procedures_df) ]:     if len(df) &gt; 0:         missing_pct = (df.isnull().sum() / len(df) * 100)         if missing_pct.sum() &gt; 0:             print(f\"\\n  {df_name}:\")             for col, pct in missing_pct[missing_pct &gt; 0].items():                 print(f\"    {col}: {pct:.1f}% missing\")  # Patients with no encounters patients_with_encounters = set(encounters_df['PATIENT'].unique()) patients_without_encounters = set(patients_df['patient_id']) - patients_with_encounters print(f\"\\n2. Patients without encounters: {len(patients_without_encounters)} ({len(patients_without_encounters)/len(patients_df)*100:.1f}%)\")  # Sequence statistics if sequences:     total_visits = sum(seq.sequence_length for seq in sequences)     total_codes = sum(visit.num_codes() for seq in sequences for visit in seq.visits)     print(f\"\\n3. Sequence coverage:\")     print(f\"   Total sequences: {len(sequences)}\")     print(f\"   Total visits: {total_visits}\")     print(f\"   Total codes: {total_codes}\")     print(f\"   Vocabulary size: {len(sequence_builder.vocab)}\")  print(\"\\n\" + \"=\" * 60) <pre>Data Quality Assessment:\n============================================================\n\n1. Missing Data:\n\n  Patients:\n    death_date: 94.0% missing\n\n  Encounters:\n    REASONCODE: 35.9% missing\n    REASONDESCRIPTION: 35.9% missing\n\n  Conditions:\n    STOP: 25.6% missing\n\n  Observations:\n    ENCOUNTER: 5.1% missing\n    CATEGORY: 5.1% missing\n    UNITS: 29.6% missing\n\n  Medications:\n    STOP: 6.4% missing\n    REASONCODE: 21.9% missing\n    REASONDESCRIPTION: 21.9% missing\n\n  Procedures:\n    REASONCODE: 51.6% missing\n    REASONDESCRIPTION: 51.6% missing\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~/miniforge3-new/envs/ehrsequencing/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n   3811 try:\n-&gt; 3812     return self._engine.get_loc(casted_key)\n   3813 except KeyError as err:\n\nFile pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'Id'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[34], line 23\n     21 # Patients with no encounters\n     22 patients_with_encounters = set(encounters_df['PATIENT'].unique())\n---&gt; 23 patients_without_encounters = set(patients_df['Id']) - patients_with_encounters\n     24 print(f\"\\n2. Patients without encounters: {len(patients_without_encounters)} ({len(patients_without_encounters)/len(patients_df)*100:.1f}%)\")\n     26 # Vocabulary coverage\n\nFile ~/miniforge3-new/envs/ehrsequencing/lib/python3.10/site-packages/pandas/core/frame.py:4113, in DataFrame.__getitem__(self, key)\n   4111 if self.columns.nlevels &gt; 1:\n   4112     return self._getitem_multilevel(key)\n-&gt; 4113 indexer = self.columns.get_loc(key)\n   4114 if is_integer(indexer):\n   4115     indexer = [indexer]\n\nFile ~/miniforge3-new/envs/ehrsequencing/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819, in Index.get_loc(self, key)\n   3814     if isinstance(casted_key, slice) or (\n   3815         isinstance(casted_key, abc.Iterable)\n   3816         and any(isinstance(x, slice) for x in casted_key)\n   3817     ):\n   3818         raise InvalidIndexError(key)\n-&gt; 3819     raise KeyError(key) from err\n   3820 except TypeError:\n   3821     # If we have a listlike key, _check_indexing_error will raise\n   3822     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3823     #  the TypeError.\n   3824     self._check_indexing_error(key)\n\nKeyError: 'Id'</pre> In\u00a0[35]: Copied! <pre># Extract timestamps from encounters\nif 'START' in encounters_df.columns:\n    encounters_df['START'] = pd.to_datetime(encounters_df['START'])\n    encounters_df['YEAR'] = encounters_df['START'].dt.year\n    encounters_df['MONTH'] = encounters_df['START'].dt.month\n    encounters_df['DAY_OF_WEEK'] = encounters_df['START'].dt.dayofweek\n    \n    # Encounters by year\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # By year\n    year_counts = encounters_df['YEAR'].value_counts().sort_index()\n    axes[0, 0].bar(year_counts.index, year_counts.values, alpha=0.7, edgecolor='black')\n    axes[0, 0].set_xlabel('Year')\n    axes[0, 0].set_ylabel('Number of Encounters')\n    axes[0, 0].set_title('Encounters by Year')\n    axes[0, 0].tick_params(axis='x', rotation=45)\n    \n    # By month\n    month_counts = encounters_df['MONTH'].value_counts().sort_index()\n    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n    axes[0, 1].bar(range(1, 13), [month_counts.get(i, 0) for i in range(1, 13)], alpha=0.7, edgecolor='black')\n    axes[0, 1].set_xlabel('Month')\n    axes[0, 1].set_ylabel('Number of Encounters')\n    axes[0, 1].set_title('Encounters by Month')\n    axes[0, 1].set_xticks(range(1, 13))\n    axes[0, 1].set_xticklabels(month_names, rotation=45)\n    \n    # By day of week\n    dow_counts = encounters_df['DAY_OF_WEEK'].value_counts().sort_index()\n    dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n    axes[1, 0].bar(range(7), [dow_counts.get(i, 0) for i in range(7)], alpha=0.7, edgecolor='black')\n    axes[1, 0].set_xlabel('Day of Week')\n    axes[1, 0].set_ylabel('Number of Encounters')\n    axes[1, 0].set_title('Encounters by Day of Week')\n    axes[1, 0].set_xticks(range(7))\n    axes[1, 0].set_xticklabels(dow_names)\n    \n    # Timeline (cumulative encounters)\n    encounters_sorted = encounters_df.sort_values('START')\n    encounters_sorted['CUMULATIVE'] = range(1, len(encounters_sorted) + 1)\n    axes[1, 1].plot(encounters_sorted['START'], encounters_sorted['CUMULATIVE'], alpha=0.7)\n    axes[1, 1].set_xlabel('Date')\n    axes[1, 1].set_ylabel('Cumulative Encounters')\n    axes[1, 1].set_title('Cumulative Encounters Over Time')\n    axes[1, 1].tick_params(axis='x', rotation=45)\n    \n    plt.tight_layout()\n    plt.show()\n</pre> # Extract timestamps from encounters if 'START' in encounters_df.columns:     encounters_df['START'] = pd.to_datetime(encounters_df['START'])     encounters_df['YEAR'] = encounters_df['START'].dt.year     encounters_df['MONTH'] = encounters_df['START'].dt.month     encounters_df['DAY_OF_WEEK'] = encounters_df['START'].dt.dayofweek          # Encounters by year     fig, axes = plt.subplots(2, 2, figsize=(14, 10))          # By year     year_counts = encounters_df['YEAR'].value_counts().sort_index()     axes[0, 0].bar(year_counts.index, year_counts.values, alpha=0.7, edgecolor='black')     axes[0, 0].set_xlabel('Year')     axes[0, 0].set_ylabel('Number of Encounters')     axes[0, 0].set_title('Encounters by Year')     axes[0, 0].tick_params(axis='x', rotation=45)          # By month     month_counts = encounters_df['MONTH'].value_counts().sort_index()     month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']     axes[0, 1].bar(range(1, 13), [month_counts.get(i, 0) for i in range(1, 13)], alpha=0.7, edgecolor='black')     axes[0, 1].set_xlabel('Month')     axes[0, 1].set_ylabel('Number of Encounters')     axes[0, 1].set_title('Encounters by Month')     axes[0, 1].set_xticks(range(1, 13))     axes[0, 1].set_xticklabels(month_names, rotation=45)          # By day of week     dow_counts = encounters_df['DAY_OF_WEEK'].value_counts().sort_index()     dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']     axes[1, 0].bar(range(7), [dow_counts.get(i, 0) for i in range(7)], alpha=0.7, edgecolor='black')     axes[1, 0].set_xlabel('Day of Week')     axes[1, 0].set_ylabel('Number of Encounters')     axes[1, 0].set_title('Encounters by Day of Week')     axes[1, 0].set_xticks(range(7))     axes[1, 0].set_xticklabels(dow_names)          # Timeline (cumulative encounters)     encounters_sorted = encounters_df.sort_values('START')     encounters_sorted['CUMULATIVE'] = range(1, len(encounters_sorted) + 1)     axes[1, 1].plot(encounters_sorted['START'], encounters_sorted['CUMULATIVE'], alpha=0.7)     axes[1, 1].set_xlabel('Date')     axes[1, 1].set_ylabel('Cumulative Encounters')     axes[1, 1].set_title('Cumulative Encounters Over Time')     axes[1, 1].tick_params(axis='x', rotation=45)          plt.tight_layout()     plt.show()"},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#synthea-data-exploration","title":"Synthea Data Exploration\u00b6","text":"<p>This notebook demonstrates the complete data pipeline for EHR sequence modeling:</p> <ol> <li>Loading Synthea data using <code>SyntheaAdapter</code></li> <li>Visit grouping to construct clinical visits from events</li> <li>Sequence building to create patient-level sequences</li> <li>Data statistics and quality assessment</li> <li>Visualization of temporal patterns</li> </ol>"},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#1-load-synthea-data","title":"1. Load Synthea Data\u00b6","text":"<p>We'll use the <code>SyntheaAdapter</code> to load raw EHR data. If you don't have Synthea data, you can:</p> <ul> <li>Download from: https://synthea.mitre.org/downloads</li> <li>Generate synthetic data: https://github.com/synthetichealth/synthea</li> <li>Use test fixtures: <code>../../tests/fixtures/mock_synthea_data/</code></li> </ul>"},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#11-explore-available-data-files","title":"1.1 Explore Available Data Files\u00b6","text":""},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#2-data-statistics","title":"2. Data Statistics\u00b6","text":"<p>Let's compute summary statistics across all data types.</p>"},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#21-encounters-per-patient","title":"2.1 Encounters per Patient\u00b6","text":""},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#22-code-frequency-analysis","title":"2.2 Code Frequency Analysis\u00b6","text":""},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#3-visit-grouping","title":"3. Visit Grouping\u00b6","text":"<p>Now we'll group individual events into clinical visits using the <code>VisitGrouper</code>.</p>"},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#31-examine-a-sample-visit","title":"3.1 Examine a Sample Visit\u00b6","text":""},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#32-visit-statistics","title":"3.2 Visit Statistics\u00b6","text":""},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#4-sequence-building","title":"4. Sequence Building\u00b6","text":"<p>Now we'll build patient sequences suitable for modeling using <code>PatientSequenceBuilder</code>.</p>"},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#41-examine-sample-sequence","title":"4.1 Examine Sample Sequence\u00b6","text":""},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#42-sequence-length-distribution","title":"4.2 Sequence Length Distribution\u00b6","text":""},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#5-data-quality-assessment","title":"5. Data Quality Assessment\u00b6","text":""},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#6-temporal-patterns","title":"6. Temporal Patterns\u00b6","text":""},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#7-summary-and-next-steps","title":"7. Summary and Next Steps\u00b6","text":""},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#what-weve-explored","title":"What We've Explored\u00b6","text":"<ol> <li>\u2705 Loaded Synthea EHR data using <code>SyntheaAdapter</code></li> <li>\u2705 Analyzed patient demographics and clinical events</li> <li>\u2705 Grouped events into visits using <code>VisitGrouper</code></li> <li>\u2705 Built patient sequences using <code>PatientSequenceBuilder</code></li> <li>\u2705 Assessed data quality and temporal patterns</li> </ol>"},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#key-findings","title":"Key Findings\u00b6","text":"<ul> <li>Data volume: Summary statistics on patients, encounters, and codes</li> <li>Visit patterns: Distribution of visits per patient and codes per visit</li> <li>Sequence characteristics: Variable-length sequences suitable for LSTM modeling</li> <li>Temporal patterns: Seasonal and weekly trends in healthcare utilization</li> </ul>"},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#next-steps","title":"Next Steps\u00b6","text":"<ol> <li><p>Train LSTM baseline model:</p> <pre>python examples/train_lstm_baseline.py \\\n    --data-path data/synthea \\\n    --output-dir results/lstm_baseline\n</pre> </li> <li><p>Implement Med2Vec embeddings (Phase 2)</p> </li> <li><p>Explore advanced models (Transformers, BEHRT)</p> </li> <li><p>Define prediction tasks (mortality, readmission, disease progression)</p> </li> </ol>"},{"location":"notebooks/01_synthea_data_exploration/01_synthea_data_exploration/#references","title":"References\u00b6","text":"<ul> <li>Synthea: https://synthea.mitre.org/</li> <li>LSTM for EHR: Choi et al., \"Doctor AI\" (2016)</li> <li>BEHRT: Li et al., \"BEHRT: Transformer for Electronic Health Records\" (2020)</li> <li>Med2Vec: Choi et al., \"Multi-layer Representation Learning for Medical Concepts\" (2016)</li> </ul>"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/","title":"01a lstm data preparation","text":"In\u00a0[1]: Copied! <pre>import sys\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom collections import Counter\n\n# Set plotting style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Import ehrsequencing package\nfrom ehrsequencing.data.adapters import SyntheaAdapter\nfrom ehrsequencing.data.visit_grouper import VisitGrouper\nfrom ehrsequencing.data.sequence_builder import PatientSequenceBuilder\nfrom ehrsequencing.models import create_lstm_baseline\n\nprint(\"\u2705 Imports successful\")\n</pre> import sys from pathlib import Path import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import torch from collections import Counter  # Set plotting style sns.set_style('whitegrid') plt.rcParams['figure.figsize'] = (12, 6)  # Import ehrsequencing package from ehrsequencing.data.adapters import SyntheaAdapter from ehrsequencing.data.visit_grouper import VisitGrouper from ehrsequencing.data.sequence_builder import PatientSequenceBuilder from ehrsequencing.models import create_lstm_baseline  print(\"\u2705 Imports successful\") <pre>\u2705 Imports successful\n</pre> In\u00a0[2]: Copied! <pre># Path to Synthea data\ndata_path = Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'all_cohorts'\n\n# Initialize adapter\nadapter = SyntheaAdapter(data_path=str(data_path))\n\nprint(f\"\u2705 Loaded Synthea data from: {data_path}\")\n</pre> # Path to Synthea data data_path = Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'all_cohorts'  # Initialize adapter adapter = SyntheaAdapter(data_path=str(data_path))  print(f\"\u2705 Loaded Synthea data from: {data_path}\") <pre>\u2705 Loaded Synthea data from: /Users/pleiadian53/work/loinc-predictor/data/synthea/all_cohorts\n</pre> In\u00a0[3]: Copied! <pre># Load patients (using more for better examples)\npatients = adapter.load_patients(limit=50)\npatient_ids = [p.patient_id for p in patients]\n\nprint(f\"Loaded {len(patients)} patients\")\nprint(f\"\\n\ud83d\udcca Data Shape: List[PatientInfo] with length {len(patients)}\")\n</pre> # Load patients (using more for better examples) patients = adapter.load_patients(limit=50) patient_ids = [p.patient_id for p in patients]  print(f\"Loaded {len(patients)} patients\") print(f\"\\n\ud83d\udcca Data Shape: List[PatientInfo] with length {len(patients)}\") <pre>Loaded 50 patients\n\n\ud83d\udcca Data Shape: List[PatientInfo] with length 50\n</pre> In\u00a0[4]: Copied! <pre># Initialize visit grouper\nvisit_grouper = VisitGrouper(\n    strategy='hybrid',\n    time_window_hours=24,\n    preserve_code_types=True\n)\n\nprint(\"\u2705 VisitGrouper initialized\")\n</pre> # Initialize visit grouper visit_grouper = VisitGrouper(     strategy='hybrid',     time_window_hours=24,     preserve_code_types=True )  print(\"\u2705 VisitGrouper initialized\") <pre>\u2705 VisitGrouper initialized\n</pre> In\u00a0[5]: Copied! <pre># Load events and group into visits\nprint(f\"Processing {len(patient_ids)} patients...\")\n\npatient_visits = {}\nfor patient_id in patient_ids:\n    events = adapter.load_events(patient_ids=[patient_id])\n    visits = visit_grouper.group_events(events, patient_id=patient_id)\n    patient_visits[patient_id] = visits\n\nprint(f\"\\n\u2705 Grouped visits for {len(patient_visits)} patients\")\nprint(f\"\\n\ud83d\udcca Data Shape: Dict[str, List[Visit]]\")\nprint(f\"   - Keys: {len(patient_visits)} patient IDs\")\nprint(f\"   - Values: Lists of Visit objects\")\nprint(f\"   - Total visits: {sum(len(v) for v in patient_visits.values())}\")\n</pre> # Load events and group into visits print(f\"Processing {len(patient_ids)} patients...\")  patient_visits = {} for patient_id in patient_ids:     events = adapter.load_events(patient_ids=[patient_id])     visits = visit_grouper.group_events(events, patient_id=patient_id)     patient_visits[patient_id] = visits  print(f\"\\n\u2705 Grouped visits for {len(patient_visits)} patients\") print(f\"\\n\ud83d\udcca Data Shape: Dict[str, List[Visit]]\") print(f\"   - Keys: {len(patient_visits)} patient IDs\") print(f\"   - Values: Lists of Visit objects\") print(f\"   - Total visits: {sum(len(v) for v in patient_visits.values())}\") <pre>Processing 50 patients...\n\n\u2705 Grouped visits for 50 patients\n\n\ud83d\udcca Data Shape: Dict[str, List[Visit]]\n   - Keys: 50 patient IDs\n   - Values: Lists of Visit objects\n   - Total visits: 3037\n</pre> In\u00a0[6]: Copied! <pre># Initialize sequence builder\nsequence_builder = PatientSequenceBuilder(\n    vocab=None,\n    max_visits=50,\n    max_codes_per_visit=100,\n    use_semantic_order=True\n)\n\nprint(\"\u2705 PatientSequenceBuilder initialized\")\nprint(f\"   Max visits per sequence: 50\")\nprint(f\"   Max codes per visit: 100\")\n</pre> # Initialize sequence builder sequence_builder = PatientSequenceBuilder(     vocab=None,     max_visits=50,     max_codes_per_visit=100,     use_semantic_order=True )  print(\"\u2705 PatientSequenceBuilder initialized\") print(f\"   Max visits per sequence: 50\") print(f\"   Max codes per visit: 100\") <pre>\u2705 PatientSequenceBuilder initialized\n   Max visits per sequence: 50\n   Max codes per visit: 100\n</pre> In\u00a0[7]: Copied! <pre># Build vocabulary from all patient visits\nprint(\"Building vocabulary...\")\nvocab = sequence_builder.build_vocabulary(list(patient_visits.values()), min_frequency=1)\n\nprint(f\"\\n\u2705 Vocabulary built\")\nprint(f\"   Vocabulary size: {len(vocab)}\")\nprint(f\"   Special tokens: [PAD]=0, [UNK]=1, [MASK]=2, [CLS]=3, [SEP]=4\")\nprint(f\"\\n\ud83d\udcca Data Shape: Dict[str, int]\")\nprint(f\"   - Medical code \u2192 Integer ID mapping\")\nprint(f\"   - Example: {list(vocab.items())[:5]}\")\n</pre> # Build vocabulary from all patient visits print(\"Building vocabulary...\") vocab = sequence_builder.build_vocabulary(list(patient_visits.values()), min_frequency=1)  print(f\"\\n\u2705 Vocabulary built\") print(f\"   Vocabulary size: {len(vocab)}\") print(f\"   Special tokens: [PAD]=0, [UNK]=1, [MASK]=2, [CLS]=3, [SEP]=4\") print(f\"\\n\ud83d\udcca Data Shape: Dict[str, int]\") print(f\"   - Medical code \u2192 Integer ID mapping\") print(f\"   - Example: {list(vocab.items())[:5]}\") <pre>Building vocabulary...\n\n\u2705 Vocabulary built\n   Vocabulary size: 659\n   Special tokens: [PAD]=0, [UNK]=1, [MASK]=2, [CLS]=3, [SEP]=4\n\n\ud83d\udcca Data Shape: Dict[str, int]\n   - Medical code \u2192 Integer ID mapping\n   - Example: [('[PAD]', 0), ('[UNK]', 1), ('[MASK]', 2), ('[CLS]', 3), ('[SEP]', 4)]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Build patient sequences\nprint(\"Building patient sequences...\")\nsequences = sequence_builder.build_sequences(list(patient_visits.values()), min_visits=2)\n\nprint(f\"\\n\u2705 Built {len(sequences)} sequences\")\nprint(f\"   Filtered out: {len(patient_visits) - len(sequences)} patients (&lt; 2 visits)\")\nprint(f\"\\n\ud83d\udcca Data Shape: List[PatientSequence]\")\nprint(f\"   - Length: {len(sequences)}\")\nprint(f\"   - Each PatientSequence contains:\")\nprint(f\"     \u2022 patient_id: str\")\nprint(f\"     \u2022 visits: List[Visit]\")\nprint(f\"     \u2022 sequence_length: int\")\nprint(f\"     \u2022 metadata: Optional[Dict]\")\n</pre> # Build patient sequences print(\"Building patient sequences...\") sequences = sequence_builder.build_sequences(list(patient_visits.values()), min_visits=2)  print(f\"\\n\u2705 Built {len(sequences)} sequences\") print(f\"   Filtered out: {len(patient_visits) - len(sequences)} patients (&lt; 2 visits)\") print(f\"\\n\ud83d\udcca Data Shape: List[PatientSequence]\") print(f\"   - Length: {len(sequences)}\") print(f\"   - Each PatientSequence contains:\") print(f\"     \u2022 patient_id: str\") print(f\"     \u2022 visits: List[Visit]\") print(f\"     \u2022 sequence_length: int\") print(f\"     \u2022 metadata: Optional[Dict]\") <pre>Building patient sequences...\n</pre> <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[8], line 3\n      1 # Build patient sequences\n      2 print(\"Building patient sequences...\")\n----&gt; 3 sequences = sequence_builder.build_sequences(patient_visits, min_visits=2)\n      5 print(f\"\\n\u2705 Built {len(sequences)} sequences\")\n      6 print(f\"   Filtered out: {len(patient_visits) - len(sequences)} patients (&lt; 2 visits)\")\n\nFile ~/work/ehr-sequencing/src/ehrsequencing/data/sequence_builder.py:166, in PatientSequenceBuilder.build_sequences(self, patient_visits, min_visits)\n    163     continue\n    165 # Assume all visits in the list belong to the same patient\n--&gt; 166 patient_id = visits[0].patient_id\n    168 sequence = PatientSequence(\n    169     patient_id=patient_id,\n    170     visits=visits,\n   (...)\n    175     }\n    176 )\n    177 sequences.append(sequence)\n\nAttributeError: 'str' object has no attribute 'patient_id'</pre> In\u00a0[\u00a0]: Copied! <pre># Pick a sample sequence\nsample_seq = sequences[0]\n\nprint(\"Sample Patient Sequence (Before Encoding):\")\nprint(\"=\" * 70)\nprint(f\"Patient ID: {sample_seq.patient_id}\")\nprint(f\"Number of visits: {sample_seq.sequence_length}\")\nprint(f\"\\n\ud83d\udcca Data Shape:\")\nprint(f\"   - Type: PatientSequence dataclass\")\nprint(f\"   - visits: List[Visit] with length {len(sample_seq.visits)}\")\n\n# Show first 3 visits\nprint(f\"\\nFirst 3 visits:\")\nfor i, visit in enumerate(sample_seq.visits[:3]):\n    print(f\"\\n  Visit {i+1}:\")\n    print(f\"    Timestamp: {visit.timestamp}\")\n    print(f\"    Number of codes: {visit.num_codes()}\")\n    print(f\"    Code types: {list(visit.codes_by_type.keys())}\")\n    \n    # Show some actual codes\n    all_codes = visit.get_all_codes()\n    print(f\"    Sample codes (first 5): {all_codes[:5]}\")\n    print(f\"    \ud83d\udcca Shape: List[str] with length {len(all_codes)}\")\n</pre> # Pick a sample sequence sample_seq = sequences[0]  print(\"Sample Patient Sequence (Before Encoding):\") print(\"=\" * 70) print(f\"Patient ID: {sample_seq.patient_id}\") print(f\"Number of visits: {sample_seq.sequence_length}\") print(f\"\\n\ud83d\udcca Data Shape:\") print(f\"   - Type: PatientSequence dataclass\") print(f\"   - visits: List[Visit] with length {len(sample_seq.visits)}\")  # Show first 3 visits print(f\"\\nFirst 3 visits:\") for i, visit in enumerate(sample_seq.visits[:3]):     print(f\"\\n  Visit {i+1}:\")     print(f\"    Timestamp: {visit.timestamp}\")     print(f\"    Number of codes: {visit.num_codes()}\")     print(f\"    Code types: {list(visit.codes_by_type.keys())}\")          # Show some actual codes     all_codes = visit.get_all_codes()     print(f\"    Sample codes (first 5): {all_codes[:5]}\")     print(f\"    \ud83d\udcca Shape: List[str] with length {len(all_codes)}\") In\u00a0[\u00a0]: Copied! <pre># Encode the sample sequence\nencoded = sequence_builder.encode_sequence(sample_seq, return_tensors=False)\n\nprint(\"Encoded Sequence (LSTM-Ready Format):\")\nprint(\"=\" * 70)\nprint(f\"Patient ID: {encoded['patient_id']}\")\nprint(f\"Sequence length: {encoded['sequence_length']} visits\")\n\nprint(f\"\\n\ud83d\udcca Data Shapes After Encoding:\")\nprint(f\"   visit_codes: {np.array(encoded['visit_codes']).shape}\")\nprint(f\"      \u2192 [num_visits={len(encoded['visit_codes'])}, max_codes_per_visit={len(encoded['visit_codes'][0])}]\")\nprint(f\"      \u2192 Type: List[List[int]]\")\nprint(f\"\\n   visit_mask: {np.array(encoded['visit_mask']).shape}\")\nprint(f\"      \u2192 [num_visits={len(encoded['visit_mask'])}, max_codes_per_visit={len(encoded['visit_mask'][0])}]\")\nprint(f\"      \u2192 Type: List[List[int]] (1=real code, 0=padding)\")\nprint(f\"\\n   sequence_mask: {np.array(encoded['sequence_mask']).shape}\")\nprint(f\"      \u2192 [num_visits={len(encoded['sequence_mask'])}]\")\nprint(f\"      \u2192 Type: List[int] (1=real visit, 0=padding)\")\nprint(f\"\\n   time_deltas: {np.array(encoded['time_deltas']).shape}\")\nprint(f\"      \u2192 [num_visits-1={len(encoded['time_deltas'])}]\")\nprint(f\"      \u2192 Type: List[float] (days between consecutive visits)\")\n</pre> # Encode the sample sequence encoded = sequence_builder.encode_sequence(sample_seq, return_tensors=False)  print(\"Encoded Sequence (LSTM-Ready Format):\") print(\"=\" * 70) print(f\"Patient ID: {encoded['patient_id']}\") print(f\"Sequence length: {encoded['sequence_length']} visits\")  print(f\"\\n\ud83d\udcca Data Shapes After Encoding:\") print(f\"   visit_codes: {np.array(encoded['visit_codes']).shape}\") print(f\"      \u2192 [num_visits={len(encoded['visit_codes'])}, max_codes_per_visit={len(encoded['visit_codes'][0])}]\") print(f\"      \u2192 Type: List[List[int]]\") print(f\"\\n   visit_mask: {np.array(encoded['visit_mask']).shape}\") print(f\"      \u2192 [num_visits={len(encoded['visit_mask'])}, max_codes_per_visit={len(encoded['visit_mask'][0])}]\") print(f\"      \u2192 Type: List[List[int]] (1=real code, 0=padding)\") print(f\"\\n   sequence_mask: {np.array(encoded['sequence_mask']).shape}\") print(f\"      \u2192 [num_visits={len(encoded['sequence_mask'])}]\") print(f\"      \u2192 Type: List[int] (1=real visit, 0=padding)\") print(f\"\\n   time_deltas: {np.array(encoded['time_deltas']).shape}\") print(f\"      \u2192 [num_visits-1={len(encoded['time_deltas'])}]\") print(f\"      \u2192 Type: List[float] (days between consecutive visits)\") In\u00a0[\u00a0]: Copied! <pre># Show actual encoded values for first visit\nprint(\"First Visit - Detailed View:\")\nprint(\"=\" * 70)\n\nfirst_visit_codes = encoded['visit_codes'][0]\nfirst_visit_mask = encoded['visit_mask'][0]\n\n# Count real vs padded codes\nnum_real_codes = sum(first_visit_mask)\nnum_padding = len(first_visit_mask) - num_real_codes\n\nprint(f\"Real codes: {num_real_codes}\")\nprint(f\"Padding: {num_padding}\")\nprint(f\"\\nFirst 10 code IDs: {first_visit_codes[:10]}\")\nprint(f\"First 10 mask values: {first_visit_mask[:10]}\")\nprint(f\"\\nLast 10 code IDs (should be padding): {first_visit_codes[-10:]}\")\nprint(f\"Last 10 mask values (should be 0): {first_visit_mask[-10:]}\")\n</pre> # Show actual encoded values for first visit print(\"First Visit - Detailed View:\") print(\"=\" * 70)  first_visit_codes = encoded['visit_codes'][0] first_visit_mask = encoded['visit_mask'][0]  # Count real vs padded codes num_real_codes = sum(first_visit_mask) num_padding = len(first_visit_mask) - num_real_codes  print(f\"Real codes: {num_real_codes}\") print(f\"Padding: {num_padding}\") print(f\"\\nFirst 10 code IDs: {first_visit_codes[:10]}\") print(f\"First 10 mask values: {first_visit_mask[:10]}\") print(f\"\\nLast 10 code IDs (should be padding): {first_visit_codes[-10:]}\") print(f\"Last 10 mask values (should be 0): {first_visit_mask[-10:]}\") In\u00a0[\u00a0]: Copied! <pre># Define diabetes-related codes (SNOMED-CT)\ndiabetes_codes = {\n    '44054006',   # Type 2 diabetes mellitus\n    '46635009',   # Type 1 diabetes mellitus\n    '73211009',   # Diabetes mellitus\n    '11687002',   # Gestational diabetes\n    '190330002',  # Diabetes mellitus without complication\n    '190331003',  # Diabetes mellitus with complication\n}\n\nprint(f\"Diabetes codes: {diabetes_codes}\")\nprint(f\"Number of codes: {len(diabetes_codes)}\")\n</pre> # Define diabetes-related codes (SNOMED-CT) diabetes_codes = {     '44054006',   # Type 2 diabetes mellitus     '46635009',   # Type 1 diabetes mellitus     '73211009',   # Diabetes mellitus     '11687002',   # Gestational diabetes     '190330002',  # Diabetes mellitus without complication     '190331003',  # Diabetes mellitus with complication }  print(f\"Diabetes codes: {diabetes_codes}\") print(f\"Number of codes: {len(diabetes_codes)}\") In\u00a0[\u00a0]: Copied! <pre># Create labels for all sequences\ndef has_diabetes(sequence):\n    \"\"\"Check if patient has diabetes based on their visit codes.\"\"\"\n    for visit in sequence.visits:\n        all_codes = visit.get_all_codes()\n        if any(code in diabetes_codes for code in all_codes):\n            return True\n    return False\n\n# Create dataset with labels\ndataset_items = []\nfor seq in sequences:\n    encoded = sequence_builder.encode_sequence(seq, return_tensors=False)\n    label = 1 if has_diabetes(seq) else 0\n    \n    dataset_items.append({\n        'patient_id': seq.patient_id,\n        'visit_codes': encoded['visit_codes'],\n        'visit_mask': encoded['visit_mask'],\n        'sequence_mask': encoded['sequence_mask'],\n        'time_deltas': encoded['time_deltas'],\n        'label': label\n    })\n\nprint(f\"\\n\u2705 Created {len(dataset_items)} labeled sequences\")\nprint(f\"\\n\ud83d\udcca Dataset Item Shape:\")\nprint(f\"   - Type: List[Dict]\")\nprint(f\"   - Each dict contains:\")\nprint(f\"     \u2022 patient_id: str\")\nprint(f\"     \u2022 visit_codes: List[List[int]] shape [num_visits, max_codes_per_visit]\")\nprint(f\"     \u2022 visit_mask: List[List[int]] shape [num_visits, max_codes_per_visit]\")\nprint(f\"     \u2022 sequence_mask: List[int] shape [num_visits]\")\nprint(f\"     \u2022 time_deltas: List[float] shape [num_visits-1]\")\nprint(f\"     \u2022 label: int (0 or 1)\")\n\n# Label distribution\nnum_positive = sum(item['label'] for item in dataset_items)\nnum_negative = len(dataset_items) - num_positive\n\nprint(f\"\\nLabel Distribution:\")\nprint(f\"   Positive (has diabetes): {num_positive} ({num_positive/len(dataset_items)*100:.1f}%)\")\nprint(f\"   Negative (no diabetes): {num_negative} ({num_negative/len(dataset_items)*100:.1f}%)\")\n</pre> # Create labels for all sequences def has_diabetes(sequence):     \"\"\"Check if patient has diabetes based on their visit codes.\"\"\"     for visit in sequence.visits:         all_codes = visit.get_all_codes()         if any(code in diabetes_codes for code in all_codes):             return True     return False  # Create dataset with labels dataset_items = [] for seq in sequences:     encoded = sequence_builder.encode_sequence(seq, return_tensors=False)     label = 1 if has_diabetes(seq) else 0          dataset_items.append({         'patient_id': seq.patient_id,         'visit_codes': encoded['visit_codes'],         'visit_mask': encoded['visit_mask'],         'sequence_mask': encoded['sequence_mask'],         'time_deltas': encoded['time_deltas'],         'label': label     })  print(f\"\\n\u2705 Created {len(dataset_items)} labeled sequences\") print(f\"\\n\ud83d\udcca Dataset Item Shape:\") print(f\"   - Type: List[Dict]\") print(f\"   - Each dict contains:\") print(f\"     \u2022 patient_id: str\") print(f\"     \u2022 visit_codes: List[List[int]] shape [num_visits, max_codes_per_visit]\") print(f\"     \u2022 visit_mask: List[List[int]] shape [num_visits, max_codes_per_visit]\") print(f\"     \u2022 sequence_mask: List[int] shape [num_visits]\") print(f\"     \u2022 time_deltas: List[float] shape [num_visits-1]\") print(f\"     \u2022 label: int (0 or 1)\")  # Label distribution num_positive = sum(item['label'] for item in dataset_items) num_negative = len(dataset_items) - num_positive  print(f\"\\nLabel Distribution:\") print(f\"   Positive (has diabetes): {num_positive} ({num_positive/len(dataset_items)*100:.1f}%)\") print(f\"   Negative (no diabetes): {num_negative} ({num_negative/len(dataset_items)*100:.1f}%)\") In\u00a0[\u00a0]: Copied! <pre># Collate function (same as in train_lstm_baseline.py)\ndef collate_fn(batch):\n    \"\"\"\n    Collate function for DataLoader.\n    Handles variable-length sequences and creates proper masks.\n    \"\"\"\n    # Extract data\n    visit_codes = [item['visit_codes'] for item in batch]\n    labels = torch.tensor([item['label'] for item in batch], dtype=torch.float32)\n    \n    # Get dimensions\n    batch_size = len(visit_codes)\n    max_visits = max(len(seq) for seq in visit_codes)\n    max_codes = max(max(len(visit) for visit in seq) for seq in visit_codes)\n    \n    # Create padded tensors\n    padded_codes = torch.zeros(batch_size, max_visits, max_codes, dtype=torch.long)\n    visit_mask = torch.zeros(batch_size, max_visits, max_codes, dtype=torch.bool)\n    sequence_mask = torch.zeros(batch_size, max_visits, dtype=torch.bool)\n    \n    # Fill tensors\n    for i, seq in enumerate(visit_codes):\n        sequence_mask[i, :len(seq)] = 1\n        for j, visit in enumerate(seq):\n            padded_codes[i, j, :len(visit)] = torch.tensor(visit)\n            visit_mask[i, j, :len(visit)] = 1\n    \n    return {\n        'visit_codes': padded_codes,\n        'visit_mask': visit_mask,\n        'sequence_mask': sequence_mask,\n        'labels': labels.unsqueeze(1)\n    }\n\nprint(\"\u2705 Collate function defined\")\n</pre> # Collate function (same as in train_lstm_baseline.py) def collate_fn(batch):     \"\"\"     Collate function for DataLoader.     Handles variable-length sequences and creates proper masks.     \"\"\"     # Extract data     visit_codes = [item['visit_codes'] for item in batch]     labels = torch.tensor([item['label'] for item in batch], dtype=torch.float32)          # Get dimensions     batch_size = len(visit_codes)     max_visits = max(len(seq) for seq in visit_codes)     max_codes = max(max(len(visit) for visit in seq) for seq in visit_codes)          # Create padded tensors     padded_codes = torch.zeros(batch_size, max_visits, max_codes, dtype=torch.long)     visit_mask = torch.zeros(batch_size, max_visits, max_codes, dtype=torch.bool)     sequence_mask = torch.zeros(batch_size, max_visits, dtype=torch.bool)          # Fill tensors     for i, seq in enumerate(visit_codes):         sequence_mask[i, :len(seq)] = 1         for j, visit in enumerate(seq):             padded_codes[i, j, :len(visit)] = torch.tensor(visit)             visit_mask[i, j, :len(visit)] = 1          return {         'visit_codes': padded_codes,         'visit_mask': visit_mask,         'sequence_mask': sequence_mask,         'labels': labels.unsqueeze(1)     }  print(\"\u2705 Collate function defined\") In\u00a0[\u00a0]: Copied! <pre># Create a sample batch\nbatch_size = 4\nsample_batch = dataset_items[:batch_size]\n\n# Collate the batch\nbatched_data = collate_fn(sample_batch)\n\nprint(\"Sample Batch (LSTM Model Input):\")\nprint(\"=\" * 70)\nprint(f\"Batch size: {batch_size}\")\nprint(f\"\\n\ud83d\udcca Tensor Shapes:\")\nprint(f\"\\n   visit_codes: {batched_data['visit_codes'].shape}\")\nprint(f\"      \u2192 [batch_size={batched_data['visit_codes'].shape[0]}, \")\nprint(f\"         max_visits={batched_data['visit_codes'].shape[1]}, \")\nprint(f\"         max_codes_per_visit={batched_data['visit_codes'].shape[2]}]\")\nprint(f\"      \u2192 dtype: {batched_data['visit_codes'].dtype}\")\nprint(f\"\\n   visit_mask: {batched_data['visit_mask'].shape}\")\nprint(f\"      \u2192 [batch_size, max_visits, max_codes_per_visit]\")\nprint(f\"      \u2192 dtype: {batched_data['visit_mask'].dtype}\")\nprint(f\"\\n   sequence_mask: {batched_data['sequence_mask'].shape}\")\nprint(f\"      \u2192 [batch_size, max_visits]\")\nprint(f\"      \u2192 dtype: {batched_data['sequence_mask'].dtype}\")\nprint(f\"\\n   labels: {batched_data['labels'].shape}\")\nprint(f\"      \u2192 [batch_size, 1]\")\nprint(f\"      \u2192 dtype: {batched_data['labels'].dtype}\")\n\nprint(f\"\\n\\nMemory footprint:\")\nprint(f\"   visit_codes: {batched_data['visit_codes'].numel() * 8 / 1024:.2f} KB\")\nprint(f\"   visit_mask: {batched_data['visit_mask'].numel() / 1024:.2f} KB\")\nprint(f\"   sequence_mask: {batched_data['sequence_mask'].numel() / 1024:.2f} KB\")\n</pre> # Create a sample batch batch_size = 4 sample_batch = dataset_items[:batch_size]  # Collate the batch batched_data = collate_fn(sample_batch)  print(\"Sample Batch (LSTM Model Input):\") print(\"=\" * 70) print(f\"Batch size: {batch_size}\") print(f\"\\n\ud83d\udcca Tensor Shapes:\") print(f\"\\n   visit_codes: {batched_data['visit_codes'].shape}\") print(f\"      \u2192 [batch_size={batched_data['visit_codes'].shape[0]}, \") print(f\"         max_visits={batched_data['visit_codes'].shape[1]}, \") print(f\"         max_codes_per_visit={batched_data['visit_codes'].shape[2]}]\") print(f\"      \u2192 dtype: {batched_data['visit_codes'].dtype}\") print(f\"\\n   visit_mask: {batched_data['visit_mask'].shape}\") print(f\"      \u2192 [batch_size, max_visits, max_codes_per_visit]\") print(f\"      \u2192 dtype: {batched_data['visit_mask'].dtype}\") print(f\"\\n   sequence_mask: {batched_data['sequence_mask'].shape}\") print(f\"      \u2192 [batch_size, max_visits]\") print(f\"      \u2192 dtype: {batched_data['sequence_mask'].dtype}\") print(f\"\\n   labels: {batched_data['labels'].shape}\") print(f\"      \u2192 [batch_size, 1]\") print(f\"      \u2192 dtype: {batched_data['labels'].dtype}\")  print(f\"\\n\\nMemory footprint:\") print(f\"   visit_codes: {batched_data['visit_codes'].numel() * 8 / 1024:.2f} KB\") print(f\"   visit_mask: {batched_data['visit_mask'].numel() / 1024:.2f} KB\") print(f\"   sequence_mask: {batched_data['sequence_mask'].numel() / 1024:.2f} KB\") In\u00a0[\u00a0]: Copied! <pre># Visualize batch structure for first patient\nprint(\"First Patient in Batch - Detailed View:\")\nprint(\"=\" * 70)\n\npatient_0_codes = batched_data['visit_codes'][0]\npatient_0_visit_mask = batched_data['visit_mask'][0]\npatient_0_seq_mask = batched_data['sequence_mask'][0]\npatient_0_label = batched_data['labels'][0]\n\n# Count real visits\nnum_real_visits = patient_0_seq_mask.sum().item()\nprint(f\"Number of real visits: {num_real_visits}\")\nprint(f\"Label: {patient_0_label.item()} ({'Diabetes' if patient_0_label.item() == 1 else 'No Diabetes'})\")\n\n# Show first visit details\nprint(f\"\\nFirst visit:\")\nfirst_visit_codes = patient_0_codes[0]\nfirst_visit_mask = patient_0_visit_mask[0]\nnum_real_codes = first_visit_mask.sum().item()\nprint(f\"   Real codes: {num_real_codes}\")\nprint(f\"   Code IDs (first 10): {first_visit_codes[:10].tolist()}\")\nprint(f\"   Mask (first 10): {first_visit_mask[:10].tolist()}\")\n</pre> # Visualize batch structure for first patient print(\"First Patient in Batch - Detailed View:\") print(\"=\" * 70)  patient_0_codes = batched_data['visit_codes'][0] patient_0_visit_mask = batched_data['visit_mask'][0] patient_0_seq_mask = batched_data['sequence_mask'][0] patient_0_label = batched_data['labels'][0]  # Count real visits num_real_visits = patient_0_seq_mask.sum().item() print(f\"Number of real visits: {num_real_visits}\") print(f\"Label: {patient_0_label.item()} ({'Diabetes' if patient_0_label.item() == 1 else 'No Diabetes'})\")  # Show first visit details print(f\"\\nFirst visit:\") first_visit_codes = patient_0_codes[0] first_visit_mask = patient_0_visit_mask[0] num_real_codes = first_visit_mask.sum().item() print(f\"   Real codes: {num_real_codes}\") print(f\"   Code IDs (first 10): {first_visit_codes[:10].tolist()}\") print(f\"   Mask (first 10): {first_visit_mask[:10].tolist()}\") In\u00a0[\u00a0]: Copied! <pre># Create LSTM model\nmodel = create_lstm_baseline(\n    vocab_size=len(vocab),\n    task='binary_classification',\n    model_size='small'\n)\n\nprint(\"LSTM Baseline Model:\")\nprint(\"=\" * 70)\nprint(f\"Vocabulary size: {len(vocab)}\")\nprint(f\"Embedding dim: 128\")\nprint(f\"Hidden dim: 256\")\nprint(f\"Number of layers: 1\")\nprint(f\"Task: Binary classification\")\nprint(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n</pre> # Create LSTM model model = create_lstm_baseline(     vocab_size=len(vocab),     task='binary_classification',     model_size='small' )  print(\"LSTM Baseline Model:\") print(\"=\" * 70) print(f\"Vocabulary size: {len(vocab)}\") print(f\"Embedding dim: 128\") print(f\"Hidden dim: 256\") print(f\"Number of layers: 1\") print(f\"Task: Binary classification\") print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\") In\u00a0[\u00a0]: Copied! <pre># Run forward pass\nmodel.eval()\nwith torch.no_grad():\n    output = model(\n        visit_codes=batched_data['visit_codes'],\n        visit_mask=batched_data['visit_mask'],\n        sequence_mask=batched_data['sequence_mask'],\n        return_hidden=True\n    )\n\nprint(\"Model Output:\")\nprint(\"=\" * 70)\nprint(f\"\\n\ud83d\udcca Output Shapes:\")\nprint(f\"\\n   logits: {output['logits'].shape}\")\nprint(f\"      \u2192 [batch_size={output['logits'].shape[0]}, output_dim={output['logits'].shape[1]}]\")\nprint(f\"      \u2192 Raw predictions before sigmoid\")\nprint(f\"\\n   predictions: {output['predictions'].shape}\")\nprint(f\"      \u2192 [batch_size={output['predictions'].shape[0]}, output_dim={output['predictions'].shape[1]}]\")\nprint(f\"      \u2192 After sigmoid activation (probabilities)\")\nprint(f\"\\n   hidden_states: {output['hidden_states'].shape}\")\nprint(f\"      \u2192 [batch_size={output['hidden_states'].shape[0]}, \")\nprint(f\"         num_visits={output['hidden_states'].shape[1]}, \")\nprint(f\"         hidden_dim={output['hidden_states'].shape[2]}]\")\nprint(f\"      \u2192 LSTM hidden states for each visit\")\n\nprint(f\"\\n\\nPredictions:\")\nfor i in range(batch_size):\n    prob = output['predictions'][i, 0].item()\n    true_label = batched_data['labels'][i, 0].item()\n    print(f\"   Patient {i+1}: P(diabetes) = {prob:.4f}, True label = {int(true_label)}\")\n</pre> # Run forward pass model.eval() with torch.no_grad():     output = model(         visit_codes=batched_data['visit_codes'],         visit_mask=batched_data['visit_mask'],         sequence_mask=batched_data['sequence_mask'],         return_hidden=True     )  print(\"Model Output:\") print(\"=\" * 70) print(f\"\\n\ud83d\udcca Output Shapes:\") print(f\"\\n   logits: {output['logits'].shape}\") print(f\"      \u2192 [batch_size={output['logits'].shape[0]}, output_dim={output['logits'].shape[1]}]\") print(f\"      \u2192 Raw predictions before sigmoid\") print(f\"\\n   predictions: {output['predictions'].shape}\") print(f\"      \u2192 [batch_size={output['predictions'].shape[0]}, output_dim={output['predictions'].shape[1]}]\") print(f\"      \u2192 After sigmoid activation (probabilities)\") print(f\"\\n   hidden_states: {output['hidden_states'].shape}\") print(f\"      \u2192 [batch_size={output['hidden_states'].shape[0]}, \") print(f\"         num_visits={output['hidden_states'].shape[1]}, \") print(f\"         hidden_dim={output['hidden_states'].shape[2]}]\") print(f\"      \u2192 LSTM hidden states for each visit\")  print(f\"\\n\\nPredictions:\") for i in range(batch_size):     prob = output['predictions'][i, 0].item()     true_label = batched_data['labels'][i, 0].item()     print(f\"   Patient {i+1}: P(diabetes) = {prob:.4f}, True label = {int(true_label)}\") In\u00a0[\u00a0]: Copied! <pre>print(\"Complete Data Transformation Pipeline:\")\nprint(\"=\" * 70)\nprint(\"\\n1. Raw Synthea CSV Files\")\nprint(\"   \u2514\u2500&gt; patients.csv, encounters.csv, conditions.csv, etc.\")\nprint(\"\\n2. SyntheaAdapter.load_events()\")\nprint(\"   \u2514\u2500&gt; List[MedicalEvent]\")\nprint(\"       \u2022 Each event has: patient_id, timestamp, code, code_type\")\nprint(\"\\n3. VisitGrouper.group_events()\")\nprint(\"   \u2514\u2500&gt; Dict[str, List[Visit]]\")\nprint(\"       \u2022 Key: patient_id\")\nprint(\"       \u2022 Value: List of Visit objects\")\nprint(\"       \u2022 Each Visit has: visit_id, timestamp, codes_by_type\")\nprint(\"\\n4. PatientSequenceBuilder.build_sequences()\")\nprint(\"   \u2514\u2500&gt; List[PatientSequence]\")\nprint(\"       \u2022 Each sequence has: patient_id, visits, sequence_length\")\nprint(\"\\n5. PatientSequenceBuilder.encode_sequence()\")\nprint(\"   \u2514\u2500&gt; Dict with:\")\nprint(\"       \u2022 visit_codes: List[List[int]] - [num_visits, max_codes_per_visit]\")\nprint(\"       \u2022 visit_mask: List[List[int]] - [num_visits, max_codes_per_visit]\")\nprint(\"       \u2022 sequence_mask: List[int] - [num_visits]\")\nprint(\"\\n6. Add Labels\")\nprint(\"   \u2514\u2500&gt; List[Dict] with encoded data + label\")\nprint(\"       \u2022 label: int (0 or 1 for binary classification)\")\nprint(\"\\n7. collate_fn() - Batch Creation\")\nprint(\"   \u2514\u2500&gt; Dict with PyTorch tensors:\")\nprint(\"       \u2022 visit_codes: [batch_size, max_visits, max_codes_per_visit]\")\nprint(\"       \u2022 visit_mask: [batch_size, max_visits, max_codes_per_visit]\")\nprint(\"       \u2022 sequence_mask: [batch_size, max_visits]\")\nprint(\"       \u2022 labels: [batch_size, 1]\")\nprint(\"\\n8. LSTM Model Forward Pass\")\nprint(\"   \u2514\u2500&gt; Dict with:\")\nprint(\"       \u2022 logits: [batch_size, 1] - Raw predictions\")\nprint(\"       \u2022 predictions: [batch_size, 1] - Probabilities (after sigmoid)\")\nprint(\"       \u2022 hidden_states: [batch_size, max_visits, hidden_dim]\")\nprint(\"\\n\" + \"=\" * 70)\n</pre> print(\"Complete Data Transformation Pipeline:\") print(\"=\" * 70) print(\"\\n1. Raw Synthea CSV Files\") print(\"   \u2514\u2500&gt; patients.csv, encounters.csv, conditions.csv, etc.\") print(\"\\n2. SyntheaAdapter.load_events()\") print(\"   \u2514\u2500&gt; List[MedicalEvent]\") print(\"       \u2022 Each event has: patient_id, timestamp, code, code_type\") print(\"\\n3. VisitGrouper.group_events()\") print(\"   \u2514\u2500&gt; Dict[str, List[Visit]]\") print(\"       \u2022 Key: patient_id\") print(\"       \u2022 Value: List of Visit objects\") print(\"       \u2022 Each Visit has: visit_id, timestamp, codes_by_type\") print(\"\\n4. PatientSequenceBuilder.build_sequences()\") print(\"   \u2514\u2500&gt; List[PatientSequence]\") print(\"       \u2022 Each sequence has: patient_id, visits, sequence_length\") print(\"\\n5. PatientSequenceBuilder.encode_sequence()\") print(\"   \u2514\u2500&gt; Dict with:\") print(\"       \u2022 visit_codes: List[List[int]] - [num_visits, max_codes_per_visit]\") print(\"       \u2022 visit_mask: List[List[int]] - [num_visits, max_codes_per_visit]\") print(\"       \u2022 sequence_mask: List[int] - [num_visits]\") print(\"\\n6. Add Labels\") print(\"   \u2514\u2500&gt; List[Dict] with encoded data + label\") print(\"       \u2022 label: int (0 or 1 for binary classification)\") print(\"\\n7. collate_fn() - Batch Creation\") print(\"   \u2514\u2500&gt; Dict with PyTorch tensors:\") print(\"       \u2022 visit_codes: [batch_size, max_visits, max_codes_per_visit]\") print(\"       \u2022 visit_mask: [batch_size, max_visits, max_codes_per_visit]\") print(\"       \u2022 sequence_mask: [batch_size, max_visits]\") print(\"       \u2022 labels: [batch_size, 1]\") print(\"\\n8. LSTM Model Forward Pass\") print(\"   \u2514\u2500&gt; Dict with:\") print(\"       \u2022 logits: [batch_size, 1] - Raw predictions\") print(\"       \u2022 predictions: [batch_size, 1] - Probabilities (after sigmoid)\") print(\"       \u2022 hidden_states: [batch_size, max_visits, hidden_dim]\") print(\"\\n\" + \"=\" * 70) In\u00a0[\u00a0]: Copied! <pre># Create a visual diagram\nfig, ax = plt.subplots(figsize=(14, 10))\nax.axis('off')\n\n# Define stages\nstages = [\n    (\"Raw Data\\n(CSV Files)\", \"patients.csv\\nencounters.csv\\nconditions.csv\\nobservations.csv\"),\n    (\"Medical Events\\n(List[MedicalEvent])\", f\"{sum(len(adapter.load_events([pid])) for pid in patient_ids[:5])} events\\n(sample)\"),\n    (\"Visit Groups\\n(Dict[str, List[Visit]])\", f\"{len(patient_visits)} patients\\n{sum(len(v) for v in patient_visits.values())} visits\"),\n    (\"Patient Sequences\\n(List[PatientSequence])\", f\"{len(sequences)} sequences\\nmin_visits \u2265 2\"),\n    (\"Encoded Sequences\\n(List[Dict])\", f\"{len(dataset_items)} items\\nwith labels\"),\n    (\"Batched Tensors\\n(PyTorch)\", f\"[{batch_size}, {batched_data['visit_codes'].shape[1]}, {batched_data['visit_codes'].shape[2]}]\"),\n    (\"Model Output\\n(Predictions)\", f\"[{batch_size}, 1]\\nprobabilities\")\n]\n\ny_positions = np.linspace(0.9, 0.1, len(stages))\n\nfor i, ((title, desc), y) in enumerate(zip(stages, y_positions)):\n    # Draw box\n    box = plt.Rectangle((0.2, y-0.05), 0.6, 0.08, \n                        facecolor='lightblue', edgecolor='black', linewidth=2)\n    ax.add_patch(box)\n    \n    # Add text\n    ax.text(0.5, y+0.02, title, ha='center', va='center', \n           fontsize=12, fontweight='bold')\n    ax.text(0.5, y-0.02, desc, ha='center', va='center', \n           fontsize=9, style='italic')\n    \n    # Draw arrow to next stage\n    if i &lt; len(stages) - 1:\n        ax.arrow(0.5, y-0.05, 0, -0.04, head_width=0.03, head_length=0.01,\n                fc='black', ec='black', linewidth=2)\n\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nplt.title('EHR Data Transformation Pipeline for LSTM', fontsize=16, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u2705 Pipeline visualization complete!\")\n</pre> # Create a visual diagram fig, ax = plt.subplots(figsize=(14, 10)) ax.axis('off')  # Define stages stages = [     (\"Raw Data\\n(CSV Files)\", \"patients.csv\\nencounters.csv\\nconditions.csv\\nobservations.csv\"),     (\"Medical Events\\n(List[MedicalEvent])\", f\"{sum(len(adapter.load_events([pid])) for pid in patient_ids[:5])} events\\n(sample)\"),     (\"Visit Groups\\n(Dict[str, List[Visit]])\", f\"{len(patient_visits)} patients\\n{sum(len(v) for v in patient_visits.values())} visits\"),     (\"Patient Sequences\\n(List[PatientSequence])\", f\"{len(sequences)} sequences\\nmin_visits \u2265 2\"),     (\"Encoded Sequences\\n(List[Dict])\", f\"{len(dataset_items)} items\\nwith labels\"),     (\"Batched Tensors\\n(PyTorch)\", f\"[{batch_size}, {batched_data['visit_codes'].shape[1]}, {batched_data['visit_codes'].shape[2]}]\"),     (\"Model Output\\n(Predictions)\", f\"[{batch_size}, 1]\\nprobabilities\") ]  y_positions = np.linspace(0.9, 0.1, len(stages))  for i, ((title, desc), y) in enumerate(zip(stages, y_positions)):     # Draw box     box = plt.Rectangle((0.2, y-0.05), 0.6, 0.08,                          facecolor='lightblue', edgecolor='black', linewidth=2)     ax.add_patch(box)          # Add text     ax.text(0.5, y+0.02, title, ha='center', va='center',             fontsize=12, fontweight='bold')     ax.text(0.5, y-0.02, desc, ha='center', va='center',             fontsize=9, style='italic')          # Draw arrow to next stage     if i &lt; len(stages) - 1:         ax.arrow(0.5, y-0.05, 0, -0.04, head_width=0.03, head_length=0.01,                 fc='black', ec='black', linewidth=2)  ax.set_xlim(0, 1) ax.set_ylim(0, 1) plt.title('EHR Data Transformation Pipeline for LSTM', fontsize=16, fontweight='bold', pad=20) plt.tight_layout() plt.show()  print(\"\\n\u2705 Pipeline visualization complete!\")"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#lstm-data-preparation-from-patient-sequences-to-model-inputs","title":"LSTM Data Preparation: From Patient Sequences to Model Inputs\u00b6","text":"<p>This notebook demonstrates how to prepare visit-grouped patient sequences for the LSTM baseline model.</p> <p>Prediction Objective: Binary classification - predicting if a patient has diabetes based on their EHR sequence.</p>"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#overview","title":"Overview\u00b6","text":"<p>We'll walk through the complete data transformation pipeline:</p> <ol> <li>Load processed sequences from the main exploration notebook</li> <li>Encode sequences to integer IDs using vocabulary</li> <li>Create labels for prediction task (diabetes detection)</li> <li>Prepare batches with proper padding and masking</li> <li>Visualize data shapes at each transformation step</li> <li>Create LSTM-ready tensors for model input</li> </ol> <p>See <code>data_shape_transformations.md</code> for detailed documentation of all shape changes.</p>"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#1-load-and-prepare-data","title":"1. Load and Prepare Data\u00b6","text":"<p>We'll start by loading Synthea data and creating patient sequences (same as notebook 01).</p>"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#2-build-patient-sequences","title":"2. Build Patient Sequences\u00b6","text":"<p>Transform visit groups into structured patient sequences with vocabulary.</p>"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#3-examine-raw-patient-sequence","title":"3. Examine Raw Patient Sequence\u00b6","text":"<p>Let's look at a patient sequence before encoding.</p>"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#4-encode-sequences-for-lstm","title":"4. Encode Sequences for LSTM\u00b6","text":"<p>Transform string codes to integer IDs with proper padding and masking.</p>"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#5-create-labels-for-prediction-task","title":"5. Create Labels for Prediction Task\u00b6","text":"<p>Task: Binary classification - predict if patient has diabetes.</p> <p>We'll use SNOMED-CT codes for diabetes diagnosis.</p>"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#6-create-batched-tensors-for-lstm","title":"6. Create Batched Tensors for LSTM\u00b6","text":"<p>Convert to PyTorch tensors and demonstrate batching with proper padding.</p>"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#7-create-lstm-model-and-test-forward-pass","title":"7. Create LSTM Model and Test Forward Pass\u00b6","text":"<p>Instantiate the LSTM baseline model and run a forward pass to verify shapes.</p>"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#8-summary-complete-data-flow","title":"8. Summary: Complete Data Flow\u00b6","text":"<p>Let's visualize the complete transformation pipeline.</p>"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#conclusion","title":"Conclusion\u00b6","text":"<p>This notebook demonstrated the complete pipeline for preparing EHR sequences for the LSTM baseline model:</p> <ol> <li>\u2705 Loaded and grouped raw Synthea data into visits</li> <li>\u2705 Built patient sequences with vocabulary</li> <li>\u2705 Encoded sequences to integer IDs with padding/masking</li> <li>\u2705 Created labels for diabetes prediction task</li> <li>\u2705 Batched data into PyTorch tensors</li> <li>\u2705 Ran model forward pass to verify shapes</li> </ol>"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#key-takeaways","title":"Key Takeaways:\u00b6","text":"<ul> <li>Visit-level representation: Codes within each visit are aggregated (mean/sum/attention)</li> <li>Sequence-level modeling: LSTM captures temporal dependencies across visits</li> <li>Proper masking: Essential for handling variable-length sequences</li> <li>Shape transformations: From raw CSV \u2192 tensors \u2192 predictions</li> </ul>"},{"location":"notebooks/01_synthea_data_exploration/01a_lstm_data_preparation/#next-steps","title":"Next Steps:\u00b6","text":"<ul> <li>See <code>data_shape_transformations.md</code> for detailed shape documentation</li> <li>See <code>examples/train_lstm_baseline.py</code> for full training script</li> <li>Experiment with different prediction tasks (readmission, mortality, etc.)</li> <li>Try different model configurations (attention, bidirectional LSTM, etc.)</li> </ul>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/","title":"Data Shape Transformations: From Raw EHR to LSTM Input","text":"<p>This document provides a comprehensive reference for all data shape transformations in the EHR sequence modeling pipeline, from raw Synthea CSV files to LSTM model predictions.</p> <p>Prediction Task: Binary classification - predicting diabetes diagnosis from patient EHR sequences.</p>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Stage-by-Stage Transformations</li> <li>Detailed Shape Specifications</li> <li>Memory Considerations</li> <li>Common Pitfalls</li> </ol>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#overview","title":"Overview","text":""},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#pipeline-summary","title":"Pipeline Summary","text":"<pre><code>Raw CSV Files\n    \u2193 SyntheaAdapter.load_events()\nList[MedicalEvent]\n    \u2193 VisitGrouper.group_events()\nDict[str, List[Visit]]\n    \u2193 PatientSequenceBuilder.build_sequences()\nList[PatientSequence]\n    \u2193 PatientSequenceBuilder.encode_sequence()\nEncoded Sequences (with padding)\n    \u2193 Add labels + collate_fn()\nBatched PyTorch Tensors\n    \u2193 LSTM Model forward()\nPredictions\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#key-dimensions","title":"Key Dimensions","text":"Symbol Meaning Typical Value <code>N</code> Number of patients 50-10000 <code>V</code> Number of visits per patient 2-100 <code>C</code> Number of codes per visit 1-50 <code>B</code> Batch size 16-128 <code>E</code> Embedding dimension 128-512 <code>H</code> Hidden dimension 256-1024 <code>vocab_size</code> Vocabulary size 500-5000"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#stage-by-stage-transformations","title":"Stage-by-Stage Transformations","text":""},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#stage-1-raw-csv-files-medical-events","title":"Stage 1: Raw CSV Files \u2192 Medical Events","text":"<p>Input: CSV files on disk - <code>patients.csv</code> - <code>encounters.csv</code> - <code>conditions.csv</code> - <code>observations.csv</code> - <code>medications.csv</code> - <code>procedures.csv</code></p> <p>Transformation: <code>SyntheaAdapter.load_events(patient_ids)</code></p> <p>Output: <code>List[MedicalEvent]</code></p> <pre><code># Data structure\nMedicalEvent(\n    patient_id: str,           # UUID\n    timestamp: datetime,       # Timezone-naive\n    code: str,                 # Medical code (SNOMED-CT, LOINC, RxNorm, etc.)\n    code_type: str,            # 'diagnosis', 'lab', 'medication', 'procedure'\n    encounter_id: Optional[str],\n    metadata: Optional[Dict]\n)\n\n# Shape\nList with length = total number of events across all patients\nExample: 4,372 events for 50 patients\n</code></pre> <p>Key Points: - Events are sorted by timestamp - All timestamps are timezone-naive for consistency - Each event represents a single medical code occurrence</p>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#stage-2-medical-events-visit-groups","title":"Stage 2: Medical Events \u2192 Visit Groups","text":"<p>Input: <code>List[MedicalEvent]</code></p> <p>Transformation: <code>VisitGrouper.group_events(events, patient_id)</code></p> <p>Output: <code>Dict[str, List[Visit]]</code></p> <pre><code># Data structure\n{\n    patient_id_1: [Visit_1, Visit_2, ..., Visit_n1],\n    patient_id_2: [Visit_1, Visit_2, ..., Visit_n2],\n    ...\n}\n\nVisit(\n    visit_id: str,\n    patient_id: str,\n    timestamp: datetime,\n    encounter_id: Optional[str],\n    codes_by_type: Dict[str, List[str]],  # e.g., {'diagnosis': [...], 'lab': [...]}\n    codes_flat: List[str],\n    metadata: Optional[Dict]\n)\n\n# Shape\nDict with:\n  - Keys: N patient IDs (strings)\n  - Values: Lists of Visit objects\n  - Total visits: sum(len(visits) for visits in dict.values())\n\nExample: \n  - 50 patients\n  - 421 total visits\n  - Average 8.4 visits per patient\n</code></pre> <p>Key Points: - Visits group events that occur within the same encounter or time window - <code>codes_by_type</code> preserves semantic structure - <code>codes_flat</code> provides simple list for basic models - Visit timestamps represent the start of the visit</p>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#stage-3-visit-groups-patient-sequences","title":"Stage 3: Visit Groups \u2192 Patient Sequences","text":"<p>Input: <code>Dict[str, List[Visit]]</code></p> <p>Transformation: <code>PatientSequenceBuilder.build_sequences(patient_visits, min_visits=2)</code></p> <p>Output: <code>List[PatientSequence]</code></p> <pre><code># Data structure\nPatientSequence(\n    patient_id: str,\n    visits: List[Visit],\n    sequence_length: int,\n    metadata: Optional[Dict]\n)\n\n# Shape\nList with length = number of patients with &gt;= min_visits\nExample: 42 sequences (filtered from 50 patients)\n\n# Each sequence contains:\n  - visits: List[Visit] with length = sequence_length\n  - sequence_length: int (number of visits)\n</code></pre> <p>Key Points: - Filters out patients with insufficient visits - Visits are chronologically ordered - Sequences can have variable length - No padding at this stage</p>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#stage-4-patient-sequences-encoded-sequences","title":"Stage 4: Patient Sequences \u2192 Encoded Sequences","text":"<p>Input: <code>PatientSequence</code></p> <p>Transformation: <code>PatientSequenceBuilder.encode_sequence(sequence, return_tensors=False)</code></p> <p>Output: <code>Dict[str, Any]</code></p> <pre><code># Data structure\n{\n    'patient_id': str,\n    'visit_codes': List[List[int]],      # [num_visits, max_codes_per_visit]\n    'visit_mask': List[List[int]],       # [num_visits, max_codes_per_visit]\n    'sequence_mask': List[int],          # [num_visits]\n    'time_deltas': List[float],          # [num_visits - 1]\n    'sequence_length': int\n}\n\n# Shape details\nvisit_codes: [V, C]\n  - V = min(sequence.sequence_length, max_visits)\n  - C = max_codes_per_visit\n  - Values: integer IDs from vocabulary (0 = [PAD], 1 = [UNK], 2+ = codes)\n  - Padded with 0s\n\nvisit_mask: [V, C]\n  - Same shape as visit_codes\n  - Values: 1 for real codes, 0 for padding\n  - Used to ignore padding in aggregation\n\nsequence_mask: [V]\n  - Values: 1 for real visits, 0 for padding visits\n  - Used to handle variable-length sequences in LSTM\n\ntime_deltas: [V-1]\n  - Time between consecutive visits in days\n  - Padded with 0.0 for padding visits\n\n# Example with max_visits=50, max_codes_per_visit=100\nvisit_codes: [50, 100]     # 5,000 integers\nvisit_mask: [50, 100]      # 5,000 integers\nsequence_mask: [50]        # 50 integers\ntime_deltas: [49]          # 49 floats\n</code></pre> <p>Key Points: - Codes converted from strings to integer IDs via vocabulary - Padding applied to standardize dimensions - Masks track real vs. padded data - Most recent visits kept if sequence exceeds <code>max_visits</code></p>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#stage-5-encoded-sequences-labeled-dataset","title":"Stage 5: Encoded Sequences \u2192 Labeled Dataset","text":"<p>Input: <code>List[PatientSequence]</code></p> <p>Transformation: Add labels based on prediction task</p> <p>Output: <code>List[Dict]</code></p> <pre><code># Data structure\n[\n    {\n        'patient_id': str,\n        'visit_codes': List[List[int]],\n        'visit_mask': List[List[int]],\n        'sequence_mask': List[int],\n        'time_deltas': List[float],\n        'label': int  # 0 or 1 for binary classification\n    },\n    ...\n]\n\n# Shape\nList with length = number of sequences\nEach item is a dict with encoded sequence + label\n\n# Label creation (diabetes example)\ndiabetes_codes = {'44054006', '46635009', '73211009', ...}\nlabel = 1 if any code in diabetes_codes appears in any visit else 0\n\n# Example label distribution\nTotal sequences: 42\nPositive (has diabetes): 8 (19.0%)\nNegative (no diabetes): 34 (81.0%)\n</code></pre> <p>Key Points: - Labels derived from medical codes in the sequence - For diabetes: check if any visit contains diabetes diagnosis code - Other tasks: readmission (time-based), mortality (death_date), etc. - Labels can be binary, multi-class, or continuous</p>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#stage-6-labeled-dataset-batched-tensors","title":"Stage 6: Labeled Dataset \u2192 Batched Tensors","text":"<p>Input: <code>List[Dict]</code> (dataset items)</p> <p>Transformation: <code>collate_fn(batch)</code> in DataLoader</p> <p>Output: <code>Dict[str, torch.Tensor]</code></p> <pre><code># Data structure\n{\n    'visit_codes': torch.Tensor,    # [B, V_max, C_max]\n    'visit_mask': torch.Tensor,     # [B, V_max, C_max]\n    'sequence_mask': torch.Tensor,  # [B, V_max]\n    'labels': torch.Tensor          # [B, 1]\n}\n\n# Shape details\nvisit_codes: [B, V_max, C_max]\n  - B = batch_size (e.g., 32)\n  - V_max = max number of visits in batch\n  - C_max = max number of codes per visit in batch\n  - dtype: torch.long\n  - Values: 0 (padding) or vocabulary IDs\n\nvisit_mask: [B, V_max, C_max]\n  - Same shape as visit_codes\n  - dtype: torch.bool\n  - Values: True for real codes, False for padding\n\nsequence_mask: [B, V_max]\n  - dtype: torch.bool\n  - Values: True for real visits, False for padding\n\nlabels: [B, 1]\n  - dtype: torch.float32\n  - Values: 0.0 or 1.0 for binary classification\n\n# Example with batch_size=32\nvisit_codes: [32, 45, 87]      # 125,280 values\nvisit_mask: [32, 45, 87]       # 125,280 values\nsequence_mask: [32, 45]        # 1,440 values\nlabels: [32, 1]                # 32 values\n\n# Memory footprint (batch_size=32)\nvisit_codes: ~977 KB (int64)\nvisit_mask: ~122 KB (bool)\nsequence_mask: ~1.4 KB (bool)\nlabels: ~0.1 KB (float32)\nTotal: ~1.1 MB per batch\n</code></pre> <p>Key Points: - Dynamic padding: <code>V_max</code> and <code>C_max</code> determined by batch contents - Efficient packing: only pad to max in current batch, not global max - Masks essential for proper gradient computation - DataLoader handles batching automatically</p>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#stage-7-batched-tensors-model-output","title":"Stage 7: Batched Tensors \u2192 Model Output","text":"<p>Input: Batched tensors from Stage 6</p> <p>Transformation: <code>model.forward(visit_codes, visit_mask, sequence_mask)</code></p> <p>Output: <code>Dict[str, torch.Tensor]</code></p> <pre><code># Model architecture flow\nvisit_codes [B, V, C]\n    \u2193 Embedding layer\ncode_embeddings [B, V, C, E]\n    \u2193 Visit encoder (aggregation)\nvisit_vectors [B, V, E]\n    \u2193 LSTM\nlstm_output [B, V, H]\n    \u2193 Take final hidden state\nfinal_hidden [B, H]\n    \u2193 Linear + activation\npredictions [B, 1]\n\n# Output structure\n{\n    'logits': torch.Tensor,         # [B, 1]\n    'predictions': torch.Tensor,    # [B, 1]\n    'hidden_states': torch.Tensor   # [B, V, H] (if return_hidden=True)\n}\n\n# Shape details\nlogits: [B, 1]\n  - dtype: torch.float32\n  - Raw predictions before activation\n  - Range: (-\u221e, +\u221e)\n\npredictions: [B, 1]\n  - dtype: torch.float32\n  - After sigmoid activation\n  - Range: (0, 1) - interpreted as probabilities\n  - P(patient has diabetes)\n\nhidden_states: [B, V, H]\n  - dtype: torch.float32\n  - LSTM hidden states for each visit\n  - Can be used for attention, interpretability, etc.\n\n# Example with batch_size=32, hidden_dim=256\nlogits: [32, 1]           # 32 values\npredictions: [32, 1]      # 32 values (probabilities)\nhidden_states: [32, 45, 256]  # 368,640 values\n\n# Memory footprint\nhidden_states: ~1.4 MB (float32)\n</code></pre> <p>Key Points: - Embedding layer maps code IDs to dense vectors - Visit encoder aggregates codes within each visit (mean/sum/attention) - LSTM captures temporal dependencies across visits - Final prediction from last hidden state - Sigmoid activation for binary classification</p>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#detailed-shape-specifications","title":"Detailed Shape Specifications","text":""},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#vocabulary","title":"Vocabulary","text":"<pre><code>vocab: Dict[str, int]\n  - Keys: Medical codes (strings)\n  - Values: Integer IDs\n  - Special tokens:\n    \u2022 [PAD]: 0\n    \u2022 [UNK]: 1\n    \u2022 [MASK]: 2\n    \u2022 [CLS]: 3\n    \u2022 [SEP]: 4\n  - Regular codes: 5, 6, 7, ...\n\nExample:\n{\n    '[PAD]': 0,\n    '[UNK]': 1,\n    '[MASK]': 2,\n    '[CLS]': 3,\n    '[SEP]': 4,\n    '44054006': 5,  # Type 2 diabetes\n    '8302-2': 6,    # Body height\n    ...\n}\n\nTypical size: 500-5000 codes\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#visit-object","title":"Visit Object","text":"<pre><code>Visit:\n  - visit_id: str (UUID)\n  - patient_id: str (UUID)\n  - timestamp: datetime (timezone-naive)\n  - encounter_id: Optional[str]\n  - codes_by_type: Dict[str, List[str]]\n    \u2022 Keys: 'diagnosis', 'lab', 'medication', 'procedure'\n    \u2022 Values: Lists of code strings\n  - codes_flat: List[str]\n    \u2022 Flattened list of all codes\n  - metadata: Optional[Dict]\n\nMethods:\n  - num_codes() -&gt; int\n  - get_all_codes() -&gt; List[str]\n  - get_ordered_codes(type_order) -&gt; List[str]\n\nExample:\nVisit(\n    visit_id='abc-123',\n    patient_id='patient-456',\n    timestamp=datetime(2024, 5, 5),\n    codes_by_type={\n        'diagnosis': ['44054006', '73211009'],\n        'lab': ['8302-2', '29463-7', '8867-4'],\n        'medication': ['197361']\n    },\n    codes_flat=['44054006', '73211009', '8302-2', '29463-7', '8867-4', '197361']\n)\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#patientsequence-object","title":"PatientSequence Object","text":"<pre><code>PatientSequence:\n  - patient_id: str\n  - visits: List[Visit]\n  - sequence_length: int (len(visits))\n  - metadata: Optional[Dict]\n\nMethods:\n  - get_code_sequence(use_semantic_order: bool) -&gt; List[List[str]]\n  - get_flat_code_sequence() -&gt; List[str]\n  - get_time_deltas() -&gt; List[float]\n\nExample:\nPatientSequence(\n    patient_id='patient-456',\n    visits=[Visit_1, Visit_2, ..., Visit_10],\n    sequence_length=10,\n    metadata={'age': 45, 'gender': 'M'}\n)\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#memory-considerations","title":"Memory Considerations","text":""},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#per-sequence-memory","title":"Per-Sequence Memory","text":"<p>For a single encoded sequence with <code>max_visits=50</code>, <code>max_codes_per_visit=100</code>:</p> <pre><code>visit_codes:    50 \u00d7 100 \u00d7 8 bytes (int64)   = 40 KB\nvisit_mask:     50 \u00d7 100 \u00d7 1 byte (bool)     = 5 KB\nsequence_mask:  50 \u00d7 1 byte (bool)           = 50 bytes\ntime_deltas:    49 \u00d7 4 bytes (float32)       = 196 bytes\nTotal:                                       \u2248 45 KB per sequence\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#per-batch-memory","title":"Per-Batch Memory","text":"<p>For a batch of 32 sequences:</p> <pre><code>visit_codes:    32 \u00d7 50 \u00d7 100 \u00d7 8 bytes     = 1.28 MB\nvisit_mask:     32 \u00d7 50 \u00d7 100 \u00d7 1 byte      = 160 KB\nsequence_mask:  32 \u00d7 50 \u00d7 1 byte            = 1.6 KB\nlabels:         32 \u00d7 1 \u00d7 4 bytes            = 128 bytes\nTotal:                                      \u2248 1.44 MB per batch\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#model-memory","title":"Model Memory","text":"<p>For LSTM baseline (small) with <code>vocab_size=1000</code>, <code>embedding_dim=128</code>, <code>hidden_dim=256</code>:</p> <pre><code>Embedding:      1000 \u00d7 128 \u00d7 4 bytes        = 512 KB\nLSTM:           ~500K parameters            = 2 MB\nLinear:         256 \u00d7 1 \u00d7 4 bytes           = 1 KB\nTotal parameters:                           \u2248 2.5 MB\n\nForward pass (batch_size=32):\n  - Embeddings:     32 \u00d7 50 \u00d7 100 \u00d7 128     = 20.48 MB\n  - Visit vectors:  32 \u00d7 50 \u00d7 128           = 819 KB\n  - LSTM hidden:    32 \u00d7 50 \u00d7 256           = 1.64 MB\n  - Gradients:      ~2\u00d7 forward pass        = 45 MB\nTotal:                                      \u2248 70 MB per batch\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#scaling-considerations","title":"Scaling Considerations","text":"Dataset Size Sequences Batches (B=32) Memory Training Time Small 100 4 ~6 MB Minutes Medium 1,000 32 ~50 MB Hours Large 10,000 313 ~450 MB Days Very Large 100,000 3,125 ~4.5 GB Weeks <p>Recommendations: - Use <code>max_visits=50</code> and <code>max_codes_per_visit=100</code> for most tasks - Reduce dimensions if memory-constrained - Use gradient accumulation for larger effective batch sizes - Consider mixed precision training (fp16) to reduce memory by 50%</p>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#1-forgetting-masks","title":"1. Forgetting Masks","text":"<p>Problem: Not using masks leads to incorrect aggregation and gradient computation.</p> <pre><code># \u274c Wrong - includes padding in mean\nvisit_vector = code_embeddings.mean(dim=1)\n\n# \u2705 Correct - masks out padding\nmasked_embeddings = code_embeddings * visit_mask.unsqueeze(-1)\nvisit_vector = masked_embeddings.sum(dim=1) / visit_mask.sum(dim=1, keepdim=True).clamp(min=1)\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#2-incorrect-padding-direction","title":"2. Incorrect Padding Direction","text":"<p>Problem: Padding at the beginning instead of the end.</p> <pre><code># \u274c Wrong - pads at start (shifts temporal order)\npadded = [PAD, PAD, code1, code2, code3]\n\n# \u2705 Correct - pads at end (preserves temporal order)\npadded = [code1, code2, code3, PAD, PAD]\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#3-timezone-issues","title":"3. Timezone Issues","text":"<p>Problem: Mixing timezone-aware and timezone-naive timestamps.</p> <pre><code># \u274c Wrong - causes comparison errors\ntimestamp1 = pd.to_datetime('2024-01-01')  # naive\ntimestamp2 = pd.to_datetime('2024-01-01').tz_localize('UTC')  # aware\n\n# \u2705 Correct - all timestamps naive\ntimestamp = pd.to_datetime(row['DATE']).tz_localize(None)\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#4-dictionary-vs-dataclass-access","title":"4. Dictionary vs. Dataclass Access","text":"<p>Problem: Treating dataclasses as dictionaries.</p> <pre><code># \u274c Wrong - PatientSequence is a dataclass\npatient_id = sequence['patient_id']\n\n# \u2705 Correct - use attribute access\npatient_id = sequence.patient_id\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#5-batch-dimension-confusion","title":"5. Batch Dimension Confusion","text":"<p>Problem: Forgetting batch dimension in reshaping.</p> <pre><code># \u274c Wrong - loses batch structure\nembeddings = embeddings.view(-1, embedding_dim)\n\n# \u2705 Correct - preserves batch\nembeddings = embeddings.view(batch_size, num_visits, -1, embedding_dim)\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#6-variable-length-handling","title":"6. Variable Length Handling","text":"<p>Problem: Not using packed sequences for efficiency.</p> <pre><code># \u274c Inefficient - processes padding\nlstm_output, _ = lstm(visit_vectors)\n\n# \u2705 Efficient - skips padding\nlengths = sequence_mask.sum(dim=1).cpu()\npacked = nn.utils.rnn.pack_padded_sequence(visit_vectors, lengths, batch_first=True, enforce_sorted=False)\npacked_output, _ = lstm(packed)\nlstm_output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n</code></pre>"},{"location":"notebooks/01_synthea_data_exploration/data_shape_transformations/#summary","title":"Summary","text":"<p>This document provides a complete reference for data shapes throughout the EHR sequence modeling pipeline. Key points:</p> <ol> <li>Consistent shapes: All transformations maintain clear input/output contracts</li> <li>Proper masking: Essential for handling variable-length sequences</li> <li>Memory efficiency: Dynamic padding and packed sequences reduce waste</li> <li>Type safety: Clear distinction between lists, dicts, dataclasses, and tensors</li> <li>Scalability: Pipeline handles datasets from 100 to 100,000+ patients</li> </ol> <p>For implementation details, see: - <code>01_synthea_data_exploration.ipynb</code> - Data loading and exploration - <code>01a_lstm_data_preparation.ipynb</code> - LSTM input preparation - <code>examples/train_lstm_baseline.py</code> - Full training pipeline - <code>src/ehrsequencing/models/lstm_baseline.py</code> - Model architecture</p>"},{"location":"notebooks/02_survival_analysis/","title":"Survival Analysis Notebooks","text":"<p>This directory contains educational notebooks demonstrating survival analysis methods for EHR sequence modeling.</p>"},{"location":"notebooks/02_survival_analysis/#overview","title":"Overview","text":"<p>Survival analysis extends traditional prediction tasks by modeling when events occur, not just if they occur. This temporal dimension is crucial for clinical decision-making, risk stratification, and resource planning.</p>"},{"location":"notebooks/02_survival_analysis/#contents","title":"Contents","text":""},{"location":"notebooks/02_survival_analysis/#notebooks","title":"Notebooks","text":""},{"location":"notebooks/02_survival_analysis/#01_discrete_time_survival_lstmipynb","title":"<code>01_discrete_time_survival_lstm.ipynb</code>","text":"<p>Comprehensive introduction to discrete-time survival analysis with LSTMs.</p> <p>Topics Covered:</p> <ol> <li>Understanding the C-index</li> <li>Mathematical definition and intuition</li> <li>How it handles censoring</li> <li>Interpretation in clinical context</li> <li> <p>Comparison with AUC and other metrics</p> </li> <li> <p>Research Questions &amp; Clinical Applications</p> </li> <li>Disease progression modeling (CKD, cancer, heart failure)</li> <li>Treatment response prediction</li> <li>Adverse event forecasting</li> <li>Competing risks analysis</li> <li> <p>Resource utilization planning</p> </li> <li> <p>Data Labeling Strategies</p> </li> <li>Translating clinical questions into survival labels</li> <li>Defining events, time origins, and censoring</li> <li>Avoiding temporal leakage</li> <li>Handling different censoring types</li> <li> <p>Real-world example: CKD progression (Stage 3 \u2192 4)</p> </li> <li> <p>Complete Workflow</p> </li> <li>Data loading and preprocessing</li> <li>Synthetic outcome generation</li> <li>Model training with discrete-time survival LSTM</li> <li>Evaluation with C-index</li> <li>Visualization and interpretation</li> </ol> <p>Key Concepts: - Discrete-time hazard functions - Visit-level survival modeling - Concordance index (C-index) - Temporal leakage prevention - Synthetic data generation for testing</p> <p>Prerequisites: Basic understanding of LSTMs and EHR data structures (see <code>../01_synthea_data_exploration/</code>)</p>"},{"location":"notebooks/02_survival_analysis/#scripts","title":"Scripts","text":""},{"location":"notebooks/02_survival_analysis/#validate_survival_modelpy","title":"<code>validate_survival_model.py</code>","text":"<p>Quick validation script for testing survival models with flexible configurations.</p> <p>Features: - Patient subsampling: Test with small datasets locally (e.g., 200 patients) or full datasets on cloud GPUs - Example display: Show patient sequences with their survival outcomes - Model complexity control: Choose from small/medium/large model sizes - Memory estimation: Estimate GPU memory requirements before training - Outcome quality checks: Validate synthetic outcomes have correct risk-time correlation</p> <p>Usage Examples:</p> <pre><code># Quick local validation with 200 patients\npython validate_survival_model.py --max-patients 200 --show-examples 5\n\n# Full dataset on cloud GPU with large model\npython validate_survival_model.py --max-patients None --model-size large\n\n# Memory estimation only (no training)\npython validate_survival_model.py --estimate-memory-only\n\n# Check synthetic outcome quality\npython validate_survival_model.py --max-patients 200 --check-outcomes\n\n# Small model for fast iteration\npython validate_survival_model.py --max-patients 100 --model-size small --epochs 5\n</code></pre> <p>Command-Line Options: - <code>--max-patients</code>: Number of patients (or \"None\" for all) - <code>--model-size</code>: Model complexity (small/medium/large) - <code>--show-examples</code>: Number of example sequences to display - <code>--check-outcomes</code>: Run diagnostic checks on synthetic outcomes - <code>--estimate-memory-only</code>: Only estimate memory (skip training) - <code>--epochs</code>, <code>--batch-size</code>, <code>--lr</code>: Training hyperparameters - <code>--device</code>: Device to use (auto/cpu/mps/cuda)</p> <p>When to Use: - Local testing: Use <code>--max-patients 200</code> with <code>--model-size small</code> for quick iteration - Cloud training: Use <code>--max-patients None</code> with <code>--model-size large</code> for best performance - Debugging: Use <code>--show-examples</code> and <code>--check-outcomes</code> to validate data quality - Planning: Use <code>--estimate-memory-only</code> to check if your system can handle the model</p>"},{"location":"notebooks/02_survival_analysis/#why-survival-analysis","title":"Why Survival Analysis?","text":""},{"location":"notebooks/02_survival_analysis/#traditional-classification-vs-survival-analysis","title":"Traditional Classification vs. Survival Analysis","text":"<p>Binary Classification: <pre><code>Question: \"Will patient develop disease X?\"\nAnswer: Yes/No\nProblem: Ignores timing, treats all events as equal\n</code></pre></p> <p>Survival Analysis: <pre><code>Question: \"When will patient develop disease X?\"\nAnswer: Time-to-event + risk trajectory\nAdvantages: \n  \u2022 Captures temporal dynamics\n  \u2022 Handles censoring naturally\n  \u2022 Enables risk stratification over time\n  \u2022 Supports causal inference\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/#clinical-impact","title":"Clinical Impact","text":"<ol> <li>Early Intervention: Identify high-risk patients before events occur</li> <li>Resource Planning: Predict when patients will need specific treatments</li> <li>Personalized Medicine: Tailor interventions based on individual risk trajectories</li> <li>Clinical Trials: Account for dropout and variable follow-up times</li> </ol>"},{"location":"notebooks/02_survival_analysis/#survival-model-types","title":"Survival Model Types","text":""},{"location":"notebooks/02_survival_analysis/#discrete-time-models","title":"Discrete-Time Models","text":"<ul> <li>When to use: Events occur at visits (discrete time points)</li> <li>Examples: Disease progression at clinic visits, treatment response at follow-ups</li> <li>Model: LSTM predicting hazard at each visit</li> <li>Loss: Discrete-time survival loss (negative log-likelihood)</li> <li>Notebook: <code>01_discrete_time_survival_lstm.ipynb</code></li> </ul>"},{"location":"notebooks/02_survival_analysis/#continuous-time-models","title":"Continuous-Time Models","text":"<ul> <li>When to use: Events can occur at any time</li> <li>Examples: Time to death, time to hospital admission</li> <li>Model: Cox proportional hazards with neural networks</li> <li>Loss: Partial likelihood or ranking loss</li> <li>Notebook: Coming soon</li> </ul>"},{"location":"notebooks/02_survival_analysis/#competing-risks-models","title":"Competing Risks Models","text":"<ul> <li>When to use: Multiple event types, occurrence of one precludes others</li> <li>Examples: Death from different causes, disease vs. dropout</li> <li>Model: Multi-output survival model</li> <li>Loss: Cause-specific hazards</li> <li>Notebook: Coming soon</li> </ul>"},{"location":"notebooks/02_survival_analysis/#multi-state-models","title":"Multi-State Models","text":"<ul> <li>When to use: Complex disease trajectories with multiple states</li> <li>Examples: CKD stages, cancer progression, treatment pathways</li> <li>Model: Transition-based survival model</li> <li>Loss: State-specific hazards</li> <li>Notebook: Coming soon</li> </ul>"},{"location":"notebooks/02_survival_analysis/#key-evaluation-metrics","title":"Key Evaluation Metrics","text":""},{"location":"notebooks/02_survival_analysis/#concordance-index-c-index","title":"Concordance Index (C-index)","text":"<ul> <li>What: Probability model correctly ranks pairs by risk</li> <li>Range: 0 to 1 (0.5 = random, 1.0 = perfect)</li> <li>Advantages: Handles censoring, interpretable, standard metric</li> <li>Use: Primary metric for survival models</li> </ul>"},{"location":"notebooks/02_survival_analysis/#brier-score","title":"Brier Score","text":"<ul> <li>What: Mean squared error between predicted and observed survival</li> <li>Range: 0 to 1 (lower is better)</li> <li>Advantages: Calibration-focused, time-specific</li> <li>Use: Assess prediction accuracy at specific time points</li> </ul>"},{"location":"notebooks/02_survival_analysis/#integrated-brier-score-ibs","title":"Integrated Brier Score (IBS)","text":"<ul> <li>What: Average Brier score over time</li> <li>Advantages: Single summary metric, accounts for entire follow-up</li> <li>Use: Compare models across full time range</li> </ul>"},{"location":"notebooks/02_survival_analysis/#time-dependent-auc","title":"Time-Dependent AUC","text":"<ul> <li>What: AUC for binary outcome at specific time point</li> <li>Advantages: Familiar interpretation, time-specific discrimination</li> <li>Use: Assess discrimination at clinically relevant time points</li> </ul>"},{"location":"notebooks/02_survival_analysis/#common-pitfalls-and-solutions","title":"Common Pitfalls and Solutions","text":""},{"location":"notebooks/02_survival_analysis/#pitfall-1-temporal-leakage","title":"Pitfall 1: Temporal Leakage","text":"<p>Problem: Using future information to predict the past</p> <p>Example: <pre><code># \u2717 WRONG: Using all visit codes to predict event at visit 5\nfeatures = all_codes_in_sequence\n\n# \u2713 CORRECT: Only use codes up to current visit\nfeatures = codes_up_to_visit_t\n</code></pre></p> <p>Solution: Respect temporal ordering, truncate sequences at prediction time</p>"},{"location":"notebooks/02_survival_analysis/#pitfall-2-ignoring-censoring","title":"Pitfall 2: Ignoring Censoring","text":"<p>Problem: Treating censored patients as non-events</p> <p>Example: <pre><code># \u2717 WRONG: Binary classification (ignores censoring)\nlabel = 1 if event_occurred else 0\n\n# \u2713 CORRECT: Survival label (includes censoring)\nlabel = (event_time, event_indicator)\n</code></pre></p> <p>Solution: Use survival-specific losses that handle censoring</p>"},{"location":"notebooks/02_survival_analysis/#pitfall-3-informative-censoring","title":"Pitfall 3: Informative Censoring","text":"<p>Problem: Censoring is related to outcome risk</p> <p>Example: <pre><code># \u2717 WRONG: Censoring sicker patients (informative)\nif patient_very_sick:\n    censored = True\n\n# \u2713 CORRECT: Administrative censoring (independent)\nif end_of_study:\n    censored = True\n</code></pre></p> <p>Solution: Use administrative censoring or model censoring mechanism</p>"},{"location":"notebooks/02_survival_analysis/#pitfall-4-wrong-time-origin","title":"Pitfall 4: Wrong Time Origin","text":"<p>Problem: Starting clock at wrong time point</p> <p>Example: <pre><code># \u2717 WRONG: Starting at birth for adult-onset disease\ntime_origin = birth_date\n\n# \u2713 CORRECT: Starting at disease diagnosis\ntime_origin = diagnosis_date\n</code></pre></p> <p>Solution: Define clinically meaningful time origin</p>"},{"location":"notebooks/02_survival_analysis/#data-requirements","title":"Data Requirements","text":""},{"location":"notebooks/02_survival_analysis/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>Longitudinal data: Multiple observations per patient over time</li> <li>Event definition: Clear criteria for outcome of interest</li> <li>Time information: Timestamps for events and censoring</li> <li>Censoring indicators: Flag for observed vs. censored events</li> </ul>"},{"location":"notebooks/02_survival_analysis/#recommended-data-elements","title":"Recommended Data Elements","text":"<ul> <li>Demographics: Age, sex, race/ethnicity</li> <li>Diagnoses: ICD codes with timestamps</li> <li>Procedures: CPT codes with timestamps</li> <li>Medications: Drug codes with start/stop dates</li> <li>Lab values: Results with timestamps</li> <li>Vital signs: Measurements with timestamps</li> </ul>"},{"location":"notebooks/02_survival_analysis/#data-quality-considerations","title":"Data Quality Considerations","text":"<ul> <li>Completeness: Sufficient follow-up time for events to occur</li> <li>Missingness: Handle missing data appropriately</li> <li>Coding accuracy: Validate event definitions</li> <li>Temporal resolution: Adequate granularity for research question</li> </ul>"},{"location":"notebooks/02_survival_analysis/#getting-started","title":"Getting Started","text":""},{"location":"notebooks/02_survival_analysis/#1-set-up-environment","title":"1. Set Up Environment","text":"<pre><code># Activate conda environment\nmamba activate ehrsequencing\n\n# Navigate to notebooks directory\ncd notebooks/02_survival_analysis/\n\n# Launch Jupyter\njupyter lab\n</code></pre>"},{"location":"notebooks/02_survival_analysis/#2-run-first-notebook","title":"2. Run First Notebook","text":"<p>Open <code>01_discrete_time_survival_lstm.ipynb</code> and run cells sequentially.</p>"},{"location":"notebooks/02_survival_analysis/#3-experiment","title":"3. Experiment","text":"<ul> <li>Modify synthetic outcome parameters</li> <li>Try different model architectures</li> <li>Visualize survival curves</li> <li>Compare with baseline models</li> </ul>"},{"location":"notebooks/02_survival_analysis/#4-apply-to-real-data","title":"4. Apply to Real Data","text":"<ul> <li>Define your clinical question</li> <li>Create appropriate labels</li> <li>Train and evaluate model</li> <li>Interpret results in clinical context</li> </ul>"},{"location":"notebooks/02_survival_analysis/#references","title":"References","text":""},{"location":"notebooks/02_survival_analysis/#foundational-papers","title":"Foundational Papers","text":"<ul> <li>Harrell et al. (1982): \"Evaluating the Yield of Medical Tests\" - Original C-index</li> <li>Cox (1972): \"Regression Models and Life-Tables\" - Cox proportional hazards</li> <li>Kalbfleisch &amp; Prentice (2002): \"The Statistical Analysis of Failure Time Data\" - Survival analysis textbook</li> </ul>"},{"location":"notebooks/02_survival_analysis/#deep-learning-for-survival","title":"Deep Learning for Survival","text":"<ul> <li>Lee et al. (2018): \"DeepHit: A Deep Learning Approach to Survival Analysis with Competing Risks\"</li> <li>Katzman et al. (2018): \"DeepSurv: Personalized Treatment Recommender System Using A Cox Proportional Hazards Deep Neural Network\"</li> <li>Kvamme et al. (2019): \"Time-to-Event Prediction with Neural Networks and Cox Regression\"</li> </ul>"},{"location":"notebooks/02_survival_analysis/#ehr-specific-applications","title":"EHR-Specific Applications","text":"<ul> <li>Rajkomar et al. (2018): \"Scalable and accurate deep learning with electronic health records\"</li> <li>Choi et al. (2016): \"RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism\"</li> </ul>"},{"location":"notebooks/02_survival_analysis/#our-documentation","title":"Our Documentation","text":"<ul> <li><code>../../docs/methods/causal-survival-analysis-1.md</code> - Temporal leakage and causal labels</li> <li><code>../../docs/methods/causal-survival-analysis-2.md</code> - Discrete-time survival derivation</li> </ul>"},{"location":"notebooks/02_survival_analysis/#next-steps","title":"Next Steps","text":"<p>After completing these notebooks, you'll be ready to:</p> <ol> <li>Apply to real clinical questions: Use your own EHR data</li> <li>Explore advanced models: Competing risks, multi-state models</li> <li>Add interpretability: Attention mechanisms, feature importance</li> <li>Integrate pretrained embeddings: Med2Vec, BEHRT (Phase 2)</li> <li>Deploy models: Production-ready survival prediction systems</li> </ol>"},{"location":"notebooks/02_survival_analysis/#questions-or-issues","title":"Questions or Issues?","text":"<ul> <li>Check <code>../../docs/methods/</code> for detailed methodology</li> <li>Review <code>../01_synthea_data_exploration/</code> for data pipeline basics</li> <li>See <code>../../examples/train_survival_lstm.py</code> for production training script</li> <li>Consult survival analysis textbooks for statistical foundations</li> </ul>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/","title":"Notebook: Discrete-Time Survival LSTM","text":"In\u00a0[\u00a0]: Copied! <pre>import sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# Add src to path\nsys.path.insert(0, str(Path.cwd().parent.parent / 'src'))\n\nfrom ehrsequencing.data.adapters.synthea import SyntheaAdapter\nfrom ehrsequencing.data.visit_grouper import VisitGrouper\nfrom ehrsequencing.data.sequence_builder import PatientSequenceBuilder\nfrom ehrsequencing.models.survival_lstm import DiscreteTimeSurvivalLSTM\nfrom ehrsequencing.models.losses import DiscreteTimeSurvivalLoss, concordance_index\nfrom ehrsequencing.synthetic.survival import DiscreteTimeSurvivalGenerator\n\n# Set random seeds\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Plotting style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n</pre> import sys import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import torch import torch.nn as nn from torch.utils.data import DataLoader from pathlib import Path from tqdm import tqdm  # Add src to path sys.path.insert(0, str(Path.cwd().parent.parent / 'src'))  from ehrsequencing.data.adapters.synthea import SyntheaAdapter from ehrsequencing.data.visit_grouper import VisitGrouper from ehrsequencing.data.sequence_builder import PatientSequenceBuilder from ehrsequencing.models.survival_lstm import DiscreteTimeSurvivalLSTM from ehrsequencing.models.losses import DiscreteTimeSurvivalLoss, concordance_index from ehrsequencing.synthetic.survival import DiscreteTimeSurvivalGenerator  # Set random seeds np.random.seed(42) torch.manual_seed(42)  # Plotting style sns.set_style('whitegrid') plt.rcParams['figure.figsize'] = (12, 6) In\u00a0[\u00a0]: Copied! <pre># Visualize C-index computation\ndef visualize_cindex_example():\n    \"\"\"\n    Show a simple example of C-index computation.\n    \"\"\"\n    # Example patients\n    patients = pd.DataFrame({\n        'Patient': ['A', 'B', 'C', 'D', 'E'],\n        'Event_Time': [5, 10, 15, 20, 12],\n        'Event': [1, 1, 0, 1, 1],  # 1=event, 0=censored\n        'Risk_Score': [0.8, 0.6, 0.4, 0.3, 0.7],  # Model predictions\n    })\n    \n    print(\"Example Patients:\")\n    print(patients)\n    print()\n    \n    # Compute C-index manually\n    concordant = 0\n    total = 0\n    \n    print(\"Comparable Pairs (i has event, i's time &lt; j's time):\")\n    print(\"-\" * 70)\n    \n    for i in range(len(patients)):\n        if patients.iloc[i]['Event'] == 0:\n            continue  # Skip censored as index case\n        \n        for j in range(len(patients)):\n            if i == j:\n                continue\n            \n            if patients.iloc[i]['Event_Time'] &lt; patients.iloc[j]['Event_Time']:\n                total += 1\n                is_concordant = patients.iloc[i]['Risk_Score'] &gt; patients.iloc[j]['Risk_Score']\n                concordant += int(is_concordant)\n                \n                status = \"\u2713 Concordant\" if is_concordant else \"\u2717 Discordant\"\n                print(f\"Pair ({patients.iloc[i]['Patient']}, {patients.iloc[j]['Patient']}): \"\n                      f\"Time ({patients.iloc[i]['Event_Time']}, {patients.iloc[j]['Event_Time']}), \"\n                      f\"Risk ({patients.iloc[i]['Risk_Score']:.1f}, {patients.iloc[j]['Risk_Score']:.1f}) \"\n                      f\"\u2192 {status}\")\n    \n    c_index = concordant / total if total &gt; 0 else 0.5\n    print()\n    print(f\"C-index = {concordant}/{total} = {c_index:.3f}\")\n    \n    # Visualize\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Timeline\n    for idx, row in patients.iterrows():\n        color = 'red' if row['Event'] == 1 else 'gray'\n        marker = 'X' if row['Event'] == 1 else 'o'\n        ax1.scatter(row['Event_Time'], idx, c=color, marker=marker, s=200, zorder=3)\n        ax1.text(row['Event_Time'] + 0.5, idx, row['Patient'], fontsize=12, va='center')\n    \n    ax1.set_yticks(range(len(patients)))\n    ax1.set_yticklabels(patients['Patient'])\n    ax1.set_xlabel('Time (visits)', fontsize=12)\n    ax1.set_ylabel('Patient', fontsize=12)\n    ax1.set_title('Event Timeline\\n(Red X = Event, Gray O = Censored)', fontsize=13)\n    ax1.grid(True, alpha=0.3)\n    \n    # Risk scores\n    colors = ['red' if e == 1 else 'gray' for e in patients['Event']]\n    ax2.barh(patients['Patient'], patients['Risk_Score'], color=colors, alpha=0.7)\n    ax2.set_xlabel('Predicted Risk Score', fontsize=12)\n    ax2.set_ylabel('Patient', fontsize=12)\n    ax2.set_title('Model Risk Predictions\\n(Higher = More Risk)', fontsize=13)\n    ax2.set_xlim(0, 1)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return c_index\n\nc_index = visualize_cindex_example()\n</pre> # Visualize C-index computation def visualize_cindex_example():     \"\"\"     Show a simple example of C-index computation.     \"\"\"     # Example patients     patients = pd.DataFrame({         'Patient': ['A', 'B', 'C', 'D', 'E'],         'Event_Time': [5, 10, 15, 20, 12],         'Event': [1, 1, 0, 1, 1],  # 1=event, 0=censored         'Risk_Score': [0.8, 0.6, 0.4, 0.3, 0.7],  # Model predictions     })          print(\"Example Patients:\")     print(patients)     print()          # Compute C-index manually     concordant = 0     total = 0          print(\"Comparable Pairs (i has event, i's time &lt; j's time):\")     print(\"-\" * 70)          for i in range(len(patients)):         if patients.iloc[i]['Event'] == 0:             continue  # Skip censored as index case                  for j in range(len(patients)):             if i == j:                 continue                          if patients.iloc[i]['Event_Time'] &lt; patients.iloc[j]['Event_Time']:                 total += 1                 is_concordant = patients.iloc[i]['Risk_Score'] &gt; patients.iloc[j]['Risk_Score']                 concordant += int(is_concordant)                                  status = \"\u2713 Concordant\" if is_concordant else \"\u2717 Discordant\"                 print(f\"Pair ({patients.iloc[i]['Patient']}, {patients.iloc[j]['Patient']}): \"                       f\"Time ({patients.iloc[i]['Event_Time']}, {patients.iloc[j]['Event_Time']}), \"                       f\"Risk ({patients.iloc[i]['Risk_Score']:.1f}, {patients.iloc[j]['Risk_Score']:.1f}) \"                       f\"\u2192 {status}\")          c_index = concordant / total if total &gt; 0 else 0.5     print()     print(f\"C-index = {concordant}/{total} = {c_index:.3f}\")          # Visualize     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))          # Timeline     for idx, row in patients.iterrows():         color = 'red' if row['Event'] == 1 else 'gray'         marker = 'X' if row['Event'] == 1 else 'o'         ax1.scatter(row['Event_Time'], idx, c=color, marker=marker, s=200, zorder=3)         ax1.text(row['Event_Time'] + 0.5, idx, row['Patient'], fontsize=12, va='center')          ax1.set_yticks(range(len(patients)))     ax1.set_yticklabels(patients['Patient'])     ax1.set_xlabel('Time (visits)', fontsize=12)     ax1.set_ylabel('Patient', fontsize=12)     ax1.set_title('Event Timeline\\n(Red X = Event, Gray O = Censored)', fontsize=13)     ax1.grid(True, alpha=0.3)          # Risk scores     colors = ['red' if e == 1 else 'gray' for e in patients['Event']]     ax2.barh(patients['Patient'], patients['Risk_Score'], color=colors, alpha=0.7)     ax2.set_xlabel('Predicted Risk Score', fontsize=12)     ax2.set_ylabel('Patient', fontsize=12)     ax2.set_title('Model Risk Predictions\\n(Higher = More Risk)', fontsize=13)     ax2.set_xlim(0, 1)          plt.tight_layout()     plt.show()          return c_index  c_index = visualize_cindex_example() In\u00a0[\u00a0]: Copied! <pre>def label_ckd_progression(patient_sequence):\n    \"\"\"\n    Label CKD progression from Stage 3 to Stage 4+.\n    \n    Returns:\n        event_time: Visit index of progression (or last visit if censored)\n        event_indicator: 1 if progression observed, 0 if censored\n    \"\"\"\n    stage3_codes = {'585.3', 'N18.3'}  # CKD Stage 3\n    stage4_codes = {'585.4', '585.5', 'N18.4', 'N18.5'}  # CKD Stage 4+\n    \n    # Find first Stage 3 visit (time origin)\n    time_origin = None\n    for i, visit in enumerate(patient_sequence.visits):\n        visit_codes = set(visit.get_all_codes())\n        if visit_codes &amp; stage3_codes:\n            time_origin = i\n            break\n    \n    if time_origin is None:\n        return None, None  # Patient never had Stage 3\n    \n    # Find first Stage 4+ visit after time origin\n    for i in range(time_origin + 1, len(patient_sequence.visits)):\n        visit_codes = set(patient_sequence.visits[i].get_all_codes())\n        if visit_codes &amp; stage4_codes:\n            # Event observed!\n            event_time = i - time_origin  # Visits since Stage 3\n            return event_time, 1\n    \n    # No progression observed \u2192 censored\n    event_time = len(patient_sequence.visits) - time_origin - 1\n    return event_time, 0\n\n# Example usage\nprint(\"Example: CKD Progression Labeling\")\nprint(\"=\"*50)\nprint(\"Patient Timeline:\")\nprint(\"  Visit 0: Hypertension (no CKD)\")\nprint(\"  Visit 1: CKD Stage 3 diagnosed \u2190 TIME ORIGIN\")\nprint(\"  Visit 2: CKD Stage 3 (stable)\")\nprint(\"  Visit 3: CKD Stage 4 diagnosed \u2190 EVENT\")\nprint(\"  Visit 4: CKD Stage 4 (continued)\")\nprint()\nprint(\"Label: event_time = 2 visits, event_indicator = 1\")\nprint(\"(Progression occurred 2 visits after Stage 3 diagnosis)\")\n</pre> def label_ckd_progression(patient_sequence):     \"\"\"     Label CKD progression from Stage 3 to Stage 4+.          Returns:         event_time: Visit index of progression (or last visit if censored)         event_indicator: 1 if progression observed, 0 if censored     \"\"\"     stage3_codes = {'585.3', 'N18.3'}  # CKD Stage 3     stage4_codes = {'585.4', '585.5', 'N18.4', 'N18.5'}  # CKD Stage 4+          # Find first Stage 3 visit (time origin)     time_origin = None     for i, visit in enumerate(patient_sequence.visits):         visit_codes = set(visit.get_all_codes())         if visit_codes &amp; stage3_codes:             time_origin = i             break          if time_origin is None:         return None, None  # Patient never had Stage 3          # Find first Stage 4+ visit after time origin     for i in range(time_origin + 1, len(patient_sequence.visits)):         visit_codes = set(patient_sequence.visits[i].get_all_codes())         if visit_codes &amp; stage4_codes:             # Event observed!             event_time = i - time_origin  # Visits since Stage 3             return event_time, 1          # No progression observed \u2192 censored     event_time = len(patient_sequence.visits) - time_origin - 1     return event_time, 0  # Example usage print(\"Example: CKD Progression Labeling\") print(\"=\"*50) print(\"Patient Timeline:\") print(\"  Visit 0: Hypertension (no CKD)\") print(\"  Visit 1: CKD Stage 3 diagnosed \u2190 TIME ORIGIN\") print(\"  Visit 2: CKD Stage 3 (stable)\") print(\"  Visit 3: CKD Stage 4 diagnosed \u2190 EVENT\") print(\"  Visit 4: CKD Stage 4 (continued)\") print() print(\"Label: event_time = 2 visits, event_indicator = 1\") print(\"(Progression occurred 2 visits after Stage 3 diagnosis)\") In\u00a0[\u00a0]: Copied! <pre># Configuration: Dataset size and memory management\n# =====================================================\n# For local testing on systems with limited memory (e.g., MPS with ~20GB limit),\n# use a smaller subset. For full training, use all patients on a cloud GPU.\n\n# Option 1: Load pre-generated synthetic outcomes (FAST - recommended for iteration)\n# Run: python test_synthetic_outcomes.py --max-patients 200 --save synthetic_outcomes.pt\n\n# LOAD_PREGENERATED = None  # Set to 'synthetic_outcomes.pt' to load pre-validated data\nLOAD_PREGENERATED = 'synthetic_outcomes.pt'\n\n# Option 2: Generate fresh data (slower, but ensures latest generator code)\nMAX_PATIENTS = 200  # Use 200 for local testing, None for full training on RunPods\n\nif LOAD_PREGENERATED and Path(LOAD_PREGENERATED).exists():\n    print(\"=\"*70)\n    print(\"LOADING PRE-GENERATED SYNTHETIC OUTCOMES\")\n    print(\"=\"*70)\n    print(f\"\\nLoading from: {LOAD_PREGENERATED}\")\n    \n    # Load with weights_only=False for custom classes (safe - we created this file)\n    saved_data = torch.load(LOAD_PREGENERATED, weights_only=False)\n    sequences = saved_data['sequences']\n    outcome = saved_data['outcome']\n    \n    # Reconstruct SurvivalOutcome object\n    from ehrsequencing.synthetic.survival import SurvivalOutcome\n    outcome = SurvivalOutcome(\n        event_times=outcome['event_times'],\n        event_indicators=outcome['event_indicators'],\n        risk_scores=outcome['risk_scores'],\n        metadata=outcome['metadata']\n    )\n    \n    # Build vocabulary from loaded sequences\n    builder = PatientSequenceBuilder()\n    builder.build_vocabulary([seq.visits for seq in sequences])\n    \n    print(f\"\u2713 Loaded {len(sequences)} sequences\")\n    print(f\"\u2713 Vocabulary size: {builder.vocabulary_size}\")\n    print(f\"\u2713 Event rate: {outcome.event_indicators.float().mean():.1%}\")\n    print(f\"\u2713 Config: {saved_data['config']}\")\n    print(\"\\n\ud83d\udca1 To regenerate, set LOAD_PREGENERATED = None\")\n    \nelse:\n    # Generate fresh data\n    print(\"=\"*70)\n    print(\"GENERATING FRESH DATA\")\n    print(\"=\"*70)\n    \n    # Load example data\n    data_dir = Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'large_cohort_1000'\n\n    print(\"\\nLoading Synthea data...\")\n    adapter = SyntheaAdapter(data_dir)\n    events = adapter.load_events()\n    print(f\"Loaded {len(events)} events\")\n\n    # Group into visits\n    print(\"\\nGrouping events into visits...\")\n    grouper = VisitGrouper(time_window_hours=24)\n    visits = grouper.group_events(events)\n    print(f\"Created {len(visits)} visits\")\n\n    # Group by patient\n    from collections import defaultdict\n    visits_by_patient = defaultdict(list)\n    for visit in visits:\n        visits_by_patient[visit.patient_id].append(visit)\n\n    # Sort visits by timestamp\n    for patient_id in visits_by_patient:\n        visits_by_patient[patient_id].sort(key=lambda v: v.timestamp)\n\n    total_patients = len(visits_by_patient)\n    print(f\"\\nData organized for {total_patients} patients\")\n\n    # Subsample patients if MAX_PATIENTS is set\n    if MAX_PATIENTS is not None and total_patients &gt; MAX_PATIENTS:\n        print(f\"\\n\u26a0\ufe0f  Subsampling to {MAX_PATIENTS} patients for local testing\")\n        print(f\"   (Set MAX_PATIENTS=None for full training on cloud GPU)\")\n        \n        # Randomly sample patient IDs\n        np.random.seed(42)\n        sampled_patient_ids = np.random.choice(\n            list(visits_by_patient.keys()), \n            size=MAX_PATIENTS, \n            replace=False\n        )\n        visits_by_patient = {\n            pid: visits_by_patient[pid] \n            for pid in sampled_patient_ids\n        }\n        print(f\"   Using {len(visits_by_patient)} patients\")\n\n    # Build vocabulary first (before building sequences)\n    print(\"\\nBuilding vocabulary...\")\n    builder = PatientSequenceBuilder()\n    builder.build_vocabulary(list(visits_by_patient.values()))\n    print(f\"Vocabulary size: {builder.vocabulary_size}\")\n\n    # Build sequences\n    print(\"\\nBuilding patient sequences...\")\n    sequences = builder.build_sequences(list(visits_by_patient.values()), min_visits=2)\n    print(f\"Built {len(sequences)} sequences\")\n    \n    print(\"\\n\ud83d\udca1 To skip this step next time:\")\n    print(\"   1. Run: python test_synthetic_outcomes.py --max-patients 200 --save synthetic_outcomes.pt\")\n    print(\"   2. Set: LOAD_PREGENERATED = 'synthetic_outcomes.pt'\")\n\n# Memory estimation using refactored utility\nfrom ehrsequencing.utils import estimate_memory_from_sequences\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"MEMORY ESTIMATION\")\nprint(\"=\"*70)\n\nmem_est = estimate_memory_from_sequences(\n    sequences, \n    vocab_size=builder.vocabulary_size,\n    embedding_dim=128,\n    hidden_dim=256,\n    batch_size=32,\n    verbose=True\n)\n\nprint(\"=\"*70)\n</pre> # Configuration: Dataset size and memory management # ===================================================== # For local testing on systems with limited memory (e.g., MPS with ~20GB limit), # use a smaller subset. For full training, use all patients on a cloud GPU.  # Option 1: Load pre-generated synthetic outcomes (FAST - recommended for iteration) # Run: python test_synthetic_outcomes.py --max-patients 200 --save synthetic_outcomes.pt  # LOAD_PREGENERATED = None  # Set to 'synthetic_outcomes.pt' to load pre-validated data LOAD_PREGENERATED = 'synthetic_outcomes.pt'  # Option 2: Generate fresh data (slower, but ensures latest generator code) MAX_PATIENTS = 200  # Use 200 for local testing, None for full training on RunPods  if LOAD_PREGENERATED and Path(LOAD_PREGENERATED).exists():     print(\"=\"*70)     print(\"LOADING PRE-GENERATED SYNTHETIC OUTCOMES\")     print(\"=\"*70)     print(f\"\\nLoading from: {LOAD_PREGENERATED}\")          # Load with weights_only=False for custom classes (safe - we created this file)     saved_data = torch.load(LOAD_PREGENERATED, weights_only=False)     sequences = saved_data['sequences']     outcome = saved_data['outcome']          # Reconstruct SurvivalOutcome object     from ehrsequencing.synthetic.survival import SurvivalOutcome     outcome = SurvivalOutcome(         event_times=outcome['event_times'],         event_indicators=outcome['event_indicators'],         risk_scores=outcome['risk_scores'],         metadata=outcome['metadata']     )          # Build vocabulary from loaded sequences     builder = PatientSequenceBuilder()     builder.build_vocabulary([seq.visits for seq in sequences])          print(f\"\u2713 Loaded {len(sequences)} sequences\")     print(f\"\u2713 Vocabulary size: {builder.vocabulary_size}\")     print(f\"\u2713 Event rate: {outcome.event_indicators.float().mean():.1%}\")     print(f\"\u2713 Config: {saved_data['config']}\")     print(\"\\n\ud83d\udca1 To regenerate, set LOAD_PREGENERATED = None\")      else:     # Generate fresh data     print(\"=\"*70)     print(\"GENERATING FRESH DATA\")     print(\"=\"*70)          # Load example data     data_dir = Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'large_cohort_1000'      print(\"\\nLoading Synthea data...\")     adapter = SyntheaAdapter(data_dir)     events = adapter.load_events()     print(f\"Loaded {len(events)} events\")      # Group into visits     print(\"\\nGrouping events into visits...\")     grouper = VisitGrouper(time_window_hours=24)     visits = grouper.group_events(events)     print(f\"Created {len(visits)} visits\")      # Group by patient     from collections import defaultdict     visits_by_patient = defaultdict(list)     for visit in visits:         visits_by_patient[visit.patient_id].append(visit)      # Sort visits by timestamp     for patient_id in visits_by_patient:         visits_by_patient[patient_id].sort(key=lambda v: v.timestamp)      total_patients = len(visits_by_patient)     print(f\"\\nData organized for {total_patients} patients\")      # Subsample patients if MAX_PATIENTS is set     if MAX_PATIENTS is not None and total_patients &gt; MAX_PATIENTS:         print(f\"\\n\u26a0\ufe0f  Subsampling to {MAX_PATIENTS} patients for local testing\")         print(f\"   (Set MAX_PATIENTS=None for full training on cloud GPU)\")                  # Randomly sample patient IDs         np.random.seed(42)         sampled_patient_ids = np.random.choice(             list(visits_by_patient.keys()),              size=MAX_PATIENTS,              replace=False         )         visits_by_patient = {             pid: visits_by_patient[pid]              for pid in sampled_patient_ids         }         print(f\"   Using {len(visits_by_patient)} patients\")      # Build vocabulary first (before building sequences)     print(\"\\nBuilding vocabulary...\")     builder = PatientSequenceBuilder()     builder.build_vocabulary(list(visits_by_patient.values()))     print(f\"Vocabulary size: {builder.vocabulary_size}\")      # Build sequences     print(\"\\nBuilding patient sequences...\")     sequences = builder.build_sequences(list(visits_by_patient.values()), min_visits=2)     print(f\"Built {len(sequences)} sequences\")          print(\"\\n\ud83d\udca1 To skip this step next time:\")     print(\"   1. Run: python test_synthetic_outcomes.py --max-patients 200 --save synthetic_outcomes.pt\")     print(\"   2. Set: LOAD_PREGENERATED = 'synthetic_outcomes.pt'\")  # Memory estimation using refactored utility from ehrsequencing.utils import estimate_memory_from_sequences  print(\"\\n\" + \"=\"*70) print(\"MEMORY ESTIMATION\") print(\"=\"*70)  mem_est = estimate_memory_from_sequences(     sequences,      vocab_size=builder.vocabulary_size,     embedding_dim=128,     hidden_dim=256,     batch_size=32,     verbose=True )  print(\"=\"*70) In\u00a0[\u00a0]: Copied! <pre># Generate synthetic survival outcomes (if not already loaded)\nif LOAD_PREGENERATED and Path(LOAD_PREGENERATED).exists():\n    print(\"\u2713 Using pre-loaded synthetic outcomes\")\n    print(f\"  \u2022 Event rate: {outcome.event_indicators.float().mean():.1%}\")\n    print(f\"  \u2022 Median event/censoring time: {outcome.event_times.float().median():.1f} visits\")\n    print(f\"  \u2022 Mean risk score: {outcome.risk_scores.mean():.3f}\")\nelse:\n    print(\"Generating synthetic survival outcomes...\")\n    print(\"=\"*60)\n\n    generator = DiscreteTimeSurvivalGenerator(\n        censoring_rate=0.3,\n        risk_weights={'comorbidity': 0.4, 'frequency': 0.4, 'diversity': 0.2},\n        time_scale=0.3,\n        seed=42\n    )\n\n    outcome = generator.generate(sequences)\n\n    print(f\"\\nGenerated outcomes for {len(sequences)} patients\")\n    print(f\"Event rate: {outcome.event_indicators.float().mean():.1%}\")\n    print(f\"Median event/censoring time: {outcome.event_times.float().median():.1f} visits\")\n    print(f\"Mean risk score: {outcome.risk_scores.mean():.3f}\")\n\n# Visualize outcome distribution\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Event time distribution\naxes[0].hist(outcome.event_times[outcome.event_indicators == 1].numpy(), \n             bins=20, alpha=0.7, color='red', label='Events')\naxes[0].hist(outcome.event_times[outcome.event_indicators == 0].numpy(), \n             bins=20, alpha=0.7, color='gray', label='Censored')\naxes[0].set_xlabel('Time (visits)', fontsize=11)\naxes[0].set_ylabel('Count', fontsize=11)\naxes[0].set_title('Event Time Distribution', fontsize=12)\naxes[0].legend()\n\n# Risk score distribution\naxes[1].hist(outcome.risk_scores[outcome.event_indicators == 1].numpy(), \n             bins=20, alpha=0.7, color='red', label='Events')\naxes[1].hist(outcome.risk_scores[outcome.event_indicators == 0].numpy(), \n             bins=20, alpha=0.7, color='gray', label='Censored')\naxes[1].set_xlabel('Risk Score', fontsize=11)\naxes[1].set_ylabel('Count', fontsize=11)\naxes[1].set_title('Risk Score Distribution', fontsize=12)\naxes[1].legend()\n\n# Risk vs. Time scatter\ncolors = ['red' if e == 1 else 'gray' for e in outcome.event_indicators]\naxes[2].scatter(outcome.risk_scores.numpy(), outcome.event_times.numpy(), \n                c=colors, alpha=0.6, s=50)\naxes[2].set_xlabel('Risk Score', fontsize=11)\naxes[2].set_ylabel('Event Time (visits)', fontsize=11)\naxes[2].set_title('Risk vs. Event Time\\n(Red=Event, Gray=Censored)', fontsize=12)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udcca Key Observations:\")\nprint(\"  \u2022 Higher risk scores \u2192 earlier event times (negative correlation)\")\nprint(\"  \u2022 Censored patients have lower average risk\")\nprint(\"  \u2022 Event times follow exponential-like distribution\")\n</pre> # Generate synthetic survival outcomes (if not already loaded) if LOAD_PREGENERATED and Path(LOAD_PREGENERATED).exists():     print(\"\u2713 Using pre-loaded synthetic outcomes\")     print(f\"  \u2022 Event rate: {outcome.event_indicators.float().mean():.1%}\")     print(f\"  \u2022 Median event/censoring time: {outcome.event_times.float().median():.1f} visits\")     print(f\"  \u2022 Mean risk score: {outcome.risk_scores.mean():.3f}\") else:     print(\"Generating synthetic survival outcomes...\")     print(\"=\"*60)      generator = DiscreteTimeSurvivalGenerator(         censoring_rate=0.3,         risk_weights={'comorbidity': 0.4, 'frequency': 0.4, 'diversity': 0.2},         time_scale=0.3,         seed=42     )      outcome = generator.generate(sequences)      print(f\"\\nGenerated outcomes for {len(sequences)} patients\")     print(f\"Event rate: {outcome.event_indicators.float().mean():.1%}\")     print(f\"Median event/censoring time: {outcome.event_times.float().median():.1f} visits\")     print(f\"Mean risk score: {outcome.risk_scores.mean():.3f}\")  # Visualize outcome distribution fig, axes = plt.subplots(1, 3, figsize=(15, 4))  # Event time distribution axes[0].hist(outcome.event_times[outcome.event_indicators == 1].numpy(),               bins=20, alpha=0.7, color='red', label='Events') axes[0].hist(outcome.event_times[outcome.event_indicators == 0].numpy(),               bins=20, alpha=0.7, color='gray', label='Censored') axes[0].set_xlabel('Time (visits)', fontsize=11) axes[0].set_ylabel('Count', fontsize=11) axes[0].set_title('Event Time Distribution', fontsize=12) axes[0].legend()  # Risk score distribution axes[1].hist(outcome.risk_scores[outcome.event_indicators == 1].numpy(),               bins=20, alpha=0.7, color='red', label='Events') axes[1].hist(outcome.risk_scores[outcome.event_indicators == 0].numpy(),               bins=20, alpha=0.7, color='gray', label='Censored') axes[1].set_xlabel('Risk Score', fontsize=11) axes[1].set_ylabel('Count', fontsize=11) axes[1].set_title('Risk Score Distribution', fontsize=12) axes[1].legend()  # Risk vs. Time scatter colors = ['red' if e == 1 else 'gray' for e in outcome.event_indicators] axes[2].scatter(outcome.risk_scores.numpy(), outcome.event_times.numpy(),                  c=colors, alpha=0.6, s=50) axes[2].set_xlabel('Risk Score', fontsize=11) axes[2].set_ylabel('Event Time (visits)', fontsize=11) axes[2].set_title('Risk vs. Event Time\\n(Red=Event, Gray=Censored)', fontsize=12)  plt.tight_layout() plt.show()  print(\"\\n\ud83d\udcca Key Observations:\") print(\"  \u2022 Higher risk scores \u2192 earlier event times (negative correlation)\") print(\"  \u2022 Censored patients have lower average risk\") print(\"  \u2022 Event times follow exponential-like distribution\") In\u00a0[\u00a0]: Copied! <pre># DIAGNOSTIC: Check synthetic outcome quality\nprint(\"=\"*70)\nprint(\"DIAGNOSTIC: Synthetic Outcome Validation\")\nprint(\"=\"*70)\n\nimport scipy.stats as stats\n\n# Only look at patients with events (not censored)\nevent_mask = outcome.event_indicators == 1\nevent_risk_scores = outcome.risk_scores[event_mask].numpy()\nevent_times = outcome.event_times[event_mask].numpy()\n\ncorrelation, p_value = stats.pearsonr(event_risk_scores, event_times)\n\nprint(f\"\\nCorrelation between risk score and event time:\")\nprint(f\"  \u2022 Pearson r = {correlation:.3f} (p={p_value:.4f})\")\nprint(f\"  \u2022 Expected: NEGATIVE correlation (high risk \u2192 early events)\")\n\nif correlation &gt; 0:\n    print(f\"  \u26a0\ufe0f  WARNING: POSITIVE correlation detected!\")\n    print(f\"     This means high-risk patients have LATE events (inverse relationship)\")\n    print(f\"     The model will learn backwards \u2192 C-index &lt; 0.5\")\nelif correlation &lt; -0.3:\n    print(f\"  \u2705 Good: Strong negative correlation\")\n    print(f\"     High-risk patients have early events (correct relationship)\")\nelse:\n    print(f\"  \u26a0\ufe0f  Weak correlation: synthetic outcomes may be too random\")\n\n# Show some examples\nprint(f\"\\nExample patients (with events):\")\nprint(f\"{'Risk Score':&lt;12} {'Event Time':&lt;12} {'Expected':&lt;20}\")\nprint(\"-\" * 50)\nfor i in range(min(10, len(event_risk_scores))):\n    expected = \"Early\" if event_risk_scores[i] &gt; 0.6 else \"Late\"\n    actual = \"Early\" if event_times[i] &lt; np.median(event_times) else \"Late\"\n    match = \"\u2713\" if expected == actual else \"\u2717\"\n    print(f\"{event_risk_scores[i]:.3f}        {event_times[i]:&lt;12} {expected:&lt;10} (actual: {actual}) {match}\")\n\nprint(\"=\"*70)\n</pre> # DIAGNOSTIC: Check synthetic outcome quality print(\"=\"*70) print(\"DIAGNOSTIC: Synthetic Outcome Validation\") print(\"=\"*70)  import scipy.stats as stats  # Only look at patients with events (not censored) event_mask = outcome.event_indicators == 1 event_risk_scores = outcome.risk_scores[event_mask].numpy() event_times = outcome.event_times[event_mask].numpy()  correlation, p_value = stats.pearsonr(event_risk_scores, event_times)  print(f\"\\nCorrelation between risk score and event time:\") print(f\"  \u2022 Pearson r = {correlation:.3f} (p={p_value:.4f})\") print(f\"  \u2022 Expected: NEGATIVE correlation (high risk \u2192 early events)\")  if correlation &gt; 0:     print(f\"  \u26a0\ufe0f  WARNING: POSITIVE correlation detected!\")     print(f\"     This means high-risk patients have LATE events (inverse relationship)\")     print(f\"     The model will learn backwards \u2192 C-index &lt; 0.5\") elif correlation &lt; -0.3:     print(f\"  \u2705 Good: Strong negative correlation\")     print(f\"     High-risk patients have early events (correct relationship)\") else:     print(f\"  \u26a0\ufe0f  Weak correlation: synthetic outcomes may be too random\")  # Show some examples print(f\"\\nExample patients (with events):\") print(f\"{'Risk Score':&lt;12} {'Event Time':&lt;12} {'Expected':&lt;20}\") print(\"-\" * 50) for i in range(min(10, len(event_risk_scores))):     expected = \"Early\" if event_risk_scores[i] &gt; 0.6 else \"Late\"     actual = \"Early\" if event_times[i] &lt; np.median(event_times) else \"Late\"     match = \"\u2713\" if expected == actual else \"\u2717\"     print(f\"{event_risk_scores[i]:.3f}        {event_times[i]:&lt;12} {expected:&lt;10} (actual: {actual}) {match}\")  print(\"=\"*70) In\u00a0[\u00a0]: Copied! <pre># Create dataset\nclass SurvivalDataset:\n    def __init__(self, sequences, event_times, event_indicators):\n        self.sequences = sequences\n        self.event_times = event_times\n        self.event_indicators = event_indicators\n    \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        return {\n            'sequence': self.sequences[idx],\n            'event_time': self.event_times[idx],\n            'event_indicator': self.event_indicators[idx],\n        }\n\n# Create collate function\ndef create_collate_fn(builder):\n    def collate_fn(batch):\n        sequences = [item['sequence'] for item in batch]\n        event_times = torch.tensor([item['event_time'] for item in batch])\n        event_indicators = torch.tensor([item['event_indicator'] for item in batch])\n        \n        max_visits = max(len(seq.visits) for seq in sequences)\n        max_codes_per_visit = max(\n            max(visit.num_codes() for visit in seq.visits)\n            for seq in sequences\n        )\n        \n        batch_size = len(sequences)\n        \n        visit_codes = torch.zeros(batch_size, max_visits, max_codes_per_visit, dtype=torch.long)\n        visit_mask = torch.zeros(batch_size, max_visits, max_codes_per_visit, dtype=torch.bool)\n        sequence_mask = torch.zeros(batch_size, max_visits, dtype=torch.bool)\n        \n        for i, seq in enumerate(sequences):\n            num_visits = len(seq.visits)\n            sequence_mask[i, :num_visits] = True\n            \n            for j, visit in enumerate(seq.visits):\n                codes = visit.get_all_codes()\n                encoded_codes = [\n                    builder.vocab.get(code, builder.unk_id)\n                    for code in codes[:max_codes_per_visit]\n                ]\n                \n                num_codes = len(encoded_codes)\n                \n                if num_codes &gt; 0:\n                    visit_codes[i, j, :num_codes] = torch.tensor(encoded_codes)\n                    visit_mask[i, j, :num_codes] = True\n        \n        return {\n            'visit_codes': visit_codes,\n            'visit_mask': visit_mask,\n            'sequence_mask': sequence_mask,\n            'event_times': event_times,\n            'event_indicators': event_indicators,\n        }\n    return collate_fn\n\n# Split data\nfrom sklearn.model_selection import train_test_split\n\ntrain_sequences, val_sequences, train_times, val_times, train_indicators, val_indicators = train_test_split(\n    sequences, outcome.event_times, outcome.event_indicators, \n    test_size=0.2, random_state=42\n)\n\ntrain_dataset = SurvivalDataset(train_sequences, train_times, train_indicators)\nval_dataset = SurvivalDataset(val_sequences, val_times, val_indicators)\n\n# Adjust batch size based on dataset size\n# Smaller datasets can use smaller batches to reduce memory\nif len(train_dataset) &lt; 300:\n    batch_size = 16  # Smaller batch for local testing\n    print(f\"Using batch_size={batch_size} for small dataset (local testing)\")\nelse:\n    batch_size = 32  # Standard batch size for full training\n    print(f\"Using batch_size={batch_size} for full dataset\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=create_collate_fn(builder))\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=create_collate_fn(builder))\n\nprint(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\nprint(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n</pre> # Create dataset class SurvivalDataset:     def __init__(self, sequences, event_times, event_indicators):         self.sequences = sequences         self.event_times = event_times         self.event_indicators = event_indicators          def __len__(self):         return len(self.sequences)          def __getitem__(self, idx):         return {             'sequence': self.sequences[idx],             'event_time': self.event_times[idx],             'event_indicator': self.event_indicators[idx],         }  # Create collate function def create_collate_fn(builder):     def collate_fn(batch):         sequences = [item['sequence'] for item in batch]         event_times = torch.tensor([item['event_time'] for item in batch])         event_indicators = torch.tensor([item['event_indicator'] for item in batch])                  max_visits = max(len(seq.visits) for seq in sequences)         max_codes_per_visit = max(             max(visit.num_codes() for visit in seq.visits)             for seq in sequences         )                  batch_size = len(sequences)                  visit_codes = torch.zeros(batch_size, max_visits, max_codes_per_visit, dtype=torch.long)         visit_mask = torch.zeros(batch_size, max_visits, max_codes_per_visit, dtype=torch.bool)         sequence_mask = torch.zeros(batch_size, max_visits, dtype=torch.bool)                  for i, seq in enumerate(sequences):             num_visits = len(seq.visits)             sequence_mask[i, :num_visits] = True                          for j, visit in enumerate(seq.visits):                 codes = visit.get_all_codes()                 encoded_codes = [                     builder.vocab.get(code, builder.unk_id)                     for code in codes[:max_codes_per_visit]                 ]                                  num_codes = len(encoded_codes)                                  if num_codes &gt; 0:                     visit_codes[i, j, :num_codes] = torch.tensor(encoded_codes)                     visit_mask[i, j, :num_codes] = True                  return {             'visit_codes': visit_codes,             'visit_mask': visit_mask,             'sequence_mask': sequence_mask,             'event_times': event_times,             'event_indicators': event_indicators,         }     return collate_fn  # Split data from sklearn.model_selection import train_test_split  train_sequences, val_sequences, train_times, val_times, train_indicators, val_indicators = train_test_split(     sequences, outcome.event_times, outcome.event_indicators,      test_size=0.2, random_state=42 )  train_dataset = SurvivalDataset(train_sequences, train_times, train_indicators) val_dataset = SurvivalDataset(val_sequences, val_times, val_indicators)  # Adjust batch size based on dataset size # Smaller datasets can use smaller batches to reduce memory if len(train_dataset) &lt; 300:     batch_size = 16  # Smaller batch for local testing     print(f\"Using batch_size={batch_size} for small dataset (local testing)\") else:     batch_size = 32  # Standard batch size for full training     print(f\"Using batch_size={batch_size} for full dataset\")  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=create_collate_fn(builder)) val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=create_collate_fn(builder))  print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\") print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\") In\u00a0[8]: Copied! <pre># Create model\ndevice = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nmodel = DiscreteTimeSurvivalLSTM(\n    vocab_size=builder.vocabulary_size,\n    embedding_dim=128,\n    hidden_dim=256,\n    num_layers=2,\n    dropout=0.3,\n).to(device)\n\ncriterion = DiscreteTimeSurvivalLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n</pre> # Create model device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') print(f\"Using device: {device}\")  model = DiscreteTimeSurvivalLSTM(     vocab_size=builder.vocabulary_size,     embedding_dim=128,     hidden_dim=256,     num_layers=2,     dropout=0.3, ).to(device)  criterion = DiscreteTimeSurvivalLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\") <pre>Model parameters: 1,105,537\n</pre> In\u00a0[9]: Copied! <pre># Training loop\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    \n    for batch in tqdm(dataloader, desc='Training'):\n        visit_codes = batch['visit_codes'].to(device)\n        visit_mask = batch['visit_mask'].to(device)\n        sequence_mask = batch['sequence_mask'].to(device)\n        event_times = batch['event_times'].to(device)\n        event_indicators = batch['event_indicators'].to(device)\n        \n        hazards = model(visit_codes, visit_mask, sequence_mask)\n        loss = criterion(hazards, event_times, event_indicators, sequence_mask)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_risk_scores = []\n    all_event_times = []\n    all_event_indicators = []\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc='Evaluating'):\n            visit_codes = batch['visit_codes'].to(device)\n            visit_mask = batch['visit_mask'].to(device)\n            sequence_mask = batch['sequence_mask'].to(device)\n            event_times = batch['event_times'].to(device)\n            event_indicators = batch['event_indicators'].to(device)\n            \n            hazards = model(visit_codes, visit_mask, sequence_mask)\n            loss = criterion(hazards, event_times, event_indicators, sequence_mask)\n            total_loss += loss.item()\n            \n            # Use early hazards (first few visits) as risk score\n            # High-risk patients should show elevated hazard early\n            # This avoids length bias and captures baseline risk\n            batch_size = hazards.shape[0]\n            risk_scores_batch = []\n            \n            # Use mean of first 10 visits (or all if fewer)\n            horizon = 10\n            \n            for i in range(batch_size):\n                # Get valid hazards for this patient\n                valid_mask = sequence_mask[i].bool()\n                patient_hazards = hazards[i][valid_mask]\n                \n                # Take mean of early hazards (up to horizon)\n                early_hazards = patient_hazards[:min(horizon, len(patient_hazards))]\n                risk_score = early_hazards.mean()\n                risk_scores_batch.append(risk_score)\n            \n            risk_scores = torch.stack(risk_scores_batch)\n            \n            all_risk_scores.append(risk_scores.cpu())\n            all_event_times.append(event_times.cpu())\n            all_event_indicators.append(event_indicators.cpu())\n    \n    avg_loss = total_loss / len(dataloader)\n    \n    # Concatenate risk scores (1D tensors - no dimension mismatch)\n    all_risk_scores = torch.cat(all_risk_scores, dim=0)\n    all_event_times = torch.cat(all_event_times, dim=0)\n    all_event_indicators = torch.cat(all_event_indicators, dim=0)\n    \n    # Compute C-index using risk scores instead of full hazard tensors\n    c_index = compute_cindex_from_risks(all_risk_scores, all_event_times, all_event_indicators)\n    \n    return avg_loss, c_index\n\ndef compute_cindex_from_risks(risk_scores, event_times, event_indicators):\n    \"\"\"\n    Compute C-index from risk scores (avoids tensor shape issues).\n    \n    Args:\n        risk_scores: 1D tensor of risk scores [batch_size]\n        event_times: 1D tensor of event times [batch_size]\n        event_indicators: 1D tensor of event indicators [batch_size]\n    \n    Returns:\n        C-index value\n    \"\"\"\n    batch_size = len(risk_scores)\n    \n    concordant = 0\n    total = 0\n    \n    for i in range(batch_size):\n        # Only use patients with observed events as index cases\n        if event_indicators[i] == 0:\n            continue\n        \n        for j in range(batch_size):\n            if i == j:\n                continue\n            \n            # Compare pairs where times are different\n            if event_times[j] &gt; event_times[i]:\n                total += 1\n                # Concordant: patient with earlier event has higher risk\n                if risk_scores[i] &gt; risk_scores[j]:\n                    concordant += 1\n    \n    if total == 0:\n        return 0.5  # No comparable pairs\n    \n    return concordant / total\n\n# Train for a few epochs\nnum_epochs = 10\nhistory = {'train_loss': [], 'val_loss': [], 'val_c_index': []}\n\nprint(\"\\nStarting training...\\n\")\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, val_c_index = evaluate(model, val_loader, criterion, device)\n    \n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['val_c_index'].append(val_c_index)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n          f\"Train Loss={train_loss:.4f}, \"\n          f\"Val Loss={val_loss:.4f}, \"\n          f\"Val C-index={val_c_index:.4f}\")\n\nprint(\"\\n\u2705 Training complete!\")\n</pre> # Training loop def train_epoch(model, dataloader, criterion, optimizer, device):     model.train()     total_loss = 0          for batch in tqdm(dataloader, desc='Training'):         visit_codes = batch['visit_codes'].to(device)         visit_mask = batch['visit_mask'].to(device)         sequence_mask = batch['sequence_mask'].to(device)         event_times = batch['event_times'].to(device)         event_indicators = batch['event_indicators'].to(device)                  hazards = model(visit_codes, visit_mask, sequence_mask)         loss = criterion(hazards, event_times, event_indicators, sequence_mask)                  optimizer.zero_grad()         loss.backward()         optimizer.step()                  total_loss += loss.item()          return total_loss / len(dataloader)  def evaluate(model, dataloader, criterion, device):     model.eval()     total_loss = 0     all_risk_scores = []     all_event_times = []     all_event_indicators = []          with torch.no_grad():         for batch in tqdm(dataloader, desc='Evaluating'):             visit_codes = batch['visit_codes'].to(device)             visit_mask = batch['visit_mask'].to(device)             sequence_mask = batch['sequence_mask'].to(device)             event_times = batch['event_times'].to(device)             event_indicators = batch['event_indicators'].to(device)                          hazards = model(visit_codes, visit_mask, sequence_mask)             loss = criterion(hazards, event_times, event_indicators, sequence_mask)             total_loss += loss.item()                          # Use early hazards (first few visits) as risk score             # High-risk patients should show elevated hazard early             # This avoids length bias and captures baseline risk             batch_size = hazards.shape[0]             risk_scores_batch = []                          # Use mean of first 10 visits (or all if fewer)             horizon = 10                          for i in range(batch_size):                 # Get valid hazards for this patient                 valid_mask = sequence_mask[i].bool()                 patient_hazards = hazards[i][valid_mask]                                  # Take mean of early hazards (up to horizon)                 early_hazards = patient_hazards[:min(horizon, len(patient_hazards))]                 risk_score = early_hazards.mean()                 risk_scores_batch.append(risk_score)                          risk_scores = torch.stack(risk_scores_batch)                          all_risk_scores.append(risk_scores.cpu())             all_event_times.append(event_times.cpu())             all_event_indicators.append(event_indicators.cpu())          avg_loss = total_loss / len(dataloader)          # Concatenate risk scores (1D tensors - no dimension mismatch)     all_risk_scores = torch.cat(all_risk_scores, dim=0)     all_event_times = torch.cat(all_event_times, dim=0)     all_event_indicators = torch.cat(all_event_indicators, dim=0)          # Compute C-index using risk scores instead of full hazard tensors     c_index = compute_cindex_from_risks(all_risk_scores, all_event_times, all_event_indicators)          return avg_loss, c_index  def compute_cindex_from_risks(risk_scores, event_times, event_indicators):     \"\"\"     Compute C-index from risk scores (avoids tensor shape issues).          Args:         risk_scores: 1D tensor of risk scores [batch_size]         event_times: 1D tensor of event times [batch_size]         event_indicators: 1D tensor of event indicators [batch_size]          Returns:         C-index value     \"\"\"     batch_size = len(risk_scores)          concordant = 0     total = 0          for i in range(batch_size):         # Only use patients with observed events as index cases         if event_indicators[i] == 0:             continue                  for j in range(batch_size):             if i == j:                 continue                          # Compare pairs where times are different             if event_times[j] &gt; event_times[i]:                 total += 1                 # Concordant: patient with earlier event has higher risk                 if risk_scores[i] &gt; risk_scores[j]:                     concordant += 1          if total == 0:         return 0.5  # No comparable pairs          return concordant / total  # Train for a few epochs num_epochs = 10 history = {'train_loss': [], 'val_loss': [], 'val_c_index': []}  print(\"\\nStarting training...\\n\") for epoch in range(num_epochs):     train_loss = train_epoch(model, train_loader, criterion, optimizer, device)     val_loss, val_c_index = evaluate(model, val_loader, criterion, device)          history['train_loss'].append(train_loss)     history['val_loss'].append(val_loss)     history['val_c_index'].append(val_c_index)          print(f\"Epoch {epoch+1}/{num_epochs}: \"           f\"Train Loss={train_loss:.4f}, \"           f\"Val Loss={val_loss:.4f}, \"           f\"Val C-index={val_c_index:.4f}\")  print(\"\\n\u2705 Training complete!\") <pre>\nStarting training...\n\n</pre> <pre>Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:13&lt;00:00,  1.40s/it]\nEvaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01&lt;00:00,  1.82it/s]\n</pre> <pre>Epoch 1/10: Train Loss=7.6608, Val Loss=3.3192, Val C-index=0.5045\n</pre> <pre>Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:17&lt;00:00,  1.78s/it]\nEvaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:02&lt;00:00,  1.23it/s]\n</pre> <pre>Epoch 2/10: Train Loss=3.5123, Val Loss=3.3205, Val C-index=0.6365\n</pre> <pre>Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:16&lt;00:00,  1.62s/it]\nEvaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00,  7.95it/s]\n</pre> <pre>Epoch 3/10: Train Loss=3.1047, Val Loss=2.4947, Val C-index=0.6546\n</pre> <pre>Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:14&lt;00:00,  1.48s/it]\nEvaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:03&lt;00:00,  1.16s/it]\n</pre> <pre>Epoch 4/10: Train Loss=3.0311, Val Loss=2.5275, Val C-index=0.6872\n</pre> <pre>Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:23&lt;00:00,  2.32s/it]\nEvaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:05&lt;00:00,  1.78s/it]\n</pre> <pre>Epoch 5/10: Train Loss=2.9595, Val Loss=2.4355, Val C-index=0.6890\n</pre> <pre>Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:16&lt;00:00,  1.69s/it]\nEvaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01&lt;00:00,  2.01it/s]\n</pre> <pre>Epoch 6/10: Train Loss=2.9655, Val Loss=2.3821, Val C-index=0.6890\n</pre> <pre>Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:12&lt;00:00,  1.22s/it]\nEvaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01&lt;00:00,  1.80it/s]\n</pre> <pre>Epoch 7/10: Train Loss=2.7914, Val Loss=2.4265, Val C-index=0.6980\n</pre> <pre>Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:10&lt;00:00,  1.07s/it]\nEvaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01&lt;00:00,  2.80it/s]\n</pre> <pre>Epoch 8/10: Train Loss=2.6731, Val Loss=2.3695, Val C-index=0.6908\n</pre> <pre>Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:09&lt;00:00,  1.09it/s]\nEvaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00,  5.68it/s]\n</pre> <pre>Epoch 9/10: Train Loss=2.5414, Val Loss=2.5032, Val C-index=0.6510\n</pre> <pre>Training: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:14&lt;00:00,  1.46s/it]\nEvaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:03&lt;00:00,  1.00s/it]</pre> <pre>Epoch 10/10: Train Loss=2.3698, Val Loss=2.4428, Val C-index=0.6691\n\n\u2705 Training complete!\n</pre> <pre>\n</pre> In\u00a0[10]: Copied! <pre># Visualize training history\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Loss curves\nepochs = range(1, num_epochs + 1)\nax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\nax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\nax1.set_xlabel('Epoch', fontsize=12)\nax1.set_ylabel('Loss', fontsize=12)\nax1.set_title('Training and Validation Loss', fontsize=13)\nax1.legend(fontsize=11)\nax1.grid(True, alpha=0.3)\n\n# C-index curve\nax2.plot(epochs, history['val_c_index'], 'g-', linewidth=2)\nax2.axhline(y=0.5, color='gray', linestyle='--', label='Random (0.5)')\nax2.set_xlabel('Epoch', fontsize=12)\nax2.set_ylabel('C-index', fontsize=12)\nax2.set_title('Validation C-index', fontsize=13)\nax2.set_ylim(0, 1)\nax2.legend(fontsize=11)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n\ud83d\udcc8 Final Results:\")\nprint(f\"  \u2022 Best C-index: {max(history['val_c_index']):.4f}\")\nprint(f\"  \u2022 Final C-index: {history['val_c_index'][-1]:.4f}\")\nprint(f\"  \u2022 Improvement over random: {(history['val_c_index'][-1] - 0.5) / 0.5 * 100:.1f}%\")\n</pre> # Visualize training history fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))  # Loss curves epochs = range(1, num_epochs + 1) ax1.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2) ax1.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2) ax1.set_xlabel('Epoch', fontsize=12) ax1.set_ylabel('Loss', fontsize=12) ax1.set_title('Training and Validation Loss', fontsize=13) ax1.legend(fontsize=11) ax1.grid(True, alpha=0.3)  # C-index curve ax2.plot(epochs, history['val_c_index'], 'g-', linewidth=2) ax2.axhline(y=0.5, color='gray', linestyle='--', label='Random (0.5)') ax2.set_xlabel('Epoch', fontsize=12) ax2.set_ylabel('C-index', fontsize=12) ax2.set_title('Validation C-index', fontsize=13) ax2.set_ylim(0, 1) ax2.legend(fontsize=11) ax2.grid(True, alpha=0.3)  plt.tight_layout() plt.show()  print(f\"\\n\ud83d\udcc8 Final Results:\") print(f\"  \u2022 Best C-index: {max(history['val_c_index']):.4f}\") print(f\"  \u2022 Final C-index: {history['val_c_index'][-1]:.4f}\") print(f\"  \u2022 Improvement over random: {(history['val_c_index'][-1] - 0.5) / 0.5 * 100:.1f}%\") <pre>\n\ud83d\udcc8 Final Results:\n  \u2022 Best C-index: 0.6980\n  \u2022 Final C-index: 0.6691\n  \u2022 Improvement over random: 33.8%\n</pre>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#discrete-time-survival-analysis-with-lstms","title":"Discrete-Time Survival Analysis with LSTMs\u00b6","text":"<p>This notebook demonstrates how to apply discrete-time survival analysis to EHR sequences using LSTMs. We'll cover:</p> <ol> <li>Understanding the C-index: What it measures and how it's computed</li> <li>Research Questions: Clinical applications and progression modeling</li> <li>Data Labeling Strategies: Translating clinical questions into survival labels</li> <li>Complete Workflow: From raw data to trained model</li> </ol>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#background","title":"Background\u00b6","text":"<p>Why Survival Analysis for EHR Data?</p> <p>Traditional binary classification asks: \"Will this patient develop disease X?\"</p> <p>Survival analysis asks: \"When will this patient develop disease X?\"</p> <p>This temporal dimension is crucial for:</p> <ul> <li>Risk stratification: Identify high-risk patients who need intervention soon</li> <li>Resource planning: Predict when patients will need specific treatments</li> <li>Clinical trials: Account for censoring (patients lost to follow-up)</li> <li>Causal inference: Avoid temporal leakage by respecting time ordering</li> </ul>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#part-1-understanding-the-c-index-concordance-index","title":"Part 1: Understanding the C-index (Concordance Index)\u00b6","text":""},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#what-is-the-c-index","title":"What is the C-index?\u00b6","text":"<p>The concordance index (C-index) measures how well a survival model ranks patients by risk.</p> <p>Intuition: If patient A has an event before patient B, the model should assign patient A a higher risk score.</p>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#mathematical-definition","title":"Mathematical Definition\u00b6","text":"<p>For all pairs of patients $(i, j)$ where:</p> <ul> <li>Patient $i$ has an observed event (not censored)</li> <li>Patient $i$'s event time $T_i &lt; T_j$ (earlier event)</li> </ul> <p>The pair is concordant if: $\\text{risk}_i &gt; \\text{risk}_j$</p> <p>$$ \\text{C-index} = \\frac{\\text{concordant pairs}}{\\text{comparable pairs}} $$</p> <p>Interpretation:</p> <ul> <li>C-index = 1.0: Perfect ranking (all pairs concordant)</li> <li>C-index = 0.5: Random ranking (coin flip)</li> <li>C-index = 0.0: Inverse ranking (all pairs discordant)</li> </ul>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#c-index-in-discrete-time-survival","title":"C-index in Discrete-Time Survival\u00b6","text":"<p>In discrete-time models, we predict hazard at each visit:</p> <ul> <li>$h_t$ = probability of event at visit $t$ given survival up to $t$</li> </ul> <p>Risk score = cumulative hazard = $\\sum_{t=1}^{T} h_t$</p> <p>Higher cumulative hazard \u2192 higher risk \u2192 expect earlier event</p>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#why-not-use-auc","title":"Why Not Use AUC?\u00b6","text":"<p>AUC (Area Under ROC Curve) is for binary classification at a fixed time point.</p> <p>C-index handles:</p> <ul> <li>Variable event times: Patients have events at different visits</li> <li>Censoring: Some patients are lost to follow-up</li> <li>Ranking: Focuses on relative risk, not absolute probabilities</li> </ul>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#key-insights-from-example","title":"Key Insights from Example\u00b6","text":"<ol> <li>Patient C is censored (gray circle) \u2192 not used as index case</li> <li>Patient A has earliest event (time=5) \u2192 should have highest risk</li> <li>Concordant pairs: Model correctly ranks patient with earlier event as higher risk</li> <li>Discordant pairs: Model incorrectly ranks patient with later event as higher risk</li> </ol> <p>Clinical Interpretation: A C-index of 0.7-0.8 means the model can correctly identify which of two patients will have an event first about 70-80% of the time.</p>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#part-2-research-questions-and-clinical-applications","title":"Part 2: Research Questions and Clinical Applications\u00b6","text":"<p>Survival models can answer many types of clinical questions. Here's a taxonomy:</p>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#21-disease-progression-modeling","title":"2.1 Disease Progression Modeling\u00b6","text":"<p>Question: When will a patient progress from one disease stage to another?</p> <p>Examples:</p> <ul> <li>Chronic Kidney Disease (CKD): Stage 3 \u2192 Stage 4 \u2192 Stage 5 (ESRD)</li> <li>Cancer: Localized \u2192 Regional \u2192 Metastatic</li> <li>Heart Failure: NYHA Class I \u2192 II \u2192 III \u2192 IV</li> <li>Diabetes: Prediabetes \u2192 Type 2 Diabetes \u2192 Complications</li> </ul> <p>Model Type:</p> <ul> <li>Single-event survival: Time to next stage</li> <li>Multi-state models: Transitions between multiple stages</li> </ul> <p>Labeling Strategy:</p> <pre># For each patient sequence:\n# 1. Identify current stage from diagnosis codes\n# 2. Find first visit with next-stage codes\n# 3. Event time = visit index of progression\n# 4. Censored if no progression observed\n</pre>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#22-treatment-response-prediction","title":"2.2 Treatment Response Prediction\u00b6","text":"<p>Question: How long until a patient responds to treatment?</p> <p>Examples:</p> <ul> <li>Antidepressants: Time to symptom improvement</li> <li>Chemotherapy: Time to tumor shrinkage</li> <li>Antibiotics: Time to infection clearance</li> </ul> <p>Model Type: Time-to-response survival</p> <p>Labeling Strategy:</p> <pre># 1. Identify treatment initiation visit\n# 2. Define response criteria (e.g., lab values, symptom codes)\n# 3. Event time = visits from treatment start to response\n# 4. Censored if treatment stopped or follow-up ended\n</pre>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#23-adverse-event-prediction","title":"2.3 Adverse Event Prediction\u00b6","text":"<p>Question: When will a patient experience an adverse event?</p> <p>Examples:</p> <ul> <li>Hospital readmission: Time to 30-day readmission</li> <li>Drug side effects: Time to adverse reaction</li> <li>Surgical complications: Time to post-op infection</li> </ul> <p>Model Type: Time-to-event survival</p> <p>Labeling Strategy:</p> <pre># 1. Define index event (e.g., hospital discharge, drug start)\n# 2. Define adverse event codes\n# 3. Event time = visits from index to adverse event\n# 4. Censored if no event within observation window\n</pre>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#24-competing-risks","title":"2.4 Competing Risks\u00b6","text":"<p>Question: Which outcome will occur first?</p> <p>Examples:</p> <ul> <li>Cardiovascular outcomes: MI vs. Stroke vs. Death</li> <li>Cancer: Progression vs. Treatment toxicity vs. Death</li> <li>Transplant: Graft failure vs. Rejection vs. Death</li> </ul> <p>Model Type: Competing risks survival</p> <p>Labeling Strategy:</p> <pre># 1. Define multiple event types\n# 2. For each patient, identify which event occurred first\n# 3. Event time = visit of first event\n# 4. Event type = which outcome occurred\n# 5. Censored if no events observed\n</pre>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#25-resource-utilization","title":"2.5 Resource Utilization\u00b6","text":"<p>Question: When will a patient need a specific resource?</p> <p>Examples:</p> <ul> <li>ICU admission: Time to critical care need</li> <li>Mechanical ventilation: Time to respiratory failure</li> <li>Dialysis: Time to renal replacement therapy</li> </ul> <p>Model Type: Time-to-resource survival</p> <p>Labeling Strategy:</p> <pre># 1. Define resource utilization codes (procedures, locations)\n# 2. Event time = first visit with resource use\n# 3. Censored if no resource use observed\n</pre>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#summary-table-research-questions","title":"Summary Table: Research Questions\u00b6","text":"Research Question Clinical Example Model Type Key Challenge Disease Progression CKD Stage 3 \u2192 4 Single/Multi-state Defining stage transitions Treatment Response Time to symptom improvement Time-to-response Defining response criteria Adverse Events 30-day readmission Time-to-event Temporal window definition Competing Risks MI vs. Stroke vs. Death Competing risks Multiple outcomes Resource Use Time to ICU admission Time-to-resource Resource availability data"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#common-themes","title":"Common Themes\u00b6","text":"<ol> <li>Temporal ordering matters: Must respect time to avoid leakage</li> <li>Censoring is common: Not all patients have observed events</li> <li>Definition is critical: Clear event definitions enable accurate labeling</li> <li>Clinical relevance: Questions must align with actionable decisions</li> </ol>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#part-3-data-labeling-strategies","title":"Part 3: Data Labeling Strategies\u00b6","text":"<p>Translating clinical questions into survival labels requires careful consideration of:</p> <ol> <li>Event definition: What constitutes an event?</li> <li>Time origin: When does the clock start?</li> <li>Censoring: When is a patient censored?</li> <li>Temporal leakage: Are we using future information?</li> </ol>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#31-example-ckd-progression-stage-3-4","title":"3.1 Example: CKD Progression (Stage 3 \u2192 4)\u00b6","text":"<p>Clinical Question: Given a patient with CKD Stage 3, when will they progress to Stage 4?</p> <p>Event Definition:</p> <ul> <li>ICD-10 codes: N18.4 (Stage 4), N18.5 (Stage 5)</li> <li>Lab values: eGFR &lt; 30 mL/min/1.73m\u00b2 (Stage 4)</li> </ul> <p>Time Origin: First visit with Stage 3 diagnosis</p> <p>Labeling Logic:</p>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#32-avoiding-temporal-leakage","title":"3.2 Avoiding Temporal Leakage\u00b6","text":"<p>Critical Rule: At visit $t$, the model can only use information from visits $\\leq t$.</p> <p>Common Leakage Patterns:</p> <ol> <li><p>Future outcome in features:</p> <pre># \u2717 WRONG: Using future Stage 4 codes to predict Stage 4\nfeatures = all_codes_in_sequence  # Includes future visits!\n\n# \u2713 CORRECT: Only use codes up to current visit\nfeatures = codes_up_to_visit_t\n</pre> </li> <li><p>Outcome-dependent censoring:</p> <pre># \u2717 WRONG: Censoring patients who died (informative censoring)\nif patient_died:\n    censored = True\n\n# \u2713 CORRECT: Treat death as competing risk or separate outcome\nif patient_died:\n    event_type = 'death'\n</pre> </li> <li><p>Post-event features:</p> <pre># \u2717 WRONG: Including visits after event in training\nsequence = all_visits\n\n# \u2713 CORRECT: Truncate sequence at event time\nsequence = visits_up_to_event\n</pre> </li> </ol>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#33-handling-different-censoring-types","title":"3.3 Handling Different Censoring Types\u00b6","text":"<p>Administrative Censoring: Study ends, patient still at risk</p> <pre># Patient has no event by end of observation\nevent_time = last_visit_index\nevent_indicator = 0\n</pre> <p>Loss to Follow-up: Patient stops coming to clinic</p> <pre># Patient hasn't returned in &gt; 6 months\nif days_since_last_visit &gt; 180:\n    event_time = last_visit_index\n    event_indicator = 0\n</pre> <p>Competing Risk: Different event occurred first</p> <pre># Patient died before CKD progression\n# Option 1: Treat as censored for CKD progression\nevent_indicator = 0\n\n# Option 2: Use competing risks model\nevent_type = 'death'  # vs. 'progression'\n</pre>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#34-synthetic-data-generation","title":"3.4 Synthetic Data Generation\u00b6","text":"<p>For demonstration and testing, we can generate synthetic survival outcomes based on patient characteristics.</p> <p>Advantages:</p> <ul> <li>Known ground truth for validation</li> <li>Controllable event rates and censoring</li> <li>No need for specific disease codes</li> <li>Useful for algorithm development</li> </ul> <p>Our Approach: Risk-based simulation</p>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#part-4-generate-synthetic-survival-outcomes","title":"Part 4: Generate Synthetic Survival Outcomes\u00b6","text":"<p>Now we'll generate synthetic survival outcomes based on patient characteristics. The generator creates realistic event times where:</p> <ul> <li>High-risk patients (many codes, frequent visits) have earlier events</li> <li>Low-risk patients have later events or are censored</li> <li>Censoring rate is controllable (default 30%)</li> </ul> <p>After generation, we'll run a diagnostic check to verify the outcomes have the correct risk-time relationship.</p>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#part-4-memory-requirements-and-cloud-training-setup","title":"Part 4: Memory Requirements and Cloud Training Setup\u00b6","text":""},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#memory-considerations","title":"Memory Considerations\u00b6","text":"<p>Training survival models on large EHR datasets requires significant GPU memory due to:</p> <ul> <li>Variable sequence lengths: Patients have different numbers of visits</li> <li>Dense visit representations: Each visit contains many medical codes</li> <li>LSTM hidden states: Recurrent layers maintain state across sequences</li> <li>Batch processing: Multiple patients processed simultaneously</li> </ul>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#local-vs-cloud-training","title":"Local vs. Cloud Training\u00b6","text":"<p>Local Testing (MPS/Small GPU):</p> <ul> <li>Use <code>MAX_PATIENTS=200</code> for subset training</li> <li>Reduces memory from ~20GB to ~5-8GB</li> <li>Good for: debugging, hyperparameter tuning, code validation</li> <li>Limitation: May not achieve best performance on small data</li> </ul> <p>Cloud Training (RunPods/Vast.ai):</p> <ul> <li>Use <code>MAX_PATIENTS=None</code> for full dataset (1,151 patients)</li> <li>Requires: 16-24GB GPU memory</li> <li>Recommended GPUs:<ul> <li>RTX 4090 (24GB): ~$0.40/hr on RunPods</li> <li>RTX 3090 (24GB): ~$0.30/hr on RunPods</li> <li>A100 (40GB): ~$1.00/hr on RunPods (overkill for this task)</li> </ul> </li> <li>Good for: final training, performance evaluation, production models</li> </ul>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#runpods-setup-instructions","title":"RunPods Setup Instructions\u00b6","text":"<ol> <li>Create account: https://www.runpod.io/</li> <li>Select GPU: Choose RTX 3090 or RTX 4090 pod</li> <li>Choose template: PyTorch 2.0+ with CUDA 11.8+</li> <li>Upload notebook: Use Jupyter interface or git clone</li> <li>Install dependencies:<pre>pip install pandas numpy matplotlib seaborn scikit-learn tqdm\n</pre> </li> <li>Set configuration: <code>MAX_PATIENTS = None</code> in cell 11</li> <li>Run training: Execute cells 11-16</li> <li>Download results: Save model weights and training history</li> </ol>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#memory-optimization-tips","title":"Memory Optimization Tips\u00b6","text":"<p>If you encounter OOM (Out of Memory) errors:</p> <ol> <li>Reduce batch size: Change from 32 to 16 or 8</li> <li>Reduce model size:<ul> <li><code>embedding_dim=64</code> instead of 128</li> <li><code>hidden_dim=128</code> instead of 256</li> </ul> </li> <li>Limit sequence length: Set <code>max_visits=50</code> in <code>PatientSequenceBuilder</code></li> <li>Use gradient accumulation: Simulate larger batches with less memory</li> <li>Enable mixed precision: Use <code>torch.cuda.amp</code> for FP16 training</li> </ol>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#expected-performance-by-dataset-size","title":"Expected Performance by Dataset Size\u00b6","text":"Dataset Size C-index (Expected) Training Time Memory Required 100 patients 0.50-0.55 5-10 min 2-4 GB 200 patients 0.55-0.60 10-15 min 4-8 GB 500 patients 0.60-0.65 20-30 min 8-12 GB 1,000+ patients 0.65-0.75 30-60 min 16-24 GB <p>Note: C-index depends on synthetic outcome quality and model architecture</p>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#part-5-complete-workflow-training-a-survival-lstm","title":"Part 5: Complete Workflow - Training a Survival LSTM\u00b6","text":"<p>Now let's train a discrete-time survival LSTM on the synthetic data.</p>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#summary-and-key-takeaways","title":"Summary and Key Takeaways\u00b6","text":""},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#what-we-learned","title":"What We Learned\u00b6","text":"<ol> <li><p>C-index Fundamentals:</p> <ul> <li>Measures ranking quality for survival predictions</li> <li>Handles censoring naturally</li> <li>Interpretable as pairwise concordance probability</li> </ul> </li> <li><p>Research Applications:</p> <ul> <li>Disease progression modeling</li> <li>Treatment response prediction</li> <li>Adverse event forecasting</li> <li>Competing risks analysis</li> <li>Resource utilization planning</li> </ul> </li> <li><p>Labeling Strategies:</p> <ul> <li>Define clear event criteria</li> <li>Establish time origin</li> <li>Handle censoring appropriately</li> <li>Avoid temporal leakage</li> </ul> </li> <li><p>Complete Workflow:</p> <ul> <li>Load and preprocess EHR data</li> <li>Generate or extract survival labels</li> <li>Train discrete-time survival LSTM</li> <li>Evaluate with C-index</li> </ul> </li> </ol>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Continuous-time models: Cox proportional hazards with LSTMs</li> <li>Competing risks: Multi-event survival modeling</li> <li>Multi-state models: Complex disease progression pathways</li> <li>Pretrained embeddings: Med2Vec, BEHRT for better representations</li> <li>Interpretability: Understanding which features drive risk predictions</li> </ul>"},{"location":"notebooks/02_survival_analysis/01_discrete_time_survival_lstm/#references","title":"References\u00b6","text":"<ul> <li>Harrell et al. (1982): \"Evaluating the Yield of Medical Tests\" - Original C-index paper</li> <li>Lee et al. (2018): \"DeepHit: A Deep Learning Approach to Survival Analysis with Competing Risks\"</li> <li>Katzman et al. (2018): \"DeepSurv: Personalized Treatment Recommender System Using A Cox Proportional Hazards Deep Neural Network\"</li> <li>Our documentation: <code>docs/methods/causal-survival-analysis-1.md</code> and <code>causal-survival-analysis-2.md</code></li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/","title":"Tutorial 1: The Survival Prediction Problem","text":"<p>Part of: Discrete-Time Survival Analysis for EHR Sequences Audience: Researchers and practitioners new to survival analysis in healthcare</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#table-of-contents","title":"Table of Contents","text":"<ol> <li>What is Survival Analysis?</li> <li>Why Use Survival Analysis for EHR Data?</li> <li>The Prediction Problem</li> <li>Discrete-Time vs. Continuous-Time</li> <li>Evaluation Metrics</li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#what-is-survival-analysis","title":"What is Survival Analysis?","text":"<p>Survival analysis is a branch of statistics that models time-to-event data. The key question is:</p> <p>\"When will an event occur?\"</p> <p>Rather than just:</p> <p>\"Will an event occur?\" (binary classification)</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#key-concepts","title":"Key Concepts","text":"<p>Event: The outcome of interest - Disease onset (e.g., diabetes, heart failure) - Hospital readmission - Mortality - Treatment response</p> <p>Time-to-Event (T): How long until the event occurs - Measured from a defined starting point (e.g., first visit, diagnosis) - Can be in days, visits, or other time units</p> <p>Censoring: When we don't observe the event - Right censoring: Patient leaves study before event occurs - Administrative censoring: Study ends before event occurs - Lost to follow-up: Patient stops coming to clinic</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#example","title":"Example","text":"<p>Binary Classification: - Question: \"Will this patient develop diabetes?\" - Answer: Yes (1) or No (0) - Problem: Ignores when it happens</p> <p>Survival Analysis: - Question: \"When will this patient develop diabetes?\" - Answer: Time = 365 days (1 year) - Benefit: Captures temporal dynamics</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#why-use-survival-analysis-for-ehr-data","title":"Why Use Survival Analysis for EHR Data?","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#1-temporal-information-is-critical","title":"1. Temporal Information is Critical","text":"<p>In healthcare, timing matters: - A patient who develops disease in 1 month needs urgent intervention - A patient who develops disease in 10 years can be monitored - Binary classification treats both the same!</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#2-censoring-is-inevitable","title":"2. Censoring is Inevitable","text":"<p>In real-world EHR data: - Patients move to different hospitals - Studies have finite duration - Not all patients experience the event</p> <p>Survival analysis handles censoring properly by: - Using partial information from censored patients - Not treating censored patients as \"no event\" (which is wrong!) - Accounting for uncertainty in event timing</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#3-risk-stratification","title":"3. Risk Stratification","text":"<p>Survival models enable: - High-risk identification: Patients likely to have events soon - Resource allocation: Prioritize interventions for high-risk patients - Personalized timelines: Different patients have different trajectories</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#4-clinical-relevance","title":"4. Clinical Relevance","text":"<p>Clinicians think in terms of: - \"5-year survival rate\" - \"Time to progression\" - \"Median time to event\"</p> <p>Survival analysis directly provides these metrics.</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#the-prediction-problem","title":"The Prediction Problem","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#problem-formulation","title":"Problem Formulation","text":"<p>Given: - Patient history up to time \\(t\\): \\(H_t = \\{v_1, v_2, \\ldots, v_t\\}\\) - Each visit \\(v_i\\) contains medical codes (diagnoses, procedures, medications)</p> <p>Predict: - Probability of event at each future time point - Risk score indicating overall event likelihood - Expected time to event</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#mathematical-framework","title":"Mathematical Framework","text":"<p>For each patient, we observe: - \\(T\\): Time of event or censoring - \\(\\delta\\): Event indicator (1 = event observed, 0 = censored)</p> <p>We want to model: - \\(h(t | H_t)\\): Hazard at time \\(t\\) given history   - Probability of event at time \\(t\\) given survival to \\(t\\) - \\(S(t | H_t)\\): Survival function   - Probability of surviving past time \\(t\\)</p> <p>Relationship: $\\(S(t) = \\prod_{i=1}^{t} (1 - h(i))\\)$</p> <p>The survival probability is the product of not having an event at each prior time.</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#example-diabetes-prediction","title":"Example: Diabetes Prediction","text":"<p>Patient History: <pre><code>Visit 1 (Day 0):   [Hypertension, High BMI]\nVisit 2 (Day 90):  [Hypertension, High cholesterol]\nVisit 3 (Day 180): [Hypertension, Prediabetes]\nVisit 4 (Day 270): [Diabetes] \u2190 EVENT\n</code></pre></p> <p>Prediction Task: - At Visit 1: Predict hazard at future visits - At Visit 2: Update prediction with new information - At Visit 3: High hazard expected at next visit (prediabetes signal)</p> <p>Model Output: <pre><code>h(Visit 1) = 0.05  (low risk)\nh(Visit 2) = 0.10  (moderate risk)\nh(Visit 3) = 0.30  (high risk - prediabetes!)\nh(Visit 4) = 0.60  (very high risk)\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#discrete-time-vs-continuous-time","title":"Discrete-Time vs. Continuous-Time","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#continuous-time-survival","title":"Continuous-Time Survival","text":"<p>Used when: Events can occur at any moment - Example: Death, ICU admission - Time measured precisely (hours, minutes)</p> <p>Model: Cox proportional hazards $\\(h(t | X) = h_0(t) \\exp(X^T \\beta)\\)$</p> <p>Pros: - Flexible baseline hazard \\(h_0(t)\\) - Well-established theory</p> <p>Cons: - Harder to integrate with deep learning - Assumes proportional hazards (may not hold)</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#discrete-time-survival","title":"Discrete-Time Survival","text":"<p>Used when: Events occur at discrete time points - Example: Disease onset at visits, readmission episodes - Time measured in visits, months, years</p> <p>Model: Discrete hazard at each time point $\\(h_t = P(T = t | T \\geq t, H_t)\\)$</p> <p>Pros: - Natural fit for visit-based EHR data - Easy integration with LSTMs/Transformers - Flexible hazard (no proportional hazards assumption)</p> <p>Cons: - Requires discretization if time is continuous - May lose precision if intervals are large</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#why-discrete-time-for-ehr","title":"Why Discrete-Time for EHR?","text":"<ol> <li>Visit-based data: EHR events are naturally grouped into visits</li> <li>Deep learning compatibility: LSTMs process sequences of visits</li> <li>Flexibility: Hazard can change arbitrarily over time</li> <li>Interpretability: Hazard at each visit is easy to understand</li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#1-concordance-index-c-index","title":"1. Concordance Index (C-index)","text":"<p>Definition: Fraction of patient pairs correctly ranked by risk</p> <p>Interpretation: - C-index = 1.0: Perfect ranking - C-index = 0.5: Random ranking (coin flip) - C-index = 0.0: Completely wrong ranking</p> <p>Calculation: For all pairs \\((i, j)\\) where: - Patient \\(i\\) has observed event - Event time \\(T_i &lt; T_j\\)</p> <p>Count as concordant if: \\(\\text{risk}_i &gt; \\text{risk}_j\\)</p> \\[\\text{C-index} = \\frac{\\text{concordant pairs}}{\\text{total comparable pairs}}\\] <p>Example: <pre><code>Patient A: Event at visit 5, risk = 0.8\nPatient B: Event at visit 10, risk = 0.6\nPatient C: Event at visit 15, risk = 0.4\n\nPairs:\n(A, B): T_A &lt; T_B and risk_A &gt; risk_B \u2713 Concordant\n(A, C): T_A &lt; T_C and risk_A &gt; risk_C \u2713 Concordant\n(B, C): T_B &lt; T_C and risk_B &gt; risk_C \u2713 Concordant\n\nC-index = 3/3 = 1.0 (perfect!)\n</code></pre></p> <p>Why C-index? - Standard metric in survival analysis - Handles censoring properly - Interpretable (like AUC but for time-to-event)</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#2-calibration","title":"2. Calibration","text":"<p>Definition: Do predicted probabilities match observed frequencies?</p> <p>Example: - Model predicts 30% of high-risk patients will have events in 1 year - Observed: 28% actually have events - Good calibration!</p> <p>Metrics: - Calibration plots (predicted vs. observed) - Brier score (mean squared error of probabilities)</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#3-time-dependent-auc","title":"3. Time-Dependent AUC","text":"<p>Definition: AUC for predicting event within time \\(t\\)</p> <p>Example: - AUC at 1 year: How well does model predict events within 1 year? - AUC at 5 years: How well does model predict events within 5 years?</p> <p>Benefit: Evaluates prediction at specific horizons</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#putting-it-all-together","title":"Putting It All Together","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#the-complete-workflow","title":"The Complete Workflow","text":"<ol> <li>Define the event: What are we predicting?</li> <li> <p>Example: Diabetes onset</p> </li> <li> <p>Define the time origin: When does the clock start?</p> </li> <li> <p>Example: First visit to clinic</p> </li> <li> <p>Define the time scale: How do we measure time?</p> </li> <li> <p>Example: Visits, days, months</p> </li> <li> <p>Handle censoring: Who is censored and why?</p> </li> <li> <p>Example: Patients who leave the health system</p> </li> <li> <p>Build the model: Predict hazard at each time point</p> </li> <li> <p>Example: LSTM predicting visit-level hazards</p> </li> <li> <p>Evaluate: Use C-index and calibration</p> </li> <li>Example: C-index = 0.70 (good discrimination)</li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#example-heart-failure-prediction","title":"Example: Heart Failure Prediction","text":"<p>Setup: - Event: First heart failure diagnosis - Time origin: First cardiology visit - Time scale: Visits - Censoring: Patients who leave health system</p> <p>Model: - Input: Sequence of visits with diagnoses, procedures, medications - Output: Hazard at each visit - Risk score: Mean hazard over first 10 visits</p> <p>Evaluation: - C-index = 0.72 (good) - Calibration: Predicted vs. observed matches well - Interpretation: Model successfully identifies high-risk patients</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Survival analysis captures \"when\" not just \"if\"</li> <li>Censoring is handled properly unlike binary classification</li> <li>Discrete-time models are natural for visit-based EHR data</li> <li>C-index measures ranking quality (like AUC for survival)</li> <li>Risk scores enable stratification for clinical decision-making</li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#further-reading","title":"Further Reading","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#textbooks","title":"Textbooks","text":"<ul> <li>Singer &amp; Willett (2003). Applied Longitudinal Data Analysis</li> <li>Tutz &amp; Schmid (2016). Modeling Discrete Time-to-Event Data</li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#papers","title":"Papers","text":"<ul> <li>Katzman et al. (2018). \"DeepSurv: Personalized treatment recommender system using a Cox proportional hazards deep neural network\"</li> <li>Lee et al. (2018). \"DeepHit: A deep learning approach to survival analysis with competing risks\"</li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_01_prediction_problem/#software","title":"Software","text":"<ul> <li><code>lifelines</code> (Python): Continuous-time survival analysis</li> <li><code>scikit-survival</code> (Python): Machine learning for survival analysis</li> <li><code>PyHealth</code>: EHR-specific deep learning models</li> </ul> <p>Next Tutorial: Synthetic Data Design and Labeling</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/","title":"Tutorial 2: Synthetic Data Design and Labeling","text":"<p>Part of: Discrete-Time Survival Analysis for EHR Sequences Audience: Researchers building survival models for EHR data</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Why Synthetic Data?</li> <li>Design Principles</li> <li>Risk Factor Computation</li> <li>Event Time Simulation</li> <li>Censoring Mechanism</li> <li>Validation and Quality Control</li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#why-synthetic-data","title":"Why Synthetic Data?","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#the-challenge","title":"The Challenge","text":"<p>When developing survival models, we need labeled data with: - Event times: When did the event occur? - Event indicators: Did the event occur or was it censored? - Risk factors: What patient characteristics predict the event?</p> <p>Problem: Real EHR data requires: - Manual chart review to identify events - Clinical expertise to define outcomes - IRB approval and privacy compliance - Months of data preparation</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#the-solution-synthetic-outcomes","title":"The Solution: Synthetic Outcomes","text":"<p>Generate synthetic survival outcomes from existing patient sequences: - Use patient visit patterns as features - Simulate realistic risk-time relationships - Control correlation strength for testing - Enable rapid iteration and development</p> <p>Key Insight: We're not creating fake patients, we're creating fake outcomes for real patient sequences.</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#design-principles","title":"Design Principles","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#1-realistic-risk-time-correlation","title":"1. Realistic Risk-Time Correlation","text":"<p>Goal: High-risk patients should have earlier events</p> <p>Why: This matches clinical reality - Sicker patients progress faster - More comorbidities \u2192 higher risk \u2192 earlier events - Frequent healthcare utilization \u2192 higher risk</p> <p>Implementation: <pre><code># Negative correlation between risk and event time\n# High risk (0.9) \u2192 Early event (visit 5)\n# Low risk (0.1) \u2192 Late event (visit 50)\ncorrelation_target = -0.5  # Moderate to strong\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#2-controlled-noise","title":"2. Controlled Noise","text":"<p>Goal: Realistic but not perfect correlation</p> <p>Why: Real clinical data has noise - Individual variation in disease progression - Unmeasured confounders - Stochastic biological processes</p> <p>Implementation: <pre><code># Add small Gaussian noise to event times\nnoise_std = 0.08  # Small enough to preserve correlation\n                  # Large enough to be realistic\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#3-meaningful-risk-factors","title":"3. Meaningful Risk Factors","text":"<p>Goal: Risk factors should reflect clinical knowledge</p> <p>Why: Model should learn interpretable patterns - Comorbidity burden (more conditions \u2192 higher risk) - Healthcare utilization (frequent visits \u2192 higher risk) - Code diversity (repeated conditions \u2192 higher risk)</p> <p>Implementation: <pre><code>risk_factors = {\n    'comorbidity': avg_codes_per_visit,  # Disease burden\n    'frequency': visits_per_year,         # Utilization\n    'diversity': unique_codes / total_codes  # Complexity\n}\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#4-appropriate-censoring","title":"4. Appropriate Censoring","text":"<p>Goal: Simulate realistic censoring patterns</p> <p>Why: Survival models must handle censoring - Tests model's ability to use partial information - Reflects real-world data collection</p> <p>Implementation: <pre><code>censoring_rate = 0.3  # 30% of patients censored\n# Censored patients: event_indicator = 0\n# Observed events: event_indicator = 1\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#risk-factor-computation","title":"Risk Factor Computation","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#overview","title":"Overview","text":"<p>We extract three risk factors from each patient's visit sequence:</p> <ol> <li>Comorbidity burden: How many conditions does the patient have?</li> <li>Visit frequency: How often does the patient seek care?</li> <li>Code diversity: How varied are the patient's conditions?</li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#1-comorbidity-burden","title":"1. Comorbidity Burden","text":"<p>Definition: Average number of medical codes per visit</p> <p>Rationale: - More codes \u2192 more conditions \u2192 higher risk - Reflects disease complexity and severity</p> <p>Computation: <pre><code>def compute_comorbidity(patient_sequence):\n    codes_per_visit = [visit.num_codes() for visit in patient_sequence.visits]\n    avg_codes = np.mean(codes_per_visit)\n    return avg_codes\n\n# Example:\n# Visit 1: 5 codes\n# Visit 2: 8 codes\n# Visit 3: 6 codes\n# Comorbidity = (5 + 8 + 6) / 3 = 6.33\n</code></pre></p> <p>Normalization: <pre><code># Normalize by typical maximum (20 codes per visit)\nnorm_comorbidity = avg_codes / 20.0\n# Result in [0, 1] range\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#2-visit-frequency","title":"2. Visit Frequency","text":"<p>Definition: Number of visits per year</p> <p>Rationale: - Frequent visits \u2192 higher healthcare utilization \u2192 sicker patients - Reflects disease severity and monitoring needs</p> <p>Computation: <pre><code>def compute_frequency(patient_sequence):\n    num_visits = len(patient_sequence.visits)\n\n    # Calculate time span in years\n    first_visit = patient_sequence.visits[0].timestamp\n    last_visit = patient_sequence.visits[-1].timestamp\n    time_span_years = (last_visit - first_visit).days / 365.0\n\n    # Visits per year\n    frequency = num_visits / max(time_span_years, 0.1)\n    return frequency\n\n# Example:\n# 20 visits over 4 years\n# Frequency = 20 / 4 = 5 visits/year\n</code></pre></p> <p>Normalization: <pre><code># Normalize by typical maximum (5 visits per year)\nnorm_frequency = frequency / 5.0\n# Result in [0, 1] range\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#3-code-diversity","title":"3. Code Diversity","text":"<p>Definition: Ratio of unique codes to total codes</p> <p>Rationale: - Low diversity (repeated codes) \u2192 chronic condition \u2192 higher risk - High diversity (varied codes) \u2192 exploratory care \u2192 lower risk</p> <p>Computation: <pre><code>def compute_diversity(patient_sequence):\n    all_codes = []\n    for visit in patient_sequence.visits:\n        all_codes.extend(visit.get_all_codes())\n\n    unique_codes = len(set(all_codes))\n    total_codes = len(all_codes)\n\n    diversity = unique_codes / max(total_codes, 1)\n    return diversity\n\n# Example:\n# Total codes: 100\n# Unique codes: 30\n# Diversity = 30 / 100 = 0.30 (low diversity, repeated conditions)\n</code></pre></p> <p>Interpretation: <pre><code># Low diversity (0.2-0.4): Chronic disease with repeated codes\n# High diversity (0.7-0.9): Varied conditions, exploratory care\n# For risk: LOW diversity = HIGH risk (inverted)\nrisk_from_diversity = 1 - diversity\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#combining-risk-factors","title":"Combining Risk Factors","text":"<p>Weighted combination: <pre><code>risk_weights = {\n    'comorbidity': 0.4,  # 40% weight\n    'frequency': 0.4,    # 40% weight\n    'diversity': 0.2     # 20% weight\n}\n\nrisk_score = (\n    risk_weights['comorbidity'] * norm_comorbidity +\n    risk_weights['frequency'] * norm_frequency +\n    risk_weights['diversity'] * (1 - diversity)  # Inverted!\n)\n\n# Clip to [0.1, 0.9] to avoid extreme values\nrisk_score = np.clip(risk_score, 0.1, 0.9)\n</code></pre></p> <p>Example: <pre><code># Patient A:\n# - Avg codes: 15 \u2192 norm = 15/20 = 0.75\n# - Frequency: 4/year \u2192 norm = 4/5 = 0.80\n# - Diversity: 0.30 \u2192 inverted = 0.70\n# Risk = 0.4*0.75 + 0.4*0.80 + 0.2*0.70 = 0.76 (HIGH RISK)\n\n# Patient B:\n# - Avg codes: 5 \u2192 norm = 5/20 = 0.25\n# - Frequency: 1/year \u2192 norm = 1/5 = 0.20\n# - Diversity: 0.80 \u2192 inverted = 0.20\n# Risk = 0.4*0.25 + 0.4*0.20 + 0.2*0.20 = 0.22 (LOW RISK)\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#event-time-simulation","title":"Event Time Simulation","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#goal","title":"Goal","text":"<p>Generate event times that have strong negative correlation with risk scores: - High risk \u2192 Early event - Low risk \u2192 Late event</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#approach-deterministic-base-controlled-noise","title":"Approach: Deterministic Base + Controlled Noise","text":"<p>Step 1: Normalize risk score <pre><code># Risk scores are in [0.1, 0.9]\n# Normalize to [0, 1] for better spread\nnormalized_risk = (risk_score - 0.1) / 0.8\n</code></pre></p> <p>Step 2: Compute base event time <pre><code># Invert risk to get time fraction\n# High risk (1.0) \u2192 early time (fraction ~0.1)\n# Low risk (0.0) \u2192 late time (fraction ~0.9)\nbase_time_fraction = 1.0 - normalized_risk\n</code></pre></p> <p>Step 3: Add controlled noise <pre><code># Small Gaussian noise\nnoise_std = 0.08  # Tuned for r \u2248 -0.5\nnoise = np.random.normal(0, noise_std)\nnoisy_fraction = np.clip(base_time_fraction + noise, 0.02, 0.98)\n</code></pre></p> <p>Step 4: Scale to visit range <pre><code># Use fixed horizon (e.g., first 30% of visits)\nhorizon = 10  # First 10 visits\nmax_event_visit = max(5, num_visits - 1)\nevent_time = int(noisy_fraction * max_event_visit * time_scale * 2)\n\n# Clip to valid range\nevent_visit = int(np.clip(event_time, 0, num_visits - 1))\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#why-this-works","title":"Why This Works","text":"<p>Deterministic base: Ensures strong correlation - Risk 0.9 \u2192 fraction 0.1 \u2192 early event - Risk 0.1 \u2192 fraction 0.9 \u2192 late event</p> <p>Controlled noise: Adds realism - Not all high-risk patients have immediate events - Individual variation in progression - Noise std = 0.08 gives correlation r \u2248 -0.5</p> <p>Fixed horizon: Prevents extreme values - Events occur in reasonable time window - Avoids events at visit 0 or last visit</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#example","title":"Example","text":"<pre><code># Patient with high risk\nrisk_score = 0.85\nnormalized_risk = (0.85 - 0.1) / 0.8 = 0.9375\nbase_fraction = 1.0 - 0.9375 = 0.0625\nnoise = 0.02  # Small positive noise\nnoisy_fraction = 0.0625 + 0.02 = 0.0825\nevent_time = int(0.0825 * 50 * 0.3 * 2) = 2\n# Result: Event at visit 2 (early!)\n\n# Patient with low risk\nrisk_score = 0.25\nnormalized_risk = (0.25 - 0.1) / 0.8 = 0.1875\nbase_fraction = 1.0 - 0.1875 = 0.8125\nnoise = -0.03  # Small negative noise\nnoisy_fraction = 0.8125 - 0.03 = 0.7825\nevent_time = int(0.7825 * 50 * 0.3 * 2) = 23\n# Result: Event at visit 23 (late!)\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#censoring-mechanism","title":"Censoring Mechanism","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#random-censoring","title":"Random Censoring","text":"<p>Approach: Randomly censor a fraction of patients</p> <pre><code>censoring_rate = 0.3  # 30% censored\n\nfor patient in patients:\n    is_censored = np.random.random() &lt; censoring_rate\n\n    if is_censored:\n        # Censor at random time before potential event\n        censor_time = np.random.randint(0, num_visits)\n        event_times.append(censor_time)\n        event_indicators.append(0)  # Censored\n    else:\n        # Event occurs\n        event_times.append(event_time)\n        event_indicators.append(1)  # Observed\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#interpretation","title":"Interpretation","text":"<p>Censored patient (indicator = 0): - We observe them until <code>censor_time</code> - We don't know if/when event occurs after that - Model uses partial information (visits before censoring)</p> <p>Observed event (indicator = 1): - We observe the actual event at <code>event_time</code> - Model learns from complete trajectory</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#why-30-censoring","title":"Why 30% Censoring?","text":"<p>Realistic: Typical in clinical studies - 20-40% censoring is common - Reflects patient dropout, study end, loss to follow-up</p> <p>Challenging: Tests model's ability - Must learn from incomplete data - Can't just ignore censored patients</p> <p>Balanced: Not too easy, not too hard - &lt; 20%: Too easy (mostly complete data) - &gt; 50%: Too hard (mostly incomplete data)</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#validation-and-quality-control","title":"Validation and Quality Control","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#1-correlation-check","title":"1. Correlation Check","text":"<p>Metric: Pearson correlation between risk scores and event times (events only)</p> <pre><code>event_mask = event_indicators == 1\ncorrelation, p_value = stats.pearsonr(\n    risk_scores[event_mask],\n    event_times[event_mask]\n)\n\n# Target: r &lt; -0.5 (strong negative correlation)\n# Acceptable: -0.7 &lt; r &lt; -0.3\n# Problematic: r &gt; -0.3 (too weak)\n</code></pre> <p>Interpretation: - r = -0.50: Good, model can learn - r = -0.30: Weak, model may struggle - r = -0.70: Strong, model should excel</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#2-event-rate-check","title":"2. Event Rate Check","text":"<p>Metric: Proportion of observed events</p> <pre><code>event_rate = event_indicators.mean()\n\n# Expected: 1 - censoring_rate\n# With censoring_rate = 0.3, expect event_rate \u2248 0.70\n</code></pre> <p>Validation: <pre><code>expected_rate = 1 - censoring_rate\nif abs(event_rate - expected_rate) &lt; 0.05:\n    print(\"\u2713 Event rate matches expected\")\nelse:\n    print(\"\u26a0 Event rate mismatch\")\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#3-risk-stratification-check","title":"3. Risk Stratification Check","text":"<p>Metric: Mean risk score for events vs. censored</p> <pre><code>event_risk = risk_scores[event_indicators == 1].mean()\ncensored_risk = risk_scores[event_indicators == 0].mean()\n\n# Events should have higher risk\nif event_risk &gt; censored_risk:\n    print(\"\u2713 Events have higher risk (correct)\")\nelse:\n    print(\"\u26a0 Censored have higher risk (unexpected)\")\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#4-visual-inspection","title":"4. Visual Inspection","text":"<p>Scatter plot: Risk vs. Event Time <pre><code>plt.scatter(risk_scores, event_times, c=event_indicators)\nplt.xlabel('Risk Score')\nplt.ylabel('Event Time')\n# Should see negative trend (high risk \u2192 low time)\n</code></pre></p> <p>Example patients: <pre><code>Risk Score   Event Time   Expected      Actual\n0.85         2            Early         Early  \u2713\n0.75         5            Early         Early  \u2713\n0.50         12           Mid           Mid    \u2713\n0.30         25           Late          Late   \u2713\n0.20         40           Late          Late   \u2713\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#5-distribution-checks","title":"5. Distribution Checks","text":"<p>Event time distribution: <pre><code>plt.hist(event_times[event_indicators == 1], bins=20)\n# Should be spread across time range\n# Not all at beginning or end\n</code></pre></p> <p>Risk score distribution: <pre><code>plt.hist(risk_scores, bins=20)\n# Should cover [0.1, 0.9] range\n# Not clustered in narrow band\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#complete-example","title":"Complete Example","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#code","title":"Code","text":"<pre><code>from ehrsequencing.synthetic.survival import DiscreteTimeSurvivalGenerator\n\n# Initialize generator\ngenerator = DiscreteTimeSurvivalGenerator(\n    censoring_rate=0.3,\n    risk_weights={\n        'comorbidity': 0.4,\n        'frequency': 0.4,\n        'diversity': 0.2\n    },\n    time_scale=0.3,\n    seed=42\n)\n\n# Generate outcomes\noutcome = generator.generate(sequences)\n\n# Validate\nprint(f\"Event rate: {outcome.event_indicators.float().mean():.2%}\")\nprint(f\"Mean risk: {outcome.risk_scores.mean():.3f}\")\n\n# Check correlation\nevent_mask = outcome.event_indicators == 1\ncorrelation = np.corrcoef(\n    outcome.risk_scores[event_mask],\n    outcome.event_times[event_mask]\n)[0, 1]\nprint(f\"Correlation: {correlation:.3f}\")\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#expected-output","title":"Expected Output","text":"<pre><code>Event rate: 69.5%\nMean risk: 0.628\nCorrelation: -0.500\n\n\u2713 Strong negative correlation\n\u2713 Event rate matches expected (70%)\n\u2713 Events have higher mean risk (0.639 vs 0.603)\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Synthetic outcomes enable rapid development without manual labeling</li> <li>Risk factors should be clinically meaningful (comorbidity, frequency, diversity)</li> <li>Strong correlation (r &lt; -0.5) is essential for model learning</li> <li>Controlled noise adds realism without destroying signal</li> <li>Validation is critical before training models</li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#too-much-noise","title":"\u274c Too Much Noise","text":"<pre><code>noise_std = 0.20  # Too large!\n# Result: Correlation r = -0.15 (too weak)\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#wrong-correlation-direction","title":"\u274c Wrong Correlation Direction","text":"<pre><code>event_time = risk_score * num_visits  # WRONG!\n# High risk \u2192 High time (backwards!)\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#no-normalization","title":"\u274c No Normalization","text":"<pre><code>risk_score = comorbidity + frequency  # WRONG!\n# Different scales, unbounded values\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02_synthetic_data_design/#correct-approach","title":"\u2705 Correct Approach","text":"<pre><code># Normalize inputs\nnorm_comorbidity = comorbidity / 20.0\nnorm_frequency = frequency / 5.0\n\n# Weighted combination\nrisk_score = 0.4 * norm_comorbidity + 0.4 * norm_frequency + ...\n\n# Clip to valid range\nrisk_score = np.clip(risk_score, 0.1, 0.9)\n\n# Invert for time\nbase_time_fraction = 1.0 - (risk_score - 0.1) / 0.8\n\n# Add small noise\nnoisy_fraction = base_time_fraction + np.random.normal(0, 0.08)\n</code></pre> <p>Next Tutorial: Loss Function Formulation</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/","title":"Tutorial 2a: Refining Comorbidity Burden Proxies","text":"<p>Supplementary to: Tutorial 2: Synthetic Data Design Part of: Discrete-Time Survival Analysis for EHR Sequences Audience: Researchers building survival models for EHR data</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#table-of-contents","title":"Table of Contents","text":"<ol> <li>The Problem with Aggregate Code Counts</li> <li>Medical Code Taxonomy</li> <li>Refined Approaches</li> <li>Implementation Strategies</li> <li>Clinical Validity Considerations</li> <li>Recommended Approach</li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#the-problem-with-aggregate-code-counts","title":"The Problem with Aggregate Code Counts","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#current-approach","title":"Current Approach","text":"<p>The original tutorial uses total code count per visit as a proxy for comorbidity burden:</p> <pre><code># From TUTORIAL_02, lines 92, 135-145\nrisk_factors = {\n    'comorbidity': avg_codes_per_visit,  # All codes treated equally\n}\n\ndef compute_comorbidity(patient_sequence):\n    codes_per_visit = [visit.num_codes() for visit in patient_sequence.visits]\n    avg_codes = np.mean(codes_per_visit)\n    return avg_codes\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#the-limitation","title":"The Limitation","text":"<p>Key Issue: Not all medical codes are equal indicators of disease burden.</p> <p>Consider two patients with 10 codes per visit:</p> <p>Patient A (High actual burden): - 5 ICD codes: CHF, COPD, CKD, Diabetes, Hypertension - 3 LOINC codes: BNP, Creatinine, HbA1c (monitoring) - 2 CPT codes: Echocardiogram, Chest X-ray (diagnostics)</p> <p>Patient B (Lower actual burden): - 1 ICD code: Well-controlled Type 2 Diabetes - 2 LOINC codes: HbA1c, Lipid panel (routine monitoring) - 7 NDC codes: Metformin, Lisinopril, Atorvastatin, Aspirin, Vitamin D, Fish oil, Multivitamin</p> <p>Observation: Patient B has similar code count but much lower disease severity.</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#specific-concerns-by-code-type","title":"Specific Concerns by Code Type","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#1-drug-codes-ndcrxnorm","title":"1. Drug Codes (NDC/RxNorm)","text":"<p>Problem: Medication count \u2260 disease severity</p> <ul> <li>Complex regimens for \"small\" diseases: </li> <li>Migraine prophylaxis might involve 3-4 medications</li> <li>Mild asthma might require 2-3 inhalers</li> <li>Polypharmacy patterns:</li> <li>Elderly patients on 10+ medications (preventive + treatment)</li> <li>Drug-drug interaction management adds more drugs</li> <li>Supplements and OTC medications:</li> <li>Vitamins, supplements inflate counts</li> <li>Not indicative of morbidity</li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#2-labtest-codes-loinc","title":"2. Lab/Test Codes (LOINC)","text":"<p>Problem: More tests \u2260 sicker patient</p> <ul> <li>Monitoring intensity:</li> <li>Well-controlled diabetes: frequent HbA1c checks</li> <li>Stable warfarin patient: weekly INR monitoring</li> <li>Diagnostic workup:</li> <li>Healthy patient with vague symptoms: extensive testing</li> <li>Established diagnosis: fewer confirmatory tests</li> <li>Screening protocols:</li> <li>Comprehensive metabolic panel = 1 order, 14 LOINC codes</li> <li>Doesn't indicate disease burden</li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#3-procedure-codes-cpthcpcs","title":"3. Procedure Codes (CPT/HCPCS)","text":"<p>Problem: Procedures vary wildly in invasiveness</p> <ul> <li>Minor vs. major:</li> <li>5 outpatient minor procedures \u2260 1 open-heart surgery</li> <li>Wound dressing changes (frequent) vs. transplant (rare)</li> <li>Preventive vs. therapeutic:</li> <li>Colonoscopy (screening) vs. emergency surgery</li> <li>Both count as \"1 procedure code\"</li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#4-diagnosis-codes-icd-910-cm","title":"4. Diagnosis Codes (ICD-9/10-CM)","text":"<p>Most informative: But still has nuances</p> <ul> <li>Severity hierarchy:</li> <li>\"Essential hypertension\" vs. \"Hypertensive crisis\"</li> <li>\"Type 2 diabetes\" vs. \"Diabetes with complications\"</li> <li>Redundancy:</li> <li>Multiple codes for same condition (primary + manifestations)</li> <li>Coding practices vary by institution</li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#medical-code-taxonomy","title":"Medical Code Taxonomy","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#code-type-hierarchy-by-informative-value","title":"Code Type Hierarchy by Informative Value","text":"<p>For comorbidity burden estimation:</p> <pre><code>Most Informative \u2192 Least Informative\n\n1. Diagnostic codes (ICD-9/10-CM)\n   - Direct disease indicators\n   - Severity stratification possible\n   - Strong predictor of outcomes\n\n2. Procedure codes (CPT/HCPCS)\n   - Indicates interventions\n   - Complexity varies (need weighting)\n   - Major procedures = high burden\n\n3. Lab/Test codes (LOINC)\n   - Indirect indicators (monitoring/diagnosis)\n   - High frequency \u2260 high burden\n   - Pattern matters more than count\n\n4. Medication codes (NDC/RxNorm)\n   - Treatment complexity\n   - Confounded by polypharmacy\n   - Class matters more than count\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#code-type-characteristics","title":"Code Type Characteristics","text":"Code Type Namespace Count Meaning Severity Proxy Recommended Weight Diagnosis ICD-9/10 Number of conditions Strong 1.0 (baseline) Procedure CPT/HCPCS Number of interventions Moderate 0.7-0.9 Lab/Test LOINC Monitoring intensity Weak 0.3-0.5 Medication NDC/RxNorm Treatment complexity Weak 0.2-0.4"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#refined-approaches","title":"Refined Approaches","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#approach-1-code-type-weighted-counts","title":"Approach 1: Code-Type Weighted Counts","text":"<p>Principle: Weight codes by their informative value for disease burden.</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#implementation","title":"Implementation","text":"<pre><code>def compute_weighted_comorbidity(patient_sequence, code_weights):\n    \"\"\"\n    Compute comorbidity burden using code-type-specific weights.\n\n    Args:\n        patient_sequence: Patient visit history\n        code_weights: Dict mapping code types to weights\n\n    Returns:\n        Weighted average code count per visit\n    \"\"\"\n    weighted_codes_per_visit = []\n\n    for visit in patient_sequence.visits:\n        # Count codes by type\n        icd_count = len(visit.get_codes_by_type('ICD'))\n        cpt_count = len(visit.get_codes_by_type('CPT'))\n        loinc_count = len(visit.get_codes_by_type('LOINC'))\n        ndc_count = len(visit.get_codes_by_type('NDC'))\n\n        # Apply weights\n        weighted_count = (\n            code_weights['ICD'] * icd_count +\n            code_weights['CPT'] * cpt_count +\n            code_weights['LOINC'] * loinc_count +\n            code_weights['NDC'] * ndc_count\n        )\n\n        weighted_codes_per_visit.append(weighted_count)\n\n    return np.mean(weighted_codes_per_visit)\n\n# Recommended weights\ncode_weights = {\n    'ICD': 1.0,    # Diagnoses: full weight\n    'CPT': 0.8,    # Procedures: high weight\n    'LOINC': 0.4,  # Lab tests: moderate weight\n    'NDC': 0.3     # Medications: low weight\n}\n\n# Example comparison\n# Patient with 5 ICD + 3 LOINC + 7 NDC codes\nunweighted = 5 + 3 + 7 = 15 codes\nweighted = 1.0*5 + 0.4*3 + 0.3*7 = 8.3 effective codes\n# More accurately reflects disease burden\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#rationale","title":"Rationale","text":"<ul> <li>Prioritizes diagnostic information: ICD codes get full weight</li> <li>Down-weights less informative codes: Labs and meds contribute less</li> <li>Clinically interpretable: \"Effective code count\" approximates true burden</li> <li>Preserves correlation: High-burden patients still have higher scores</li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#approach-2-diagnosis-only-focus","title":"Approach 2: Diagnosis-Only Focus","text":"<p>Principle: Use only diagnosis codes for comorbidity burden.</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#implementation_1","title":"Implementation","text":"<pre><code>def compute_diagnosis_burden(patient_sequence):\n    \"\"\"\n    Compute comorbidity burden using only diagnosis codes.\n    \"\"\"\n    icd_per_visit = []\n\n    for visit in patient_sequence.visits:\n        icd_codes = visit.get_codes_by_type('ICD')\n        icd_per_visit.append(len(icd_codes))\n\n    return np.mean(icd_per_visit)\n\n# Normalization\nnorm_comorbidity = avg_icd_per_visit / 10.0  # Typical max: 10 diagnoses\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#advantages","title":"Advantages","text":"<ul> <li>Maximum specificity: Direct measure of diagnosed conditions</li> <li>Avoids confounders: Not affected by medication polypharmacy</li> <li>Clear interpretation: \"Average number of diagnoses per visit\"</li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#disadvantages","title":"Disadvantages","text":"<ul> <li>Ignores treatment intensity: Complex procedures not captured</li> <li>Sensitive to coding practices: Under-coding biases downward</li> <li>May miss severity: Well-managed conditions with few active diagnoses</li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#approach-3-stratified-complexity-scores","title":"Approach 3: Stratified Complexity Scores","text":"<p>Principle: Create separate scores for each code type, then combine.</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#implementation_2","title":"Implementation","text":"<pre><code>def compute_stratified_burden(patient_sequence):\n    \"\"\"\n    Compute separate burden scores for each code type.\n    \"\"\"\n    # Compute type-specific averages\n    avg_icd = np.mean([len(v.get_codes_by_type('ICD')) for v in visits])\n    avg_cpt = np.mean([len(v.get_codes_by_type('CPT')) for v in visits])\n    avg_loinc = np.mean([len(v.get_codes_by_type('LOINC')) for v in visits])\n    avg_ndc = np.mean([len(v.get_codes_by_type('NDC')) for v in visits])\n\n    # Normalize each separately\n    norm_icd = avg_icd / 10.0      # Typical max: 10 diagnoses\n    norm_cpt = avg_cpt / 5.0       # Typical max: 5 procedures\n    norm_loinc = avg_loinc / 15.0  # Typical max: 15 lab tests\n    norm_ndc = avg_ndc / 10.0      # Typical max: 10 medications\n\n    # Create composite score\n    complexity_scores = {\n        'diagnostic': norm_icd,\n        'procedural': norm_cpt,\n        'monitoring': norm_loinc,\n        'therapeutic': norm_ndc\n    }\n\n    # Weighted combination\n    burden_score = (\n        0.50 * complexity_scores['diagnostic'] +\n        0.25 * complexity_scores['procedural'] +\n        0.15 * complexity_scores['monitoring'] +\n        0.10 * complexity_scores['therapeutic']\n    )\n\n    return burden_score, complexity_scores\n\n# Returns both composite and component scores\ntotal_burden, components = compute_stratified_burden(patient_sequence)\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#advantages_1","title":"Advantages","text":"<ul> <li>Multidimensional view: Captures different aspects of complexity</li> <li>Interpretable components: Can analyze which dimension drives risk</li> <li>Flexible weighting: Easy to adjust based on outcome of interest</li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#use-case","title":"Use Case","text":"<pre><code># Example: Different weights for different outcomes\n\n# Mortality risk: emphasize diagnoses and procedures\nmortality_weights = {'diagnostic': 0.5, 'procedural': 0.3, \n                     'monitoring': 0.1, 'therapeutic': 0.1}\n\n# Readmission risk: emphasize therapeutic complexity\nreadmission_weights = {'diagnostic': 0.3, 'procedural': 0.2,\n                       'monitoring': 0.2, 'therapeutic': 0.3}\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#approach-4-clinical-condition-counting-gold-standard","title":"Approach 4: Clinical Condition Counting (Gold Standard)","text":"<p>Principle: Map ICD codes to distinct clinical conditions using hierarchical groupers.</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#implementation_3","title":"Implementation","text":"<pre><code>def compute_condition_count(patient_sequence, grouper='CCS'):\n    \"\"\"\n    Count distinct clinical conditions using hierarchical grouper.\n\n    Groupers:\n        - CCS (Clinical Classifications Software): ~300 categories\n        - AHRQ Elixhauser: 31 comorbidity categories\n        - Charlson: 17 weighted comorbidity categories\n    \"\"\"\n    all_icd_codes = []\n    for visit in patient_sequence.visits:\n        all_icd_codes.extend(visit.get_codes_by_type('ICD'))\n\n    # Map to condition categories\n    conditions = set()\n    for icd_code in all_icd_codes:\n        category = grouper.map_to_category(icd_code)\n        conditions.add(category)\n\n    # Count distinct conditions\n    num_conditions = len(conditions)\n\n    return num_conditions\n\n# Example using Elixhauser comorbidities\nelixhauser_conditions = [\n    'CHF', 'Arrhythmia', 'Valvular', 'PVD', 'HTN',\n    'Paralysis', 'Neuro', 'Pulmonary', 'DM', 'Hypothyroid',\n    'Renal', 'Liver', 'PUD', 'HIV', 'Lymphoma',\n    'Mets', 'Tumor', 'Rheumatoid', 'Coagulopathy', 'Obesity',\n    'WeightLoss', 'Fluid', 'Anemia', 'BloodLoss', 'Alcohol',\n    'Drug', 'Psychoses', 'Depression'\n]\n\n# Patient has ICD codes mapping to:\nicd_codes = ['I50.9', 'I48.91', 'E11.9', 'N18.3']\n# Maps to:\nconditions = ['CHF', 'Arrhythmia', 'DM', 'Renal']\n# Condition count = 4 (not 4 ICD codes, but 4 distinct conditions)\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#advantages_2","title":"Advantages","text":"<ul> <li>Clinically meaningful: Aligns with medical understanding</li> <li>Reduces redundancy: Multiple ICD codes for same condition = 1</li> <li>Standard metrics: Can use validated comorbidity indices</li> <li>Best reflects actual burden: Gold standard in epidemiology</li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#disadvantages_1","title":"Disadvantages","text":"<ul> <li>Requires mapping: Need grouper software/lookup tables</li> <li>Implementation complexity: More code infrastructure</li> <li>Less granular: Loses within-condition severity variation</li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#recommended-groupers","title":"Recommended Groupers","text":"<ol> <li>CCS (Clinical Classifications Software) - ICD-9/10-CM</li> <li>~300 categories (e.g., \"Heart failure\", \"Diabetes\")</li> <li>Good for general comorbidity counting</li> <li> <p>Free from AHRQ</p> </li> <li> <p>Elixhauser Comorbidity Index</p> </li> <li>31 comorbidity categories</li> <li>Validated for mortality/readmission prediction</li> <li> <p>Widely used in health services research</p> </li> <li> <p>Charlson Comorbidity Index</p> </li> <li>17 weighted comorbidity categories</li> <li>Weights reflect mortality impact</li> <li>Original index: sum weights (not just count)</li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#implementation-strategies","title":"Implementation Strategies","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#strategy-1-phased-refinement-recommended","title":"Strategy 1: Phased Refinement (Recommended)","text":"<p>Phase 1: Baseline (Current) <pre><code># Start with aggregate count\ncomorbidity = avg_codes_per_visit\n</code></pre></p> <p>Phase 2: Type Weighting <pre><code># Add code-type weights\ncomorbidity = weighted_avg_codes_per_visit\ncode_weights = {'ICD': 1.0, 'CPT': 0.8, 'LOINC': 0.4, 'NDC': 0.3}\n</code></pre></p> <p>Phase 3: Diagnosis Focus <pre><code># Refine to diagnosis-heavy\ncomorbidity = 0.7 * avg_icd + 0.3 * weighted_other\n</code></pre></p> <p>Phase 4: Condition Grouping <pre><code># Ultimate: distinct conditions\ncomorbidity = num_distinct_conditions / 10.0  # Normalize\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#strategy-2-sensitivity-analysis","title":"Strategy 2: Sensitivity Analysis","text":"<p>Compare approaches on same data:</p> <pre><code># Compute multiple comorbidity metrics\nmetrics = {\n    'raw_count': compute_comorbidity(seq),\n    'weighted': compute_weighted_comorbidity(seq, weights),\n    'icd_only': compute_diagnosis_burden(seq),\n    'condition_count': compute_condition_count(seq)\n}\n\n# Train models with each metric\nfor metric_name, comorbidity_values in metrics.items():\n    outcomes = generate_outcomes(sequences, comorbidity=comorbidity_values)\n    model = train_model(sequences, outcomes)\n    performance[metric_name] = evaluate_model(model)\n\n# Compare: which metric gives best risk stratification?\nprint(f\"AUC by metric:\\n{performance}\")\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#strategy-3-ensemble-risk-score","title":"Strategy 3: Ensemble Risk Score","text":"<p>Combine multiple metrics:</p> <pre><code>def compute_ensemble_comorbidity(patient_sequence):\n    \"\"\"\n    Combine multiple comorbidity proxies.\n    \"\"\"\n    # Compute variants\n    raw = avg_codes_per_visit / 20.0\n    weighted = weighted_avg / 15.0\n    icd_only = avg_icd / 10.0\n\n    # Ensemble with weights\n    ensemble = (\n        0.2 * raw +       # Keep some raw signal\n        0.3 * weighted +  # Moderate weight\n        0.5 * icd_only    # Emphasize diagnoses\n    )\n\n    return ensemble\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#clinical-validity-considerations","title":"Clinical Validity Considerations","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#1-face-validity","title":"1. Face Validity","text":"<p>Question: Does the metric align with clinical intuition?</p> <p>Test cases: <pre><code># Patient A: 10 serious diagnoses (CHF, COPD, CKD, etc.)\n# Patient B: 1 diagnosis + 20 vitamins/supplements\n\n# Which has higher comorbidity?\n# Answer: Patient A\n\n# Check your metric:\nassert comorbidity_A &gt; comorbidity_B, \"Metric fails face validity\"\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#2-predictive-validity","title":"2. Predictive Validity","text":"<p>Question: Does the metric predict actual outcomes?</p> <pre><code># On real labeled data (if available)\ncorrelation = np.corrcoef(comorbidity_scores, actual_mortality)[0,1]\n\n# Strong correlation (|r| &gt; 0.3) = good predictive validity\n# Weak correlation (|r| &lt; 0.15) = reconsider metric\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#3-construct-validity","title":"3. Construct Validity","text":"<p>Question: Does the metric correlate with established indices?</p> <pre><code># Compare to Charlson or Elixhauser score\ncharlson_scores = compute_charlson_index(patients)\nyour_scores = compute_comorbidity(patients)\n\ncorrelation = np.corrcoef(charlson_scores, your_scores)[0,1]\n\n# High correlation (r &gt; 0.6) = good construct validity\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#4-discriminative-ability","title":"4. Discriminative Ability","text":"<p>Question: Does the metric separate high-risk from low-risk patients?</p> <pre><code># Compare score distributions\nhigh_risk_patients = patients[actual_events == 1]\nlow_risk_patients = patients[actual_events == 0]\n\nmean_high = comorbidity[high_risk_patients].mean()\nmean_low = comorbidity[low_risk_patients].mean()\n\n# Effect size (Cohen's d)\neffect_size = (mean_high - mean_low) / pooled_std\n\n# d &gt; 0.5 = medium effect (adequate)\n# d &gt; 0.8 = large effect (excellent)\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#recommended-approach","title":"Recommended Approach","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#for-synthetic-data-generation","title":"For Synthetic Data Generation","text":"<p>Recommendation: Use Approach 1 (Code-Type Weighted Counts) as starting point.</p> <p>Rationale: 1. Easy to implement: Minor modification to existing code 2. Clinically sound: Prioritizes informative code types 3. Preserves correlation: Still captures high-risk patients 4. Good compromise: Between simplicity and accuracy</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#updated-risk-factor-computation","title":"Updated Risk Factor Computation","text":"<pre><code>def compute_risk_factors_refined(patient_sequence):\n    \"\"\"\n    Compute risk factors with code-type-aware comorbidity.\n    \"\"\"\n    # Code type weights\n    code_weights = {\n        'ICD': 1.0,    # Diagnoses\n        'CPT': 0.8,    # Procedures\n        'LOINC': 0.4,  # Lab tests\n        'NDC': 0.3     # Medications\n    }\n\n    # 1. Weighted comorbidity burden\n    weighted_codes = []\n    for visit in patient_sequence.visits:\n        visit_weighted = sum(\n            code_weights.get(code.type, 0.5) * 1\n            for code in visit.codes\n        )\n        weighted_codes.append(visit_weighted)\n\n    avg_weighted_codes = np.mean(weighted_codes)\n    norm_comorbidity = avg_weighted_codes / 15.0  # Normalized\n\n    # 2. Visit frequency (unchanged)\n    num_visits = len(patient_sequence.visits)\n    time_span_years = (visits[-1].timestamp - visits[0].timestamp).days / 365.0\n    frequency = num_visits / max(time_span_years, 0.1)\n    norm_frequency = frequency / 5.0\n\n    # 3. Diagnosis diversity (refined)\n    # Use only ICD codes for diversity metric\n    all_icd_codes = []\n    for visit in patient_sequence.visits:\n        all_icd_codes.extend([c for c in visit.codes if c.type == 'ICD'])\n\n    if len(all_icd_codes) &gt; 0:\n        unique_icd = len(set(all_icd_codes))\n        total_icd = len(all_icd_codes)\n        icd_diversity = unique_icd / total_icd\n    else:\n        icd_diversity = 0.5  # Neutral if no diagnoses\n\n    # Low diversity = repeated diagnoses = chronic = high risk\n    risk_from_diversity = 1 - icd_diversity\n\n    # 4. Combine into risk score\n    risk_score = (\n        0.4 * norm_comorbidity +      # Weighted code burden\n        0.4 * norm_frequency +         # Visit frequency\n        0.2 * risk_from_diversity      # Diagnosis diversity\n    )\n\n    risk_score = np.clip(risk_score, 0.1, 0.9)\n\n    return risk_score\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#comparison-old-vs-new","title":"Comparison: Old vs. New","text":"<pre><code># Example patient: \n# - 5 ICD codes\n# - 3 LOINC codes\n# - 7 NDC codes\n# Total = 15 codes\n\n# OLD METHOD (unweighted)\ncomorbidity_old = 15 / 20.0 = 0.75\n\n# NEW METHOD (weighted)\nweighted = (1.0*5) + (0.4*3) + (0.3*7) = 5 + 1.2 + 2.1 = 8.3\ncomorbidity_new = 8.3 / 15.0 = 0.55\n\n# NEW is lower \u2192 more accurate reflection of moderate (not high) burden\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#future-directions","title":"Future Directions","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#1-severity-weighted-icd-codes","title":"1. Severity-Weighted ICD Codes","text":"<p>Idea: Not all diagnoses equal; weight by severity.</p> <pre><code># Example using ICD hierarchy\nseverity_weights = {\n    'I50.9': 1.0,   # Heart failure (severe)\n    'I10': 0.5,     # Essential HTN (moderate)\n    'Z23': 0.1      # Vaccination encounter (minimal)\n}\n\nweighted_icd_burden = sum(severity_weights.get(code, 0.7) for code in icd_codes)\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#2-temporal-comorbidity-evolution","title":"2. Temporal Comorbidity Evolution","text":"<p>Idea: Increasing comorbidity over time = higher risk.</p> <pre><code># Compare early vs. late visit comorbidity\nearly_burden = avg_comorbidity(visits[:3])\nlate_burden = avg_comorbidity(visits[-3:])\n\ncomorbidity_trend = late_burden - early_burden\n# Positive trend = worsening = higher risk\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#3-medication-class-analysis","title":"3. Medication Class Analysis","text":"<p>Idea: Weight medications by therapeutic class.</p> <pre><code>high_risk_meds = {\n    'anticoagulants': 1.5,   # High risk\n    'immunosuppressants': 1.3,\n    'chemotherapy': 1.5,\n    'insulin': 1.2,\n    'opioids': 1.1,\n    'vitamins': 0.1          # Low risk\n}\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#4-procedure-invasiveness-scoring","title":"4. Procedure Invasiveness Scoring","text":"<p>Idea: Major procedures indicate higher burden.</p> <pre><code># CPT-based invasiveness\ninvasiveness = {\n    'major_surgery': 2.0,    # CABG, transplant\n    'minor_surgery': 1.0,    # Excision, repair\n    'diagnostic': 0.5,       # Endoscopy, imaging\n    'minimal': 0.2           # Wound care, injections\n}\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#summary","title":"Summary","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#key-points","title":"Key Points","text":"<ol> <li>Aggregate code counts are imperfect proxies for comorbidity burden</li> <li>Different code types have different informative value:</li> <li>ICD (diagnoses) &gt; CPT (procedures) &gt; LOINC (labs) &gt; NDC (meds)</li> <li>Code-type weighting is a practical refinement (easy to implement)</li> <li>Condition counting is the gold standard (requires grouper software)</li> <li>Clinical validation is essential (test against known cases)</li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#practical-recommendations","title":"Practical Recommendations","text":"<p>For synthetic data generation: - Start with code-type weighted counts (Approach 1) - Validate correlation with event times (should remain r &lt; -0.5) - Conduct sensitivity analysis (compare metrics)</p> <p>For real-world risk modeling: - Use established comorbidity indices (Charlson/Elixhauser) - Map ICD codes to clinical conditions - Weight by severity and prognostic impact</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#revised-risk-factor-equation","title":"Revised Risk Factor Equation","text":"<p>Original (TUTORIAL_02): <pre><code>comorbidity = avg_codes_per_visit / 20.0\n</code></pre></p> <p>Refined (TUTORIAL_02a): <pre><code># Code-type weights\nweights = {'ICD': 1.0, 'CPT': 0.8, 'LOINC': 0.4, 'NDC': 0.3}\n\n# Weighted average\nweighted_codes = sum(weights[code.type] for code in visit.codes)\ncomorbidity = avg_weighted_codes / 15.0\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_02a_refining_comorbidity_proxies/#references","title":"References","text":"<ol> <li> <p>Elixhauser Comorbidity Index    Elixhauser et al. (1998). Medical Care, 36(1), 8-27.</p> </li> <li> <p>Charlson Comorbidity Index    Charlson et al. (1987). Journal of Chronic Diseases, 40(5), 373-383.</p> </li> <li> <p>Clinical Classifications Software (CCS)    AHRQ Healthcare Cost and Utilization Project (HCUP).    https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp</p> </li> <li> <p>ICD Code Hierarchies    WHO ICD-10-CM Official Guidelines for Coding and Reporting.</p> </li> </ol> <p>Related Documents: - Main Tutorial: Synthetic Data Design - Tutorial 1: Prediction Problem Definition - Tutorial 3: Loss Function Formulation</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/","title":"Tutorial 3: Loss Function Formulation","text":"<p>Part of: Discrete-Time Survival Analysis for EHR Sequences Audience: Researchers implementing survival models with deep learning</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Discrete-Time Survival Framework</li> <li>Likelihood Formulation</li> <li>Implementation Details</li> <li>Training Considerations</li> <li>Common Issues and Solutions</li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#overview","title":"Overview","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#the-goal","title":"The Goal","text":"<p>Train a neural network to predict hazard at each time point:</p> \\[h_t = P(T = t \\mid T \\geq t, H_t)\\] <p>Where: - \\(T\\): Time of event - \\(h_t\\): Probability of event at time \\(t\\) given survival to \\(t\\) - \\(H_t\\): Patient history up to time \\(t\\)</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#why-not-binary-cross-entropy","title":"Why Not Binary Cross-Entropy?","text":"<p>Binary classification approach (WRONG): <pre><code># Treat each visit as binary: event (1) or no event (0)\nloss = BCE(predictions, labels)\n</code></pre></p> <p>Problems: 1. Ignores survival information (patient survived to this visit) 2. Doesn't handle censoring properly 3. Treats all time points independently</p> <p>Survival approach (CORRECT): <pre><code># Model survival process explicitly\nloss = -log_likelihood(hazards, event_times, event_indicators)\n</code></pre></p> <p>Benefits: 1. Uses survival information from all visits before event 2. Handles censoring naturally 3. Respects temporal dependencies</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#discrete-time-survival-framework","title":"Discrete-Time Survival Framework","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#hazard-function","title":"Hazard Function","text":"<p>Definition: Probability of event at time \\(t\\) given survival to \\(t\\)</p> \\[h_t = P(T = t \\mid T \\geq t)\\] <p>Properties: - \\(0 \\leq h_t \\leq 1\\) (it's a probability) - Can vary arbitrarily over time (no parametric assumptions) - Predicted by neural network with sigmoid activation</p> <p>Example: <pre><code># Patient trajectory\nh_1 = 0.05  # Low risk at visit 1\nh_2 = 0.08  # Slightly higher at visit 2\nh_3 = 0.15  # Increasing risk at visit 3\nh_4 = 0.40  # High risk at visit 4 (event occurs)\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#survival-function","title":"Survival Function","text":"<p>Definition: Probability of surviving past time \\(t\\)</p> \\[S(t) = P(T &gt; t) = \\prod_{i=1}^{t} (1 - h_i)\\] <p>Interpretation: - Survival = not having event at any prior time - Product of \\((1 - h_i)\\) for all times up to \\(t\\)</p> <p>Example: <pre><code>S(1) = (1 - h_1) = 0.95\nS(2) = (1 - h_1)(1 - h_2) = 0.95 \u00d7 0.92 = 0.874\nS(3) = (1 - h_1)(1 - h_2)(1 - h_3) = 0.874 \u00d7 0.85 = 0.743\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#probability-mass-function","title":"Probability Mass Function","text":"<p>Definition: Probability of event exactly at time \\(t\\)</p> \\[P(T = t) = S(t-1) \\times h_t = \\left[\\prod_{i=1}^{t-1} (1 - h_i)\\right] \\times h_t\\] <p>Interpretation: - Survive to \\(t-1\\): \\(\\prod_{i=1}^{t-1} (1 - h_i)\\) - Then have event at \\(t\\): \\(h_t\\)</p> <p>Example: <pre><code># Probability of event at visit 3\nP(T = 3) = S(2) \u00d7 h_3\n         = (1 - h_1)(1 - h_2) \u00d7 h_3\n         = 0.95 \u00d7 0.92 \u00d7 0.15\n         = 0.131\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#likelihood-formulation","title":"Likelihood Formulation","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#for-a-single-patient","title":"For a Single Patient","text":"<p>Case 1: Event Observed (\u03b4 = 1)</p> <p>Patient has event at time \\(T\\):</p> \\[L = P(T = T) = \\left[\\prod_{t=1}^{T-1} (1 - h_t)\\right] \\times h_T\\] <p>Interpretation: - Survive through visits \\(1, 2, \\ldots, T-1\\): \\(\\prod_{t=1}^{T-1} (1 - h_t)\\) - Have event at visit \\(T\\): \\(h_T\\)</p> <p>Log-likelihood: $\\(\\log L = \\sum_{t=1}^{T-1} \\log(1 - h_t) + \\log(h_T)\\)$</p> <p>Case 2: Censored (\u03b4 = 0)</p> <p>Patient is censored at time \\(T\\) (no event observed):</p> \\[L = P(T &gt; T) = \\prod_{t=1}^{T} (1 - h_t)\\] <p>Interpretation: - Survive through all observed visits: \\(\\prod_{t=1}^{T} (1 - h_t)\\) - We don't know what happens after</p> <p>Log-likelihood: $\\(\\log L = \\sum_{t=1}^{T} \\log(1 - h_t)\\)$</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#combined-formulation","title":"Combined Formulation","text":"<p>For any patient:</p> \\[\\log L = \\sum_{t=1}^{T} \\log(1 - h_t) + \\delta \\cdot \\log(h_T)\\] <p>Where: - First term: Survival contribution (all patients) - Second term: Event contribution (only if \\(\\delta = 1\\))</p> <p>Batch Loss:</p> \\[\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[\\sum_{t=1}^{T_i} \\log(1 - h_{i,t}) + \\delta_i \\cdot \\log(h_{i,T_i})\\right]\\]"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#implementation-details","title":"Implementation Details","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#pytorch-implementation","title":"PyTorch Implementation","text":"<pre><code>class DiscreteTimeSurvivalLoss(nn.Module):\n    def __init__(self, eps=1e-7):\n        super().__init__()\n        self.eps = eps  # Numerical stability\n\n    def forward(self, hazards, event_times, event_indicators, sequence_mask):\n        \"\"\"\n        Args:\n            hazards: [batch_size, max_visits] in (0, 1)\n            event_times: [batch_size] - index of event/censoring\n            event_indicators: [batch_size] - 1 if event, 0 if censored\n            sequence_mask: [batch_size, max_visits] - valid visits\n\n        Returns:\n            Scalar loss (negative log-likelihood)\n        \"\"\"\n        batch_size, max_visits = hazards.shape\n\n        # Clamp hazards for numerical stability\n        hazards = torch.clamp(hazards, min=self.eps, max=1 - self.eps)\n\n        # Create time index tensor\n        time_idx = torch.arange(max_visits, device=hazards.device).unsqueeze(0)\n        event_times_expanded = event_times.unsqueeze(1)\n\n        # Mask for visits before event/censoring\n        before_event_mask = (time_idx &lt; event_times_expanded).float() * sequence_mask\n\n        # Mask for event visit\n        at_event_mask = (time_idx == event_times_expanded).float() * sequence_mask\n\n        # Survival log-likelihood: sum of log(1 - h_t) for t &lt; T\n        survival_ll = torch.sum(\n            torch.log(1 - hazards) * before_event_mask,\n            dim=1\n        )\n\n        # Event log-likelihood: log(h_T) if event occurred\n        event_ll = torch.sum(\n            torch.log(hazards) * at_event_mask,\n            dim=1\n        ) * event_indicators.float()\n\n        # Total log-likelihood per patient\n        log_likelihood = survival_ll + event_ll\n\n        # Return negative log-likelihood (to minimize)\n        return -torch.mean(log_likelihood)\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#step-by-step-example","title":"Step-by-Step Example","text":"<p>Patient data: <pre><code>hazards = [0.05, 0.08, 0.15, 0.40, 0.0]  # Padded to max_visits=5\nevent_time = 3  # Event at visit 3 (0-indexed)\nevent_indicator = 1  # Event observed\nsequence_mask = [1, 1, 1, 1, 0]  # 4 valid visits\n</code></pre></p> <p>Step 1: Create masks <pre><code>time_idx = [0, 1, 2, 3, 4]\nevent_time_expanded = 3\n\nbefore_event_mask = [1, 1, 1, 0, 0]  # Visits 0, 1, 2\nat_event_mask = [0, 0, 0, 1, 0]      # Visit 3\n</code></pre></p> <p>Step 2: Survival log-likelihood <pre><code>survival_ll = log(1 - 0.05) + log(1 - 0.08) + log(1 - 0.15)\n            = log(0.95) + log(0.92) + log(0.85)\n            = -0.051 - 0.083 - 0.163\n            = -0.297\n</code></pre></p> <p>Step 3: Event log-likelihood <pre><code>event_ll = log(0.40) \u00d7 1  # event_indicator = 1\n         = -0.916\n</code></pre></p> <p>Step 4: Total log-likelihood <pre><code>log_likelihood = -0.297 + (-0.916) = -1.213\n</code></pre></p> <p>Step 5: Loss (negative log-likelihood) <pre><code>loss = -(-1.213) = 1.213\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#numerical-stability","title":"Numerical Stability","text":"<p>Problem: <code>log(0)</code> is undefined</p> <p>Solution: Clamp hazards <pre><code>eps = 1e-7\nhazards = torch.clamp(hazards, min=eps, max=1 - eps)\n\n# Now:\n# log(1 - hazards) is safe (1 - hazards &gt;= eps)\n# log(hazards) is safe (hazards &gt;= eps)\n</code></pre></p> <p>Why this works: - <code>eps = 1e-7</code> is tiny (0.0000001) - Doesn't affect predictions (hazards are typically 0.01-0.9) - Prevents numerical overflow/underflow</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#training-considerations","title":"Training Considerations","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#1-masking-for-variable-length-sequences","title":"1. Masking for Variable-Length Sequences","text":"<p>Problem: Patients have different numbers of visits</p> <p>Solution: Use sequence mask <pre><code># Only compute loss for valid visits\nsurvival_ll = torch.sum(\n    torch.log(1 - hazards) * before_event_mask * sequence_mask,\n    dim=1\n)\n</code></pre></p> <p>Example: <pre><code># Patient 1: 10 visits\nsequence_mask[0] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...]\n\n# Patient 2: 5 visits\nsequence_mask[1] = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...]\n\n# Loss only computed for valid visits (mask = 1)\n</code></pre></p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#2-handling-censoring","title":"2. Handling Censoring","text":"<p>Censored patient (\u03b4 = 0): <pre><code># Only survival term contributes\nlog_likelihood = survival_ll + 0 * event_ll\n               = survival_ll\n</code></pre></p> <p>Observed event (\u03b4 = 1): <pre><code># Both terms contribute\nlog_likelihood = survival_ll + 1 * event_ll\n               = survival_ll + event_ll\n</code></pre></p> <p>Key insight: Censored patients still provide information through survival term!</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#3-gradient-flow","title":"3. Gradient Flow","text":"<p>Survival term gradient: <pre><code>\u2202L/\u2202h_t = -1/(1 - h_t)  for t &lt; T\n</code></pre> - Encourages low hazard before event - Stronger gradient when \\(h_t\\) is high (bad prediction)</p> <p>Event term gradient: <pre><code>\u2202L/\u2202h_T = -1/h_T  for t = T (if event)\n</code></pre> - Encourages high hazard at event time - Stronger gradient when \\(h_T\\) is low (bad prediction)</p> <p>Combined effect: - Model learns to predict low hazard early - High hazard at event time - Smooth transition between them</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#4-batch-size-considerations","title":"4. Batch Size Considerations","text":"<p>Small batches (16-32): - Faster iterations - More noise in gradient - Better for small datasets</p> <p>Large batches (64-128): - Stable gradients - Slower iterations - Better for large datasets</p> <p>Recommendation: Start with 32, adjust based on dataset size</p>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#issue-1-loss-not-decreasing","title":"Issue 1: Loss Not Decreasing","text":"<p>Symptoms: - Loss stays constant or increases - Model predicts same hazard for all patients</p> <p>Possible causes:</p> <ol> <li> <p>Learning rate too high <pre><code># Try smaller learning rate\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Instead of 1e-3\n</code></pre></p> </li> <li> <p>Numerical instability <pre><code># Ensure epsilon clamping\nhazards = torch.clamp(hazards, min=1e-7, max=1-1e-7)\n</code></pre></p> </li> <li> <p>Weak synthetic data correlation <pre><code># Check correlation\ncorrelation = np.corrcoef(risk_scores, event_times)[0, 1]\n# Should be r &lt; -0.5\n</code></pre></p> </li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#issue-2-exploding-gradients","title":"Issue 2: Exploding Gradients","text":"<p>Symptoms: - Loss becomes NaN - Gradients become very large</p> <p>Solutions:</p> <ol> <li> <p>Gradient clipping <pre><code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n</code></pre></p> </li> <li> <p>Check for log(0) <pre><code># Ensure epsilon clamping in loss function\nhazards = torch.clamp(hazards, min=self.eps, max=1 - self.eps)\n</code></pre></p> </li> <li> <p>Reduce learning rate <pre><code>optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n</code></pre></p> </li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#issue-3-overfitting","title":"Issue 3: Overfitting","text":"<p>Symptoms: - Training loss decreases, validation loss increases - Large gap between train and validation C-index</p> <p>Solutions:</p> <ol> <li> <p>Dropout <pre><code>model = DiscreteTimeSurvivalLSTM(\n    vocab_size=vocab_size,\n    dropout=0.3  # Increase dropout\n)\n</code></pre></p> </li> <li> <p>L2 regularization <pre><code>optimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=1e-3,\n    weight_decay=1e-4  # L2 penalty\n)\n</code></pre></p> </li> <li> <p>Early stopping <pre><code>if val_loss &gt; best_val_loss:\n    patience_counter += 1\n    if patience_counter &gt;= patience:\n        break  # Stop training\n</code></pre></p> </li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#issue-4-poor-c-index-despite-low-loss","title":"Issue 4: Poor C-index Despite Low Loss","text":"<p>Symptoms: - Loss decreases normally - C-index stays around 0.5 (random)</p> <p>Possible causes:</p> <ol> <li> <p>Length bias in risk score <pre><code># WRONG: Sum of hazards (length-dependent)\nrisk_score = hazards.sum(dim=1)\n\n# CORRECT: Fixed-horizon mean\nrisk_score = hazards[:, :10].mean(dim=1)\n</code></pre></p> </li> <li> <p>Weak correlation in synthetic data <pre><code># Regenerate with stronger correlation\ngenerator = DiscreteTimeSurvivalGenerator(\n    time_scale=0.3,\n    noise_std=0.05  # Reduce noise\n)\n</code></pre></p> </li> <li> <p>Model not learning from data <pre><code># Check if model is actually using input\n# Try predicting with random input - should be worse\n</code></pre></p> </li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#comparison-with-other-losses","title":"Comparison with Other Losses","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#binary-cross-entropy-wrong","title":"Binary Cross-Entropy (WRONG)","text":"<pre><code># Treats each visit independently\nloss = BCE(hazards, labels)\n\n# Problems:\n# - Doesn't use survival information\n# - Doesn't handle censoring\n# - Ignores temporal structure\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#mean-squared-error-wrong","title":"Mean Squared Error (WRONG)","text":"<pre><code># Regression on event time\nloss = MSE(predicted_time, actual_time)\n\n# Problems:\n# - Doesn't handle censoring\n# - Assumes Gaussian errors (wrong for time-to-event)\n# - Doesn't model hazard process\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#discrete-time-survival-correct","title":"Discrete-Time Survival (CORRECT)","text":"<pre><code># Models survival process explicitly\nloss = -log_likelihood(hazards, event_times, event_indicators)\n\n# Benefits:\n# - Uses all survival information\n# - Handles censoring naturally\n# - Respects temporal dependencies\n# - Theoretically grounded\n</code></pre>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Discrete-time survival loss models the survival process explicitly</li> <li>Two components: Survival term (all patients) + Event term (observed events only)</li> <li>Handles censoring naturally through likelihood formulation</li> <li>Numerical stability requires epsilon clamping</li> <li>Masking is essential for variable-length sequences</li> <li>C-index evaluation requires careful risk score formulation to avoid length bias</li> </ol>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#further-reading","title":"Further Reading","text":""},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#theory","title":"Theory","text":"<ul> <li>Singer &amp; Willett (2003). Applied Longitudinal Data Analysis - Chapter 10</li> <li>Tutz &amp; Schmid (2016). Modeling Discrete Time-to-Event Data</li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#implementation","title":"Implementation","text":"<ul> <li>PyHealth: <code>pyhealth.models.DeepSurv</code></li> <li>PyCox: <code>pycox.models.LogisticHazard</code></li> </ul>"},{"location":"notebooks/02_survival_analysis/TUTORIAL_03_loss_function/#related-approaches","title":"Related Approaches","text":"<ul> <li>DeepSurv (continuous-time with Cox model)</li> <li>DeepHit (competing risks)</li> <li>DRSA (deep recurrent survival analysis)</li> </ul> <p>Previous Tutorial: Synthetic Data Design Notebook: 01_discrete_time_survival_lstm.ipynb</p>"},{"location":"notebooks/02_survival_analysis/test_synthetic_outcomes/","title":"Test synthetic outcomes","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nFast validation script for synthetic survival outcome generation.\n\nThis script quickly validates that the DiscreteTimeSurvivalGenerator produces\nrealistic outcomes with proper risk-time correlation, without running the full\nnotebook training pipeline.\n\nUsage:\n    # Quick test with 50 patients (default)\n    python test_synthetic_outcomes.py\n    \n    # Test with more patients\n    python test_synthetic_outcomes.py --max-patients 200\n    \n    # Test different parameters\n    python test_synthetic_outcomes.py --censoring-rate 0.4 --time-scale 0.5\n    \n    # Show visualizations\n    python test_synthetic_outcomes.py --plot\n\"\"\"\n</pre> \"\"\" Fast validation script for synthetic survival outcome generation.  This script quickly validates that the DiscreteTimeSurvivalGenerator produces realistic outcomes with proper risk-time correlation, without running the full notebook training pipeline.  Usage:     # Quick test with 50 patients (default)     python test_synthetic_outcomes.py          # Test with more patients     python test_synthetic_outcomes.py --max-patients 200          # Test different parameters     python test_synthetic_outcomes.py --censoring-rate 0.4 --time-scale 0.5          # Show visualizations     python test_synthetic_outcomes.py --plot \"\"\" In\u00a0[\u00a0]: Copied! <pre>import sys\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nimport scipy.stats as stats\nimport torch\nfrom collections import defaultdict\n</pre> import sys import argparse from pathlib import Path import numpy as np import scipy.stats as stats import torch from collections import defaultdict In\u00a0[\u00a0]: Copied! <pre># Add src to path\nsys.path.insert(0, str(Path(__file__).parent.parent.parent / 'src'))\n</pre> # Add src to path sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'src')) In\u00a0[\u00a0]: Copied! <pre>from ehrsequencing.data.adapters.synthea import SyntheaAdapter\nfrom ehrsequencing.data.visit_grouper import VisitGrouper\nfrom ehrsequencing.data.sequence_builder import PatientSequenceBuilder\nfrom ehrsequencing.synthetic.survival import DiscreteTimeSurvivalGenerator\n</pre> from ehrsequencing.data.adapters.synthea import SyntheaAdapter from ehrsequencing.data.visit_grouper import VisitGrouper from ehrsequencing.data.sequence_builder import PatientSequenceBuilder from ehrsequencing.synthetic.survival import DiscreteTimeSurvivalGenerator In\u00a0[\u00a0]: Copied! <pre>def parse_args():\n    \"\"\"Parse command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Validate synthetic survival outcome generation'\n    )\n    parser.add_argument(\n        '--max-patients',\n        type=int,\n        default=50,\n        help='Number of patients to test (default: 50 for speed)'\n    )\n    parser.add_argument(\n        '--data-dir',\n        type=Path,\n        default=Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'large_cohort_1000',\n        help='Path to Synthea data directory'\n    )\n    parser.add_argument(\n        '--censoring-rate',\n        type=float,\n        default=0.3,\n        help='Censoring rate (default: 0.3)'\n    )\n    parser.add_argument(\n        '--time-scale',\n        type=float,\n        default=0.3,\n        help='Time scale parameter (default: 0.3)'\n    )\n    parser.add_argument(\n        '--seed',\n        type=int,\n        default=42,\n        help='Random seed (default: 42)'\n    )\n    parser.add_argument(\n        '--plot',\n        action='store_true',\n        help='Show matplotlib plots (requires display)'\n    )\n    parser.add_argument(\n        '--verbose',\n        action='store_true',\n        help='Show detailed output'\n    )\n    parser.add_argument(\n        '--save',\n        type=Path,\n        default=None,\n        help='Save validated outcomes to file (e.g., synthetic_outcomes.pt)'\n    )\n    \n    return parser.parse_args()\n</pre> def parse_args():     \"\"\"Parse command line arguments.\"\"\"     parser = argparse.ArgumentParser(         description='Validate synthetic survival outcome generation'     )     parser.add_argument(         '--max-patients',         type=int,         default=50,         help='Number of patients to test (default: 50 for speed)'     )     parser.add_argument(         '--data-dir',         type=Path,         default=Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'large_cohort_1000',         help='Path to Synthea data directory'     )     parser.add_argument(         '--censoring-rate',         type=float,         default=0.3,         help='Censoring rate (default: 0.3)'     )     parser.add_argument(         '--time-scale',         type=float,         default=0.3,         help='Time scale parameter (default: 0.3)'     )     parser.add_argument(         '--seed',         type=int,         default=42,         help='Random seed (default: 42)'     )     parser.add_argument(         '--plot',         action='store_true',         help='Show matplotlib plots (requires display)'     )     parser.add_argument(         '--verbose',         action='store_true',         help='Show detailed output'     )     parser.add_argument(         '--save',         type=Path,         default=None,         help='Save validated outcomes to file (e.g., synthetic_outcomes.pt)'     )          return parser.parse_args() In\u00a0[\u00a0]: Copied! <pre>def load_data(args):\n    \"\"\"Load and prepare patient sequences.\"\"\"\n    print(\"=\"*70)\n    print(\"LOADING DATA\")\n    print(\"=\"*70)\n    \n    # Load Synthea data\n    print(f\"\\nLoading data from: {args.data_dir}\")\n    adapter = SyntheaAdapter(args.data_dir)\n    events = adapter.load_events()\n    print(f\"\u2713 Loaded {len(events)} events\")\n    \n    # Group into visits\n    grouper = VisitGrouper(time_window_hours=24)\n    visits = grouper.group_events(events)\n    print(f\"\u2713 Created {len(visits)} visits\")\n    \n    # Group by patient\n    visits_by_patient = defaultdict(list)\n    for visit in visits:\n        visits_by_patient[visit.patient_id].append(visit)\n    \n    # Sort visits by timestamp\n    for patient_id in visits_by_patient:\n        visits_by_patient[patient_id].sort(key=lambda v: v.timestamp)\n    \n    total_patients = len(visits_by_patient)\n    print(f\"\u2713 Organized {total_patients} patients\")\n    \n    # Subsample if needed\n    if args.max_patients and total_patients &gt; args.max_patients:\n        print(f\"\\n\u26a0\ufe0f  Subsampling to {args.max_patients} patients for speed\")\n        np.random.seed(args.seed)\n        sampled_patient_ids = np.random.choice(\n            list(visits_by_patient.keys()),\n            size=args.max_patients,\n            replace=False\n        )\n        visits_by_patient = {\n            pid: visits_by_patient[pid]\n            for pid in sampled_patient_ids\n        }\n        print(f\"\u2713 Using {len(visits_by_patient)} patients\")\n    \n    # Build sequences\n    builder = PatientSequenceBuilder()\n    builder.build_vocabulary(list(visits_by_patient.values()))\n    sequences = builder.build_sequences(list(visits_by_patient.values()), min_visits=2)\n    \n    print(f\"\u2713 Built {len(sequences)} sequences\")\n    print(f\"\u2713 Vocabulary size: {builder.vocabulary_size}\")\n    \n    return sequences\n</pre> def load_data(args):     \"\"\"Load and prepare patient sequences.\"\"\"     print(\"=\"*70)     print(\"LOADING DATA\")     print(\"=\"*70)          # Load Synthea data     print(f\"\\nLoading data from: {args.data_dir}\")     adapter = SyntheaAdapter(args.data_dir)     events = adapter.load_events()     print(f\"\u2713 Loaded {len(events)} events\")          # Group into visits     grouper = VisitGrouper(time_window_hours=24)     visits = grouper.group_events(events)     print(f\"\u2713 Created {len(visits)} visits\")          # Group by patient     visits_by_patient = defaultdict(list)     for visit in visits:         visits_by_patient[visit.patient_id].append(visit)          # Sort visits by timestamp     for patient_id in visits_by_patient:         visits_by_patient[patient_id].sort(key=lambda v: v.timestamp)          total_patients = len(visits_by_patient)     print(f\"\u2713 Organized {total_patients} patients\")          # Subsample if needed     if args.max_patients and total_patients &gt; args.max_patients:         print(f\"\\n\u26a0\ufe0f  Subsampling to {args.max_patients} patients for speed\")         np.random.seed(args.seed)         sampled_patient_ids = np.random.choice(             list(visits_by_patient.keys()),             size=args.max_patients,             replace=False         )         visits_by_patient = {             pid: visits_by_patient[pid]             for pid in sampled_patient_ids         }         print(f\"\u2713 Using {len(visits_by_patient)} patients\")          # Build sequences     builder = PatientSequenceBuilder()     builder.build_vocabulary(list(visits_by_patient.values()))     sequences = builder.build_sequences(list(visits_by_patient.values()), min_visits=2)          print(f\"\u2713 Built {len(sequences)} sequences\")     print(f\"\u2713 Vocabulary size: {builder.vocabulary_size}\")          return sequences In\u00a0[\u00a0]: Copied! <pre>def generate_outcomes(args, sequences):\n    \"\"\"Generate synthetic outcomes.\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"GENERATING SYNTHETIC OUTCOMES\")\n    print(\"=\"*70)\n    \n    generator = DiscreteTimeSurvivalGenerator(\n        censoring_rate=args.censoring_rate,\n        risk_weights={'comorbidity': 0.4, 'frequency': 0.4, 'diversity': 0.2},\n        time_scale=args.time_scale,\n        seed=args.seed\n    )\n    \n    outcome = generator.generate(sequences)\n    \n    print(f\"\\nGenerated outcomes for {len(sequences)} patients\")\n    print(f\"  \u2022 Event rate: {outcome.event_indicators.float().mean():.1%}\")\n    print(f\"  \u2022 Median event/censoring time: {outcome.event_times.float().median():.1f} visits\")\n    print(f\"  \u2022 Mean risk score: {outcome.risk_scores.mean():.3f}\")\n    print(f\"  \u2022 Risk score range: [{outcome.risk_scores.min():.3f}, {outcome.risk_scores.max():.3f}]\")\n    \n    return outcome\n</pre> def generate_outcomes(args, sequences):     \"\"\"Generate synthetic outcomes.\"\"\"     print(\"\\n\" + \"=\"*70)     print(\"GENERATING SYNTHETIC OUTCOMES\")     print(\"=\"*70)          generator = DiscreteTimeSurvivalGenerator(         censoring_rate=args.censoring_rate,         risk_weights={'comorbidity': 0.4, 'frequency': 0.4, 'diversity': 0.2},         time_scale=args.time_scale,         seed=args.seed     )          outcome = generator.generate(sequences)          print(f\"\\nGenerated outcomes for {len(sequences)} patients\")     print(f\"  \u2022 Event rate: {outcome.event_indicators.float().mean():.1%}\")     print(f\"  \u2022 Median event/censoring time: {outcome.event_times.float().median():.1f} visits\")     print(f\"  \u2022 Mean risk score: {outcome.risk_scores.mean():.3f}\")     print(f\"  \u2022 Risk score range: [{outcome.risk_scores.min():.3f}, {outcome.risk_scores.max():.3f}]\")          return outcome In\u00a0[\u00a0]: Copied! <pre>def validate_correlation(outcome, sequences, verbose=False):\n    \"\"\"Validate risk-time correlation.\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"CORRELATION VALIDATION\")\n    print(\"=\"*70)\n    \n    # Only look at patients with events (not censored)\n    event_mask = outcome.event_indicators == 1\n    event_risk_scores = outcome.risk_scores[event_mask].numpy()\n    event_times = outcome.event_times[event_mask].numpy()\n    \n    if len(event_risk_scores) &lt; 2:\n        print(\"\u26a0\ufe0f  Not enough events to compute correlation\")\n        return False\n    \n    # Compute correlation\n    correlation, p_value = stats.pearsonr(event_risk_scores, event_times)\n    \n    print(f\"\\nCorrelation between risk score and event time:\")\n    print(f\"  \u2022 Pearson r = {correlation:.3f} (p={p_value:.4f})\")\n    print(f\"  \u2022 Expected: NEGATIVE correlation (high risk \u2192 early events)\")\n    print(f\"  \u2022 Sample size: {len(event_risk_scores)} events\")\n    \n    # Assess correlation strength\n    success = False\n    if correlation &gt; 0:\n        print(f\"\\n\u274c FAIL: POSITIVE correlation detected!\")\n        print(f\"   High-risk patients have LATE events (backwards)\")\n        print(f\"   The model will learn the wrong relationship\")\n    elif correlation &lt; -0.5:\n        print(f\"\\n\u2705 PASS: Strong negative correlation\")\n        print(f\"   High-risk patients have early events (correct)\")\n        success = True\n    elif correlation &lt; -0.3:\n        print(f\"\\n\u26a0\ufe0f  WARNING: Moderate negative correlation\")\n        print(f\"   Correlation exists but could be stronger\")\n        print(f\"   Consider adjusting time_scale or noise parameters\")\n        success = True\n    else:\n        print(f\"\\n\u274c FAIL: Weak correlation\")\n        print(f\"   Outcomes are too random, risk signal is lost\")\n    \n    # Show examples\n    if verbose or not success:\n        print(f\"\\nExample patients (with events):\")\n        print(f\"{'Risk Score':&lt;12} {'Event Time':&lt;12} {'Expected':&lt;20}\")\n        print(\"-\" * 50)\n        \n        num_examples = min(15, len(event_risk_scores))\n        median_time = np.median(event_times)\n        \n        for i in range(num_examples):\n            expected = \"Early\" if event_risk_scores[i] &gt; 0.6 else \"Late\"\n            actual = \"Early\" if event_times[i] &lt; median_time else \"Late\"\n            match = \"\u2713\" if expected == actual else \"\u2717\"\n            print(f\"{event_risk_scores[i]:.3f}        {event_times[i]:&lt;12} \"\n                  f\"{expected:&lt;10} (actual: {actual}) {match}\")\n    \n    return success\n</pre> def validate_correlation(outcome, sequences, verbose=False):     \"\"\"Validate risk-time correlation.\"\"\"     print(\"\\n\" + \"=\"*70)     print(\"CORRELATION VALIDATION\")     print(\"=\"*70)          # Only look at patients with events (not censored)     event_mask = outcome.event_indicators == 1     event_risk_scores = outcome.risk_scores[event_mask].numpy()     event_times = outcome.event_times[event_mask].numpy()          if len(event_risk_scores) &lt; 2:         print(\"\u26a0\ufe0f  Not enough events to compute correlation\")         return False          # Compute correlation     correlation, p_value = stats.pearsonr(event_risk_scores, event_times)          print(f\"\\nCorrelation between risk score and event time:\")     print(f\"  \u2022 Pearson r = {correlation:.3f} (p={p_value:.4f})\")     print(f\"  \u2022 Expected: NEGATIVE correlation (high risk \u2192 early events)\")     print(f\"  \u2022 Sample size: {len(event_risk_scores)} events\")          # Assess correlation strength     success = False     if correlation &gt; 0:         print(f\"\\n\u274c FAIL: POSITIVE correlation detected!\")         print(f\"   High-risk patients have LATE events (backwards)\")         print(f\"   The model will learn the wrong relationship\")     elif correlation &lt; -0.5:         print(f\"\\n\u2705 PASS: Strong negative correlation\")         print(f\"   High-risk patients have early events (correct)\")         success = True     elif correlation &lt; -0.3:         print(f\"\\n\u26a0\ufe0f  WARNING: Moderate negative correlation\")         print(f\"   Correlation exists but could be stronger\")         print(f\"   Consider adjusting time_scale or noise parameters\")         success = True     else:         print(f\"\\n\u274c FAIL: Weak correlation\")         print(f\"   Outcomes are too random, risk signal is lost\")          # Show examples     if verbose or not success:         print(f\"\\nExample patients (with events):\")         print(f\"{'Risk Score':&lt;12} {'Event Time':&lt;12} {'Expected':&lt;20}\")         print(\"-\" * 50)                  num_examples = min(15, len(event_risk_scores))         median_time = np.median(event_times)                  for i in range(num_examples):             expected = \"Early\" if event_risk_scores[i] &gt; 0.6 else \"Late\"             actual = \"Early\" if event_times[i] &lt; median_time else \"Late\"             match = \"\u2713\" if expected == actual else \"\u2717\"             print(f\"{event_risk_scores[i]:.3f}        {event_times[i]:&lt;12} \"                   f\"{expected:&lt;10} (actual: {actual}) {match}\")          return success In\u00a0[\u00a0]: Copied! <pre>def validate_distribution(outcome, verbose=False):\n    \"\"\"Validate outcome distributions.\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"DISTRIBUTION VALIDATION\")\n    print(\"=\"*70)\n    \n    event_mask = outcome.event_indicators == 1\n    censored_mask = outcome.event_indicators == 0\n    \n    # Check event rate\n    event_rate = outcome.event_indicators.float().mean().item()\n    print(f\"\\nEvent rate: {event_rate:.1%}\")\n    \n    if abs(event_rate - (1 - 0.3)) &lt; 0.15:  # Within 15% of expected\n        print(f\"  \u2713 Close to expected {1-0.3:.1%} (censoring_rate=0.3)\")\n    else:\n        print(f\"  \u26a0\ufe0f  Differs from expected {1-0.3:.1%}\")\n    \n    # Check risk score distribution\n    event_risk = outcome.risk_scores[event_mask].mean().item()\n    censored_risk = outcome.risk_scores[censored_mask].mean().item() if censored_mask.sum() &gt; 0 else 0\n    \n    print(f\"\\nRisk scores:\")\n    print(f\"  \u2022 Events:   mean={event_risk:.3f}\")\n    print(f\"  \u2022 Censored: mean={censored_risk:.3f}\")\n    \n    if event_risk &gt; censored_risk:\n        print(f\"  \u2713 Events have higher risk (correct)\")\n    else:\n        print(f\"  \u26a0\ufe0f  Censored patients have higher risk (unexpected)\")\n    \n    # Check time distribution\n    event_times = outcome.event_times[event_mask]\n    censored_times = outcome.event_times[censored_mask]\n    \n    if len(event_times) &gt; 0:\n        print(f\"\\nEvent times:\")\n        print(f\"  \u2022 Mean: {event_times.float().mean():.1f} visits\")\n        print(f\"  \u2022 Median: {event_times.float().median():.1f} visits\")\n        print(f\"  \u2022 Range: [{event_times.min()}, {event_times.max()}]\")\n    \n    if verbose and len(censored_times) &gt; 0:\n        print(f\"\\nCensoring times:\")\n        print(f\"  \u2022 Mean: {censored_times.float().mean():.1f} visits\")\n        print(f\"  \u2022 Median: {censored_times.float().median():.1f} visits\")\n</pre> def validate_distribution(outcome, verbose=False):     \"\"\"Validate outcome distributions.\"\"\"     print(\"\\n\" + \"=\"*70)     print(\"DISTRIBUTION VALIDATION\")     print(\"=\"*70)          event_mask = outcome.event_indicators == 1     censored_mask = outcome.event_indicators == 0          # Check event rate     event_rate = outcome.event_indicators.float().mean().item()     print(f\"\\nEvent rate: {event_rate:.1%}\")          if abs(event_rate - (1 - 0.3)) &lt; 0.15:  # Within 15% of expected         print(f\"  \u2713 Close to expected {1-0.3:.1%} (censoring_rate=0.3)\")     else:         print(f\"  \u26a0\ufe0f  Differs from expected {1-0.3:.1%}\")          # Check risk score distribution     event_risk = outcome.risk_scores[event_mask].mean().item()     censored_risk = outcome.risk_scores[censored_mask].mean().item() if censored_mask.sum() &gt; 0 else 0          print(f\"\\nRisk scores:\")     print(f\"  \u2022 Events:   mean={event_risk:.3f}\")     print(f\"  \u2022 Censored: mean={censored_risk:.3f}\")          if event_risk &gt; censored_risk:         print(f\"  \u2713 Events have higher risk (correct)\")     else:         print(f\"  \u26a0\ufe0f  Censored patients have higher risk (unexpected)\")          # Check time distribution     event_times = outcome.event_times[event_mask]     censored_times = outcome.event_times[censored_mask]          if len(event_times) &gt; 0:         print(f\"\\nEvent times:\")         print(f\"  \u2022 Mean: {event_times.float().mean():.1f} visits\")         print(f\"  \u2022 Median: {event_times.float().median():.1f} visits\")         print(f\"  \u2022 Range: [{event_times.min()}, {event_times.max()}]\")          if verbose and len(censored_times) &gt; 0:         print(f\"\\nCensoring times:\")         print(f\"  \u2022 Mean: {censored_times.float().mean():.1f} visits\")         print(f\"  \u2022 Median: {censored_times.float().median():.1f} visits\") In\u00a0[\u00a0]: Copied! <pre>def plot_results(outcome, sequences):\n    \"\"\"Plot outcome distributions.\"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n        sns.set_style('whitegrid')\n    except ImportError:\n        print(\"\\n\u26a0\ufe0f  Matplotlib not available, skipping plots\")\n        return\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"GENERATING PLOTS\")\n    print(\"=\"*70)\n    \n    event_mask = outcome.event_indicators == 1\n    \n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    \n    # 1. Risk score distribution\n    axes[0, 0].hist(outcome.risk_scores[event_mask].numpy(), bins=20, \n                    alpha=0.7, color='red', label='Events')\n    axes[0, 0].hist(outcome.risk_scores[~event_mask].numpy(), bins=20,\n                    alpha=0.7, color='gray', label='Censored')\n    axes[0, 0].set_xlabel('Risk Score')\n    axes[0, 0].set_ylabel('Count')\n    axes[0, 0].set_title('Risk Score Distribution')\n    axes[0, 0].legend()\n    \n    # 2. Event time distribution\n    axes[0, 1].hist(outcome.event_times[event_mask].numpy(), bins=20,\n                    alpha=0.7, color='red', label='Events')\n    axes[0, 1].hist(outcome.event_times[~event_mask].numpy(), bins=20,\n                    alpha=0.7, color='gray', label='Censored')\n    axes[0, 1].set_xlabel('Time (visits)')\n    axes[0, 1].set_ylabel('Count')\n    axes[0, 1].set_title('Event Time Distribution')\n    axes[0, 1].legend()\n    \n    # 3. Risk vs Time scatter\n    colors = ['red' if e == 1 else 'gray' for e in outcome.event_indicators]\n    axes[1, 0].scatter(outcome.risk_scores.numpy(), outcome.event_times.numpy(),\n                       c=colors, alpha=0.6, s=30)\n    axes[1, 0].set_xlabel('Risk Score')\n    axes[1, 0].set_ylabel('Event Time (visits)')\n    axes[1, 0].set_title('Risk vs Event Time\\n(Red=Event, Gray=Censored)')\n    \n    # Add correlation line for events only\n    event_risk = outcome.risk_scores[event_mask].numpy()\n    event_time = outcome.event_times[event_mask].numpy()\n    if len(event_risk) &gt; 1:\n        z = np.polyfit(event_risk, event_time, 1)\n        p = np.poly1d(z)\n        x_line = np.linspace(event_risk.min(), event_risk.max(), 100)\n        axes[1, 0].plot(x_line, p(x_line), \"r--\", alpha=0.8, linewidth=2,\n                        label=f'Trend (events only)')\n        axes[1, 0].legend()\n    \n    # 4. Sequence length distribution\n    seq_lengths = [len(seq.visits) for seq in sequences]\n    axes[1, 1].hist(seq_lengths, bins=20, alpha=0.7, color='blue')\n    axes[1, 1].set_xlabel('Number of Visits')\n    axes[1, 1].set_ylabel('Count')\n    axes[1, 1].set_title('Patient Sequence Lengths')\n    \n    plt.tight_layout()\n    plt.savefig('synthetic_outcomes_validation.png', dpi=150, bbox_inches='tight')\n    print(\"\\n\u2713 Saved plot to: synthetic_outcomes_validation.png\")\n    plt.show()\n</pre> def plot_results(outcome, sequences):     \"\"\"Plot outcome distributions.\"\"\"     try:         import matplotlib.pyplot as plt         import seaborn as sns         sns.set_style('whitegrid')     except ImportError:         print(\"\\n\u26a0\ufe0f  Matplotlib not available, skipping plots\")         return          print(\"\\n\" + \"=\"*70)     print(\"GENERATING PLOTS\")     print(\"=\"*70)          event_mask = outcome.event_indicators == 1          fig, axes = plt.subplots(2, 2, figsize=(12, 10))          # 1. Risk score distribution     axes[0, 0].hist(outcome.risk_scores[event_mask].numpy(), bins=20,                      alpha=0.7, color='red', label='Events')     axes[0, 0].hist(outcome.risk_scores[~event_mask].numpy(), bins=20,                     alpha=0.7, color='gray', label='Censored')     axes[0, 0].set_xlabel('Risk Score')     axes[0, 0].set_ylabel('Count')     axes[0, 0].set_title('Risk Score Distribution')     axes[0, 0].legend()          # 2. Event time distribution     axes[0, 1].hist(outcome.event_times[event_mask].numpy(), bins=20,                     alpha=0.7, color='red', label='Events')     axes[0, 1].hist(outcome.event_times[~event_mask].numpy(), bins=20,                     alpha=0.7, color='gray', label='Censored')     axes[0, 1].set_xlabel('Time (visits)')     axes[0, 1].set_ylabel('Count')     axes[0, 1].set_title('Event Time Distribution')     axes[0, 1].legend()          # 3. Risk vs Time scatter     colors = ['red' if e == 1 else 'gray' for e in outcome.event_indicators]     axes[1, 0].scatter(outcome.risk_scores.numpy(), outcome.event_times.numpy(),                        c=colors, alpha=0.6, s=30)     axes[1, 0].set_xlabel('Risk Score')     axes[1, 0].set_ylabel('Event Time (visits)')     axes[1, 0].set_title('Risk vs Event Time\\n(Red=Event, Gray=Censored)')          # Add correlation line for events only     event_risk = outcome.risk_scores[event_mask].numpy()     event_time = outcome.event_times[event_mask].numpy()     if len(event_risk) &gt; 1:         z = np.polyfit(event_risk, event_time, 1)         p = np.poly1d(z)         x_line = np.linspace(event_risk.min(), event_risk.max(), 100)         axes[1, 0].plot(x_line, p(x_line), \"r--\", alpha=0.8, linewidth=2,                         label=f'Trend (events only)')         axes[1, 0].legend()          # 4. Sequence length distribution     seq_lengths = [len(seq.visits) for seq in sequences]     axes[1, 1].hist(seq_lengths, bins=20, alpha=0.7, color='blue')     axes[1, 1].set_xlabel('Number of Visits')     axes[1, 1].set_ylabel('Count')     axes[1, 1].set_title('Patient Sequence Lengths')          plt.tight_layout()     plt.savefig('synthetic_outcomes_validation.png', dpi=150, bbox_inches='tight')     print(\"\\n\u2713 Saved plot to: synthetic_outcomes_validation.png\")     plt.show() In\u00a0[\u00a0]: Copied! <pre>def save_outcomes(outcome, sequences, filepath, args):\n    \"\"\"Save validated outcomes and sequences to file.\"\"\"\n    print(f\"\\nSaving outcomes to: {filepath}\")\n    \n    # Save as a dictionary with all necessary data\n    save_data = {\n        'outcome': {\n            'event_times': outcome.event_times,\n            'event_indicators': outcome.event_indicators,\n            'risk_scores': outcome.risk_scores,\n            'metadata': outcome.metadata,\n        },\n        'sequences': sequences,\n        'config': {\n            'max_patients': args.max_patients,\n            'censoring_rate': args.censoring_rate,\n            'time_scale': args.time_scale,\n            'seed': args.seed,\n        },\n        'num_patients': len(sequences),\n    }\n    \n    torch.save(save_data, filepath)\n    print(f\"\u2713 Saved {len(sequences)} sequences with outcomes\")\n</pre> def save_outcomes(outcome, sequences, filepath, args):     \"\"\"Save validated outcomes and sequences to file.\"\"\"     print(f\"\\nSaving outcomes to: {filepath}\")          # Save as a dictionary with all necessary data     save_data = {         'outcome': {             'event_times': outcome.event_times,             'event_indicators': outcome.event_indicators,             'risk_scores': outcome.risk_scores,             'metadata': outcome.metadata,         },         'sequences': sequences,         'config': {             'max_patients': args.max_patients,             'censoring_rate': args.censoring_rate,             'time_scale': args.time_scale,             'seed': args.seed,         },         'num_patients': len(sequences),     }          torch.save(save_data, filepath)     print(f\"\u2713 Saved {len(sequences)} sequences with outcomes\") In\u00a0[\u00a0]: Copied! <pre>def main():\n    \"\"\"Main validation function.\"\"\"\n    args = parse_args()\n    \n    print(\"=\"*70)\n    print(\"SYNTHETIC OUTCOME VALIDATION\")\n    print(\"=\"*70)\n    print(f\"\\nConfiguration:\")\n    print(f\"  \u2022 Max patients: {args.max_patients}\")\n    print(f\"  \u2022 Censoring rate: {args.censoring_rate}\")\n    print(f\"  \u2022 Time scale: {args.time_scale}\")\n    print(f\"  \u2022 Seed: {args.seed}\")\n    \n    # Load data\n    sequences = load_data(args)\n    \n    # Generate outcomes\n    outcome = generate_outcomes(args, sequences)\n    \n    # Validate correlation\n    correlation_ok = validate_correlation(outcome, sequences, verbose=args.verbose)\n    \n    # Validate distributions\n    validate_distribution(outcome, verbose=args.verbose)\n    \n    # Plot if requested\n    if args.plot:\n        plot_results(outcome, sequences)\n    \n    # Summary\n    print(\"\\n\" + \"=\"*70)\n    print(\"VALIDATION SUMMARY\")\n    print(\"=\"*70)\n    \n    if correlation_ok:\n        print(\"\\n\u2705 PASS: Synthetic outcomes look good!\")\n        print(\"   \u2022 Strong negative correlation between risk and event time\")\n        print(\"   \u2022 Ready to use for model training\")\n        \n        # Save if requested and validation passed\n        if args.save:\n            save_outcomes(outcome, sequences, args.save, args)\n            print(f\"\\n\ud83d\udca1 To use in notebook:\")\n            print(f\"   LOAD_PREGENERATED = '{args.save}'\")\n    else:\n        print(\"\\n\u274c FAIL: Synthetic outcomes need improvement\")\n        print(\"   \u2022 Weak or incorrect correlation\")\n        print(\"   \u2022 Check generator parameters or implementation\")\n        print(\"\\n\ud83d\udca1 Suggestions:\")\n        print(\"   \u2022 Reduce noise_std in _simulate_event_time()\")\n        print(\"   \u2022 Adjust time_scale parameter\")\n        print(\"   \u2022 Check risk score normalization\")\n        \n        if args.save:\n            print(f\"\\n\u26a0\ufe0f  Not saving outcomes (validation failed)\")\n    \n    return 0 if correlation_ok else 1\n</pre> def main():     \"\"\"Main validation function.\"\"\"     args = parse_args()          print(\"=\"*70)     print(\"SYNTHETIC OUTCOME VALIDATION\")     print(\"=\"*70)     print(f\"\\nConfiguration:\")     print(f\"  \u2022 Max patients: {args.max_patients}\")     print(f\"  \u2022 Censoring rate: {args.censoring_rate}\")     print(f\"  \u2022 Time scale: {args.time_scale}\")     print(f\"  \u2022 Seed: {args.seed}\")          # Load data     sequences = load_data(args)          # Generate outcomes     outcome = generate_outcomes(args, sequences)          # Validate correlation     correlation_ok = validate_correlation(outcome, sequences, verbose=args.verbose)          # Validate distributions     validate_distribution(outcome, verbose=args.verbose)          # Plot if requested     if args.plot:         plot_results(outcome, sequences)          # Summary     print(\"\\n\" + \"=\"*70)     print(\"VALIDATION SUMMARY\")     print(\"=\"*70)          if correlation_ok:         print(\"\\n\u2705 PASS: Synthetic outcomes look good!\")         print(\"   \u2022 Strong negative correlation between risk and event time\")         print(\"   \u2022 Ready to use for model training\")                  # Save if requested and validation passed         if args.save:             save_outcomes(outcome, sequences, args.save, args)             print(f\"\\n\ud83d\udca1 To use in notebook:\")             print(f\"   LOAD_PREGENERATED = '{args.save}'\")     else:         print(\"\\n\u274c FAIL: Synthetic outcomes need improvement\")         print(\"   \u2022 Weak or incorrect correlation\")         print(\"   \u2022 Check generator parameters or implementation\")         print(\"\\n\ud83d\udca1 Suggestions:\")         print(\"   \u2022 Reduce noise_std in _simulate_event_time()\")         print(\"   \u2022 Adjust time_scale parameter\")         print(\"   \u2022 Check risk score normalization\")                  if args.save:             print(f\"\\n\u26a0\ufe0f  Not saving outcomes (validation failed)\")          return 0 if correlation_ok else 1 In\u00a0[\u00a0]: Copied! <pre>if __name__ == '__main__':\n    sys.exit(main())\n</pre> if __name__ == '__main__':     sys.exit(main())"},{"location":"notebooks/02_survival_analysis/validate_survival_model/","title":"Validate survival model","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nValidation script for discrete-time survival LSTM model.\n\nThis script provides quick validation and testing capabilities for the survival model\nwith options for:\n- Patient subsampling for local testing\n- Example patient sequence display\n- Adjustable model complexity\n- Memory estimation\n- Synthetic outcome quality checks\n\nUsage:\n    # Quick validation with 200 patients on local system\n    python validate_survival_model.py --max-patients 200 --show-examples 5\n    \n    # Full validation on cloud GPU\n    python validate_survival_model.py --max-patients None --model-size large\n    \n    # Memory estimation only\n    python validate_survival_model.py --estimate-memory-only\n\"\"\"\n</pre> \"\"\" Validation script for discrete-time survival LSTM model.  This script provides quick validation and testing capabilities for the survival model with options for: - Patient subsampling for local testing - Example patient sequence display - Adjustable model complexity - Memory estimation - Synthetic outcome quality checks  Usage:     # Quick validation with 200 patients on local system     python validate_survival_model.py --max-patients 200 --show-examples 5          # Full validation on cloud GPU     python validate_survival_model.py --max-patients None --model-size large          # Memory estimation only     python validate_survival_model.py --estimate-memory-only \"\"\" In\u00a0[\u00a0]: Copied! <pre>import sys\nimport argparse\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom collections import defaultdict\nimport scipy.stats as stats\n</pre> import sys import argparse from pathlib import Path import numpy as np import pandas as pd import torch import torch.nn as nn from torch.utils.data import DataLoader from collections import defaultdict import scipy.stats as stats In\u00a0[\u00a0]: Copied! <pre># Add src to path\nsys.path.insert(0, str(Path(__file__).parent.parent.parent / 'src'))\n</pre> # Add src to path sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'src')) In\u00a0[\u00a0]: Copied! <pre>from ehrsequencing.data.adapters.synthea import SyntheaAdapter\nfrom ehrsequencing.data.visit_grouper import VisitGrouper\nfrom ehrsequencing.data.sequence_builder import PatientSequenceBuilder\nfrom ehrsequencing.models.survival_lstm import DiscreteTimeSurvivalLSTM\nfrom ehrsequencing.models.losses import DiscreteTimeSurvivalLoss\nfrom ehrsequencing.synthetic.survival import DiscreteTimeSurvivalGenerator\nfrom ehrsequencing.utils.sampling import (\n    subsample_patients,\n    get_recommended_batch_size,\n    estimate_memory_gb,\n    print_memory_recommendation,\n)\n</pre> from ehrsequencing.data.adapters.synthea import SyntheaAdapter from ehrsequencing.data.visit_grouper import VisitGrouper from ehrsequencing.data.sequence_builder import PatientSequenceBuilder from ehrsequencing.models.survival_lstm import DiscreteTimeSurvivalLSTM from ehrsequencing.models.losses import DiscreteTimeSurvivalLoss from ehrsequencing.synthetic.survival import DiscreteTimeSurvivalGenerator from ehrsequencing.utils.sampling import (     subsample_patients,     get_recommended_batch_size,     estimate_memory_gb,     print_memory_recommendation, ) In\u00a0[\u00a0]: Copied! <pre>def parse_args():\n    \"\"\"Parse command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Validate discrete-time survival LSTM model',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  # Local testing with 200 patients\n  python validate_survival_model.py --max-patients 200 --show-examples 5\n  \n  # Full dataset on cloud GPU\n  python validate_survival_model.py --max-patients None --model-size large\n  \n  # Small model for quick iteration\n  python validate_survival_model.py --max-patients 100 --model-size small --epochs 5\n  \n  # Memory estimation only (no training)\n  python validate_survival_model.py --estimate-memory-only\n        \"\"\"\n    )\n    \n    # Data options\n    parser.add_argument(\n        '--data-dir',\n        type=Path,\n        default=Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'large_cohort_1000',\n        help='Path to Synthea data directory'\n    )\n    parser.add_argument(\n        '--max-patients',\n        type=str,\n        default='200',\n        help='Maximum number of patients to use (or \"None\" for all)'\n    )\n    parser.add_argument(\n        '--min-visits',\n        type=int,\n        default=2,\n        help='Minimum visits per patient'\n    )\n    \n    # Model options\n    parser.add_argument(\n        '--model-size',\n        choices=['small', 'medium', 'large'],\n        default='medium',\n        help='Model complexity: small (64/128), medium (128/256), large (256/512)'\n    )\n    parser.add_argument(\n        '--num-layers',\n        type=int,\n        default=2,\n        help='Number of LSTM layers'\n    )\n    parser.add_argument(\n        '--dropout',\n        type=float,\n        default=0.3,\n        help='Dropout rate'\n    )\n    \n    # Training options\n    parser.add_argument(\n        '--epochs',\n        type=int,\n        default=10,\n        help='Number of training epochs'\n    )\n    parser.add_argument(\n        '--batch-size',\n        type=int,\n        default=None,\n        help='Batch size (auto-determined if not specified)'\n    )\n    parser.add_argument(\n        '--lr',\n        type=float,\n        default=0.001,\n        help='Learning rate'\n    )\n    \n    # Synthetic outcome options\n    parser.add_argument(\n        '--censoring-rate',\n        type=float,\n        default=0.3,\n        help='Censoring rate for synthetic outcomes'\n    )\n    parser.add_argument(\n        '--time-scale',\n        type=float,\n        default=0.3,\n        help='Time scale for synthetic outcomes'\n    )\n    \n    # Display options\n    parser.add_argument(\n        '--show-examples',\n        type=int,\n        default=0,\n        help='Number of example patient sequences to display'\n    )\n    parser.add_argument(\n        '--check-outcomes',\n        action='store_true',\n        help='Run diagnostic checks on synthetic outcomes'\n    )\n    \n    # Utility options\n    parser.add_argument(\n        '--estimate-memory-only',\n        action='store_true',\n        help='Only estimate memory requirements (no training)'\n    )\n    parser.add_argument(\n        '--seed',\n        type=int,\n        default=42,\n        help='Random seed'\n    )\n    parser.add_argument(\n        '--device',\n        type=str,\n        default='auto',\n        choices=['auto', 'cpu', 'mps', 'cuda'],\n        help='Device to use for training'\n    )\n    \n    return parser.parse_args()\n</pre> def parse_args():     \"\"\"Parse command line arguments.\"\"\"     parser = argparse.ArgumentParser(         description='Validate discrete-time survival LSTM model',         formatter_class=argparse.RawDescriptionHelpFormatter,         epilog=\"\"\" Examples:   # Local testing with 200 patients   python validate_survival_model.py --max-patients 200 --show-examples 5      # Full dataset on cloud GPU   python validate_survival_model.py --max-patients None --model-size large      # Small model for quick iteration   python validate_survival_model.py --max-patients 100 --model-size small --epochs 5      # Memory estimation only (no training)   python validate_survival_model.py --estimate-memory-only         \"\"\"     )          # Data options     parser.add_argument(         '--data-dir',         type=Path,         default=Path.home() / 'work' / 'loinc-predictor' / 'data' / 'synthea' / 'large_cohort_1000',         help='Path to Synthea data directory'     )     parser.add_argument(         '--max-patients',         type=str,         default='200',         help='Maximum number of patients to use (or \"None\" for all)'     )     parser.add_argument(         '--min-visits',         type=int,         default=2,         help='Minimum visits per patient'     )          # Model options     parser.add_argument(         '--model-size',         choices=['small', 'medium', 'large'],         default='medium',         help='Model complexity: small (64/128), medium (128/256), large (256/512)'     )     parser.add_argument(         '--num-layers',         type=int,         default=2,         help='Number of LSTM layers'     )     parser.add_argument(         '--dropout',         type=float,         default=0.3,         help='Dropout rate'     )          # Training options     parser.add_argument(         '--epochs',         type=int,         default=10,         help='Number of training epochs'     )     parser.add_argument(         '--batch-size',         type=int,         default=None,         help='Batch size (auto-determined if not specified)'     )     parser.add_argument(         '--lr',         type=float,         default=0.001,         help='Learning rate'     )          # Synthetic outcome options     parser.add_argument(         '--censoring-rate',         type=float,         default=0.3,         help='Censoring rate for synthetic outcomes'     )     parser.add_argument(         '--time-scale',         type=float,         default=0.3,         help='Time scale for synthetic outcomes'     )          # Display options     parser.add_argument(         '--show-examples',         type=int,         default=0,         help='Number of example patient sequences to display'     )     parser.add_argument(         '--check-outcomes',         action='store_true',         help='Run diagnostic checks on synthetic outcomes'     )          # Utility options     parser.add_argument(         '--estimate-memory-only',         action='store_true',         help='Only estimate memory requirements (no training)'     )     parser.add_argument(         '--seed',         type=int,         default=42,         help='Random seed'     )     parser.add_argument(         '--device',         type=str,         default='auto',         choices=['auto', 'cpu', 'mps', 'cuda'],         help='Device to use for training'     )          return parser.parse_args() In\u00a0[\u00a0]: Copied! <pre>def get_model_config(model_size: str):\n    \"\"\"Get model configuration based on size.\"\"\"\n    configs = {\n        'small': {'embedding_dim': 64, 'hidden_dim': 128},\n        'medium': {'embedding_dim': 128, 'hidden_dim': 256},\n        'large': {'embedding_dim': 256, 'hidden_dim': 512},\n    }\n    return configs[model_size]\n</pre> def get_model_config(model_size: str):     \"\"\"Get model configuration based on size.\"\"\"     configs = {         'small': {'embedding_dim': 64, 'hidden_dim': 128},         'medium': {'embedding_dim': 128, 'hidden_dim': 256},         'large': {'embedding_dim': 256, 'hidden_dim': 512},     }     return configs[model_size] In\u00a0[\u00a0]: Copied! <pre>def get_device(device_str: str):\n    \"\"\"Get torch device.\"\"\"\n    if device_str == 'auto':\n        if torch.cuda.is_available():\n            return torch.device('cuda')\n        elif torch.backends.mps.is_available():\n            return torch.device('mps')\n        else:\n            return torch.device('cpu')\n    return torch.device(device_str)\n</pre> def get_device(device_str: str):     \"\"\"Get torch device.\"\"\"     if device_str == 'auto':         if torch.cuda.is_available():             return torch.device('cuda')         elif torch.backends.mps.is_available():             return torch.device('mps')         else:             return torch.device('cpu')     return torch.device(device_str) In\u00a0[\u00a0]: Copied! <pre>def display_example_sequences(sequences, outcome, num_examples=5):\n    \"\"\"Display example patient sequences with their outcomes.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"EXAMPLE PATIENT SEQUENCES\")\n    print(\"=\"*80)\n    \n    for i in range(min(num_examples, len(sequences))):\n        seq = sequences[i]\n        event_time = outcome.event_times[i].item()\n        event_indicator = outcome.event_indicators[i].item()\n        risk_score = outcome.risk_scores[i].item()\n        \n        print(f\"\\nPatient {i+1}:\")\n        print(f\"  \u2022 Patient ID: {seq.patient_id}\")\n        print(f\"  \u2022 Number of visits: {len(seq.visits)}\")\n        print(f\"  \u2022 Risk score: {risk_score:.3f}\")\n        print(f\"  \u2022 Event time: {event_time} visits\")\n        print(f\"  \u2022 Event indicator: {event_indicator} ({'Event' if event_indicator == 1 else 'Censored'})\")\n        \n        print(f\"\\n  Visit timeline:\")\n        for j, visit in enumerate(seq.visits[:5]):  # Show first 5 visits\n            codes = visit.get_all_codes()\n            print(f\"    Visit {j}: {len(codes)} codes - {', '.join(codes[:3])}{'...' if len(codes) &gt; 3 else ''}\")\n        \n        if len(seq.visits) &gt; 5:\n            print(f\"    ... ({len(seq.visits) - 5} more visits)\")\n        \n        if j + 1 == event_time and event_indicator == 1:\n            print(f\"    \u26a0\ufe0f  Event occurred at visit {event_time}\")\n</pre> def display_example_sequences(sequences, outcome, num_examples=5):     \"\"\"Display example patient sequences with their outcomes.\"\"\"     print(\"\\n\" + \"=\"*80)     print(\"EXAMPLE PATIENT SEQUENCES\")     print(\"=\"*80)          for i in range(min(num_examples, len(sequences))):         seq = sequences[i]         event_time = outcome.event_times[i].item()         event_indicator = outcome.event_indicators[i].item()         risk_score = outcome.risk_scores[i].item()                  print(f\"\\nPatient {i+1}:\")         print(f\"  \u2022 Patient ID: {seq.patient_id}\")         print(f\"  \u2022 Number of visits: {len(seq.visits)}\")         print(f\"  \u2022 Risk score: {risk_score:.3f}\")         print(f\"  \u2022 Event time: {event_time} visits\")         print(f\"  \u2022 Event indicator: {event_indicator} ({'Event' if event_indicator == 1 else 'Censored'})\")                  print(f\"\\n  Visit timeline:\")         for j, visit in enumerate(seq.visits[:5]):  # Show first 5 visits             codes = visit.get_all_codes()             print(f\"    Visit {j}: {len(codes)} codes - {', '.join(codes[:3])}{'...' if len(codes) &gt; 3 else ''}\")                  if len(seq.visits) &gt; 5:             print(f\"    ... ({len(seq.visits) - 5} more visits)\")                  if j + 1 == event_time and event_indicator == 1:             print(f\"    \u26a0\ufe0f  Event occurred at visit {event_time}\") In\u00a0[\u00a0]: Copied! <pre>def check_synthetic_outcomes(outcome, sequences):\n    \"\"\"Run diagnostic checks on synthetic outcome quality.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"SYNTHETIC OUTCOME QUALITY CHECKS\")\n    print(\"=\"*80)\n    \n    # Check correlation between risk scores and event times\n    event_mask = outcome.event_indicators == 1\n    event_risk_scores = outcome.risk_scores[event_mask].numpy()\n    event_times = outcome.event_times[event_mask].numpy()\n    \n    if len(event_risk_scores) &gt; 1:\n        correlation, p_value = stats.pearsonr(event_risk_scores, event_times)\n        \n        print(f\"\\nCorrelation between risk score and event time:\")\n        print(f\"  \u2022 Pearson r = {correlation:.3f} (p={p_value:.4f})\")\n        print(f\"  \u2022 Expected: NEGATIVE correlation (high risk \u2192 early events)\")\n        \n        if correlation &gt; 0.1:\n            print(f\"  \u26a0\ufe0f  WARNING: POSITIVE correlation detected!\")\n            print(f\"     High-risk patients have LATE events (inverse relationship)\")\n            print(f\"     Model will learn backwards \u2192 C-index &lt; 0.5\")\n            print(f\"     FIX: Check synthetic outcome generator implementation\")\n        elif correlation &lt; -0.2:\n            print(f\"  \u2705 Good: Negative correlation detected\")\n            print(f\"     High-risk patients have early events (correct relationship)\")\n        else:\n            print(f\"  \u26a0\ufe0f  Weak correlation: outcomes may be too random\")\n    \n    # Event rate\n    event_rate = outcome.event_indicators.float().mean().item()\n    print(f\"\\nEvent statistics:\")\n    print(f\"  \u2022 Event rate: {event_rate:.1%}\")\n    print(f\"  \u2022 Censored rate: {1-event_rate:.1%}\")\n    \n    if event_rate &lt; 0.3:\n        print(f\"  \u26a0\ufe0f  Low event rate may make training difficult\")\n    elif event_rate &gt; 0.9:\n        print(f\"  \u26a0\ufe0f  Very high event rate (little censoring)\")\n    else:\n        print(f\"  \u2705 Reasonable event rate for survival analysis\")\n    \n    # Risk score distribution\n    print(f\"\\nRisk score distribution:\")\n    print(f\"  \u2022 Mean: {outcome.risk_scores.mean():.3f}\")\n    print(f\"  \u2022 Std: {outcome.risk_scores.std():.3f}\")\n    print(f\"  \u2022 Min: {outcome.risk_scores.min():.3f}\")\n    print(f\"  \u2022 Max: {outcome.risk_scores.max():.3f}\")\n    \n    # Example patients\n    print(f\"\\nExample patients (with events):\")\n    print(f\"{'Risk Score':&lt;12} {'Event Time':&lt;12} {'Expected':&lt;20}\")\n    print(\"-\" * 50)\n    for i in range(min(10, len(event_risk_scores))):\n        expected = \"Early\" if event_risk_scores[i] &gt; 0.6 else \"Late\"\n        actual = \"Early\" if event_times[i] &lt; np.median(event_times) else \"Late\"\n        match = \"\u2713\" if expected == actual else \"\u2717\"\n        print(f\"{event_risk_scores[i]:.3f}        {event_times[i]:&lt;12} {expected:&lt;10} (actual: {actual}) {match}\")\n</pre> def check_synthetic_outcomes(outcome, sequences):     \"\"\"Run diagnostic checks on synthetic outcome quality.\"\"\"     print(\"\\n\" + \"=\"*80)     print(\"SYNTHETIC OUTCOME QUALITY CHECKS\")     print(\"=\"*80)          # Check correlation between risk scores and event times     event_mask = outcome.event_indicators == 1     event_risk_scores = outcome.risk_scores[event_mask].numpy()     event_times = outcome.event_times[event_mask].numpy()          if len(event_risk_scores) &gt; 1:         correlation, p_value = stats.pearsonr(event_risk_scores, event_times)                  print(f\"\\nCorrelation between risk score and event time:\")         print(f\"  \u2022 Pearson r = {correlation:.3f} (p={p_value:.4f})\")         print(f\"  \u2022 Expected: NEGATIVE correlation (high risk \u2192 early events)\")                  if correlation &gt; 0.1:             print(f\"  \u26a0\ufe0f  WARNING: POSITIVE correlation detected!\")             print(f\"     High-risk patients have LATE events (inverse relationship)\")             print(f\"     Model will learn backwards \u2192 C-index &lt; 0.5\")             print(f\"     FIX: Check synthetic outcome generator implementation\")         elif correlation &lt; -0.2:             print(f\"  \u2705 Good: Negative correlation detected\")             print(f\"     High-risk patients have early events (correct relationship)\")         else:             print(f\"  \u26a0\ufe0f  Weak correlation: outcomes may be too random\")          # Event rate     event_rate = outcome.event_indicators.float().mean().item()     print(f\"\\nEvent statistics:\")     print(f\"  \u2022 Event rate: {event_rate:.1%}\")     print(f\"  \u2022 Censored rate: {1-event_rate:.1%}\")          if event_rate &lt; 0.3:         print(f\"  \u26a0\ufe0f  Low event rate may make training difficult\")     elif event_rate &gt; 0.9:         print(f\"  \u26a0\ufe0f  Very high event rate (little censoring)\")     else:         print(f\"  \u2705 Reasonable event rate for survival analysis\")          # Risk score distribution     print(f\"\\nRisk score distribution:\")     print(f\"  \u2022 Mean: {outcome.risk_scores.mean():.3f}\")     print(f\"  \u2022 Std: {outcome.risk_scores.std():.3f}\")     print(f\"  \u2022 Min: {outcome.risk_scores.min():.3f}\")     print(f\"  \u2022 Max: {outcome.risk_scores.max():.3f}\")          # Example patients     print(f\"\\nExample patients (with events):\")     print(f\"{'Risk Score':&lt;12} {'Event Time':&lt;12} {'Expected':&lt;20}\")     print(\"-\" * 50)     for i in range(min(10, len(event_risk_scores))):         expected = \"Early\" if event_risk_scores[i] &gt; 0.6 else \"Late\"         actual = \"Early\" if event_times[i] &lt; np.median(event_times) else \"Late\"         match = \"\u2713\" if expected == actual else \"\u2717\"         print(f\"{event_risk_scores[i]:.3f}        {event_times[i]:&lt;12} {expected:&lt;10} (actual: {actual}) {match}\") In\u00a0[\u00a0]: Copied! <pre>def load_and_prepare_data(args):\n    \"\"\"Load and prepare data for training.\"\"\"\n    print(\"=\"*80)\n    print(\"LOADING DATA\")\n    print(\"=\"*80)\n    \n    # Set random seed\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    \n    # Load data\n    print(f\"\\nLoading Synthea data from: {args.data_dir}\")\n    adapter = SyntheaAdapter(args.data_dir)\n    events = adapter.load_events()\n    print(f\"Loaded {len(events)} events\")\n    \n    # Group into visits\n    print(\"\\nGrouping events into visits...\")\n    grouper = VisitGrouper(time_window_hours=24)\n    visits = grouper.group_events(events)\n    print(f\"Created {len(visits)} visits\")\n    \n    # Group by patient\n    visits_by_patient = defaultdict(list)\n    for visit in visits:\n        visits_by_patient[visit.patient_id].append(visit)\n    \n    # Sort visits by timestamp\n    for patient_id in visits_by_patient:\n        visits_by_patient[patient_id].sort(key=lambda v: v.timestamp)\n    \n    print(f\"\\nData organized for {len(visits_by_patient)} patients\")\n    \n    # Subsample patients\n    max_patients = None if args.max_patients.lower() == 'none' else int(args.max_patients)\n    visits_by_patient = subsample_patients(\n        visits_by_patient,\n        max_patients=max_patients,\n        seed=args.seed,\n        verbose=True\n    )\n    \n    # Build vocabulary\n    print(\"\\nBuilding vocabulary...\")\n    builder = PatientSequenceBuilder()\n    builder.build_vocabulary(list(visits_by_patient.values()))\n    print(f\"Vocabulary size: {builder.vocabulary_size}\")\n    \n    # Build sequences\n    print(\"\\nBuilding patient sequences...\")\n    sequences = builder.build_sequences(list(visits_by_patient.values()), min_visits=args.min_visits)\n    print(f\"Built {len(sequences)} sequences\")\n    \n    # Compute statistics\n    avg_visits = np.mean([len(seq.visits) for seq in sequences])\n    max_visits = max([len(seq.visits) for seq in sequences])\n    avg_codes = np.mean([\n        np.mean([visit.num_codes() for visit in seq.visits])\n        for seq in sequences\n    ])\n    max_codes = max([\n        max([visit.num_codes() for visit in seq.visits])\n        for seq in sequences\n    ])\n    \n    print(f\"\\nDataset statistics:\")\n    print(f\"  \u2022 Patients: {len(sequences)}\")\n    print(f\"  \u2022 Avg visits per patient: {avg_visits:.1f}\")\n    print(f\"  \u2022 Max visits per patient: {max_visits}\")\n    print(f\"  \u2022 Avg codes per visit: {avg_codes:.1f}\")\n    print(f\"  \u2022 Max codes per visit: {max_codes}\")\n    \n    return builder, sequences, {\n        'num_patients': len(sequences),\n        'avg_visits': avg_visits,\n        'max_visits': max_visits,\n        'avg_codes_per_visit': avg_codes,\n        'max_codes_per_visit': max_codes,\n    }\n</pre> def load_and_prepare_data(args):     \"\"\"Load and prepare data for training.\"\"\"     print(\"=\"*80)     print(\"LOADING DATA\")     print(\"=\"*80)          # Set random seed     np.random.seed(args.seed)     torch.manual_seed(args.seed)          # Load data     print(f\"\\nLoading Synthea data from: {args.data_dir}\")     adapter = SyntheaAdapter(args.data_dir)     events = adapter.load_events()     print(f\"Loaded {len(events)} events\")          # Group into visits     print(\"\\nGrouping events into visits...\")     grouper = VisitGrouper(time_window_hours=24)     visits = grouper.group_events(events)     print(f\"Created {len(visits)} visits\")          # Group by patient     visits_by_patient = defaultdict(list)     for visit in visits:         visits_by_patient[visit.patient_id].append(visit)          # Sort visits by timestamp     for patient_id in visits_by_patient:         visits_by_patient[patient_id].sort(key=lambda v: v.timestamp)          print(f\"\\nData organized for {len(visits_by_patient)} patients\")          # Subsample patients     max_patients = None if args.max_patients.lower() == 'none' else int(args.max_patients)     visits_by_patient = subsample_patients(         visits_by_patient,         max_patients=max_patients,         seed=args.seed,         verbose=True     )          # Build vocabulary     print(\"\\nBuilding vocabulary...\")     builder = PatientSequenceBuilder()     builder.build_vocabulary(list(visits_by_patient.values()))     print(f\"Vocabulary size: {builder.vocabulary_size}\")          # Build sequences     print(\"\\nBuilding patient sequences...\")     sequences = builder.build_sequences(list(visits_by_patient.values()), min_visits=args.min_visits)     print(f\"Built {len(sequences)} sequences\")          # Compute statistics     avg_visits = np.mean([len(seq.visits) for seq in sequences])     max_visits = max([len(seq.visits) for seq in sequences])     avg_codes = np.mean([         np.mean([visit.num_codes() for visit in seq.visits])         for seq in sequences     ])     max_codes = max([         max([visit.num_codes() for visit in seq.visits])         for seq in sequences     ])          print(f\"\\nDataset statistics:\")     print(f\"  \u2022 Patients: {len(sequences)}\")     print(f\"  \u2022 Avg visits per patient: {avg_visits:.1f}\")     print(f\"  \u2022 Max visits per patient: {max_visits}\")     print(f\"  \u2022 Avg codes per visit: {avg_codes:.1f}\")     print(f\"  \u2022 Max codes per visit: {max_codes}\")          return builder, sequences, {         'num_patients': len(sequences),         'avg_visits': avg_visits,         'max_visits': max_visits,         'avg_codes_per_visit': avg_codes,         'max_codes_per_visit': max_codes,     } In\u00a0[\u00a0]: Copied! <pre>def estimate_memory(args, builder, stats, model_config):\n    \"\"\"Estimate memory requirements.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"MEMORY ESTIMATION\")\n    print(\"=\"*80)\n    \n    batch_size = args.batch_size or get_recommended_batch_size(\n        stats['num_patients'],\n        device=args.device,\n        model_size=args.model_size\n    )\n    \n    mem_est = estimate_memory_gb(\n        num_patients=stats['num_patients'],\n        avg_visits=stats['avg_visits'],\n        max_visits=int(stats['max_visits']),\n        avg_codes_per_visit=stats['avg_codes_per_visit'],\n        max_codes_per_visit=int(stats['max_codes_per_visit']),\n        vocab_size=builder.vocabulary_size,\n        embedding_dim=model_config['embedding_dim'],\n        hidden_dim=model_config['hidden_dim'],\n        batch_size=batch_size,\n    )\n    \n    print_memory_recommendation(mem_est, verbose=True)\n    \n    return mem_est\n</pre> def estimate_memory(args, builder, stats, model_config):     \"\"\"Estimate memory requirements.\"\"\"     print(\"\\n\" + \"=\"*80)     print(\"MEMORY ESTIMATION\")     print(\"=\"*80)          batch_size = args.batch_size or get_recommended_batch_size(         stats['num_patients'],         device=args.device,         model_size=args.model_size     )          mem_est = estimate_memory_gb(         num_patients=stats['num_patients'],         avg_visits=stats['avg_visits'],         max_visits=int(stats['max_visits']),         avg_codes_per_visit=stats['avg_codes_per_visit'],         max_codes_per_visit=int(stats['max_codes_per_visit']),         vocab_size=builder.vocabulary_size,         embedding_dim=model_config['embedding_dim'],         hidden_dim=model_config['hidden_dim'],         batch_size=batch_size,     )          print_memory_recommendation(mem_est, verbose=True)          return mem_est In\u00a0[\u00a0]: Copied! <pre>def generate_outcomes(args, sequences):\n    \"\"\"Generate synthetic survival outcomes.\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"GENERATING SYNTHETIC OUTCOMES\")\n    print(\"=\"*80)\n    \n    generator = DiscreteTimeSurvivalGenerator(\n        censoring_rate=args.censoring_rate,\n        risk_weights={'comorbidity': 0.4, 'frequency': 0.4, 'diversity': 0.2},\n        time_scale=args.time_scale,\n        seed=args.seed\n    )\n    \n    outcome = generator.generate(sequences)\n    \n    print(f\"\\nGenerated outcomes for {len(sequences)} patients\")\n    print(f\"  \u2022 Event rate: {outcome.event_indicators.float().mean():.1%}\")\n    print(f\"  \u2022 Median event/censoring time: {outcome.event_times.float().median():.1f} visits\")\n    print(f\"  \u2022 Mean risk score: {outcome.risk_scores.mean():.3f}\")\n    \n    return outcome\n</pre> def generate_outcomes(args, sequences):     \"\"\"Generate synthetic survival outcomes.\"\"\"     print(\"\\n\" + \"=\"*80)     print(\"GENERATING SYNTHETIC OUTCOMES\")     print(\"=\"*80)          generator = DiscreteTimeSurvivalGenerator(         censoring_rate=args.censoring_rate,         risk_weights={'comorbidity': 0.4, 'frequency': 0.4, 'diversity': 0.2},         time_scale=args.time_scale,         seed=args.seed     )          outcome = generator.generate(sequences)          print(f\"\\nGenerated outcomes for {len(sequences)} patients\")     print(f\"  \u2022 Event rate: {outcome.event_indicators.float().mean():.1%}\")     print(f\"  \u2022 Median event/censoring time: {outcome.event_times.float().median():.1f} visits\")     print(f\"  \u2022 Mean risk score: {outcome.risk_scores.mean():.3f}\")          return outcome In\u00a0[\u00a0]: Copied! <pre>def main():\n    \"\"\"Main validation function.\"\"\"\n    args = parse_args()\n    \n    print(\"=\"*80)\n    print(\"DISCRETE-TIME SURVIVAL LSTM VALIDATION\")\n    print(\"=\"*80)\n    print(f\"\\nConfiguration:\")\n    print(f\"  \u2022 Data directory: {args.data_dir}\")\n    print(f\"  \u2022 Max patients: {args.max_patients}\")\n    print(f\"  \u2022 Model size: {args.model_size}\")\n    print(f\"  \u2022 Epochs: {args.epochs}\")\n    print(f\"  \u2022 Device: {args.device}\")\n    \n    # Get model configuration\n    model_config = get_model_config(args.model_size)\n    \n    # Load data\n    builder, sequences, stats = load_and_prepare_data(args)\n    \n    # Estimate memory\n    mem_est = estimate_memory(args, builder, stats, model_config)\n    \n    if args.estimate_memory_only:\n        print(\"\\n\u2705 Memory estimation complete (--estimate-memory-only specified)\")\n        return\n    \n    # Generate outcomes\n    outcome = generate_outcomes(args, sequences)\n    \n    # Show examples if requested\n    if args.show_examples &gt; 0:\n        display_example_sequences(sequences, outcome, num_examples=args.show_examples)\n    \n    # Check outcome quality if requested\n    if args.check_outcomes:\n        check_synthetic_outcomes(outcome, sequences)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"VALIDATION COMPLETE\")\n    print(\"=\"*80)\n    print(\"\\n\ud83d\udca1 Next steps:\")\n    print(\"  1. If memory estimate looks good, proceed with training\")\n    print(\"  2. If correlation check shows issues, fix synthetic generator\")\n    print(\"  3. Adjust --model-size or --max-patients if needed\")\n    print(\"  4. Use full notebook for complete training workflow\")\n</pre> def main():     \"\"\"Main validation function.\"\"\"     args = parse_args()          print(\"=\"*80)     print(\"DISCRETE-TIME SURVIVAL LSTM VALIDATION\")     print(\"=\"*80)     print(f\"\\nConfiguration:\")     print(f\"  \u2022 Data directory: {args.data_dir}\")     print(f\"  \u2022 Max patients: {args.max_patients}\")     print(f\"  \u2022 Model size: {args.model_size}\")     print(f\"  \u2022 Epochs: {args.epochs}\")     print(f\"  \u2022 Device: {args.device}\")          # Get model configuration     model_config = get_model_config(args.model_size)          # Load data     builder, sequences, stats = load_and_prepare_data(args)          # Estimate memory     mem_est = estimate_memory(args, builder, stats, model_config)          if args.estimate_memory_only:         print(\"\\n\u2705 Memory estimation complete (--estimate-memory-only specified)\")         return          # Generate outcomes     outcome = generate_outcomes(args, sequences)          # Show examples if requested     if args.show_examples &gt; 0:         display_example_sequences(sequences, outcome, num_examples=args.show_examples)          # Check outcome quality if requested     if args.check_outcomes:         check_synthetic_outcomes(outcome, sequences)          print(\"\\n\" + \"=\"*80)     print(\"VALIDATION COMPLETE\")     print(\"=\"*80)     print(\"\\n\ud83d\udca1 Next steps:\")     print(\"  1. If memory estimate looks good, proceed with training\")     print(\"  2. If correlation check shows issues, fix synthetic generator\")     print(\"  3. Adjust --model-size or --max-patients if needed\")     print(\"  4. Use full notebook for complete training workflow\") In\u00a0[\u00a0]: Copied! <pre>if __name__ == '__main__':\n    main()\n</pre> if __name__ == '__main__':     main()"}]}